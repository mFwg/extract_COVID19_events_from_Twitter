{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOv4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mFwg/extract_COVID19_events_from_Twitter/blob/master/YOLOv4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDqDG23vThqE",
        "outputId": "cfefe916-8117-497a-cff4-368d33694e64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apx_Fa_dSVzz",
        "outputId": "c0a6261e-4051-4ed4-9b34-91f1b05db338",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip /content/za_traffic_2020\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/za_traffic_2020.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/za_traffic_2020 or\n",
            "        /content/za_traffic_2020.zip, and cannot find /content/za_traffic_2020.ZIP, period.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKotcJweR8hs",
        "outputId": "c02f1663-f08c-41a9-c5a4-ff7c89132de5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhz4mYKzK6Qt"
      },
      "source": [
        "!wget https://dl.challenge.zalo.ai/traffic-sign-detection/data/za_traffic_2020.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz658ECjRFVf",
        "outputId": "32f892bb-2d6b-454e-bcae-e6ab2a235ad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/  \u001b[01;34mtensorflow-yolov4-tflite\u001b[0m/  za_traffic_2020.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As2oumvGuqCy",
        "outputId": "bcdb657e-4dd5-476d-989a-9acc09b1acdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Nov  8 09:27:34 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcCnSc0mxEgT",
        "outputId": "df79bcdb-df65-4821-94fc-d073896dc132",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/  \u001b[01;34mtensorflow-yolov4-tflite\u001b[0m/  za_traffic_2020.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uq3cyMVMDaM",
        "outputId": "de2783eb-8535-4b45-8e83-b9c375ee0d65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python detect.py --weights ./checkpoints/yolov4-416 --size 416 --model yolov4 --image ./data/person-doing-skateboarding-tricks.jpg\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-08 09:38:35.744820: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-08 09:38:37.116018: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-11-08 09:38:37.132115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:38:37.132690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-11-08 09:38:37.132729: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-08 09:38:37.134571: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-08 09:38:37.136267: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-08 09:38:37.136615: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-08 09:38:37.138503: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-08 09:38:37.139666: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-08 09:38:37.143247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-08 09:38:37.143371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:38:37.143909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:38:37.144405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-08 09:38:37.162474: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-11-08 09:38:37.167837: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000175000 Hz\n",
            "2020-11-08 09:38:37.168038: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2184a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-08 09:38:37.168068: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-11-08 09:38:37.254806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:38:37.255743: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2184bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-08 09:38:37.255775: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-11-08 09:38:37.255943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:38:37.256497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-11-08 09:38:37.256540: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-08 09:38:37.256584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-08 09:38:37.256604: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-08 09:38:37.256622: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-08 09:38:37.256640: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-08 09:38:37.256657: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-08 09:38:37.256674: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-08 09:38:37.256736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:38:37.257298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:38:37.257775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-08 09:38:37.257818: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-08 09:38:37.945825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-11-08 09:38:37.945885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-11-08 09:38:37.945898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-11-08 09:38:37.946130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:38:37.946763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:38:37.947290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14951 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "2020-11-08 09:38:37.985405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:38:37.985993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-11-08 09:38:37.986037: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-08 09:38:37.986078: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-08 09:38:37.986098: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-08 09:38:37.986118: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-08 09:38:37.986137: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-08 09:38:37.986156: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-08 09:38:37.986175: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-08 09:38:37.986240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:38:37.986818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:38:37.987345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-08 09:38:37.987381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-11-08 09:38:37.987395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-11-08 09:38:37.987404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-11-08 09:38:37.987485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:38:37.988035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:38:37.988570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14951 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "2020-11-08 09:38:55.037325: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-08 09:38:56.098176: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvctPaOzwF5W",
        "outputId": "17aabc70-7589-4a25-cb18-ee4ba5848b82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-08 09:32:38.319037: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-08 09:32:39.667315: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-11-08 09:32:39.680502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:32:39.681045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-11-08 09:32:39.681079: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-08 09:32:39.682565: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-08 09:32:39.684243: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-08 09:32:39.684565: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-08 09:32:39.686200: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-08 09:32:39.687228: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-08 09:32:39.690939: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-08 09:32:39.691045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:32:39.691649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:32:39.692132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-08 09:32:39.692471: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-11-08 09:32:39.697378: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000175000 Hz\n",
            "2020-11-08 09:32:39.697557: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1660a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-08 09:32:39.697584: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-11-08 09:32:39.784382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:32:39.785302: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1660bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-08 09:32:39.785337: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-11-08 09:32:39.785510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:32:39.786018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-11-08 09:32:39.786057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-08 09:32:39.786112: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-08 09:32:39.786133: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-08 09:32:39.786162: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-08 09:32:39.786181: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-08 09:32:39.786200: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-08 09:32:39.786219: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-08 09:32:39.786319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:32:39.786849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:32:39.787348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-08 09:32:39.787399: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-08 09:32:40.468094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-11-08 09:32:40.468156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-11-08 09:32:40.468169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-11-08 09:32:40.468368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:32:40.468940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:32:40.469519: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-11-08 09:32:40.469566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14951 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 160, in <module>\n",
            "    app.run(main)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"train.py\", line 20, in main\n",
            "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/config.py\", line 531, in set_memory_growth\n",
            "    context.context().set_memory_growth(device, enable)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\", line 1419, in set_memory_growth\n",
            "    \"Physical devices cannot be modified after being initialized\")\n",
            "RuntimeError: Physical devices cannot be modified after being initialized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvfWR513gskS",
        "outputId": "56c43667-8c39-46d3-d47e-64bcfb3e63fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "pip install -r requirements.txt &&\n",
        "pip install -r requirements-gpu.txt\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencv-python==4.1.1.26\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/7e/bd5425f4dacb73367fddc71388a47c1ea570839197c2bcad86478e565186/opencv_python-4.1.1.26-cp36-cp36m-manylinux1_x86_64.whl (28.7MB)\n",
            "\u001b[K     |████████████████████████████████| 28.7MB 110kB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.2.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (4.41.1)\n",
            "Collecting tensorflow==2.3.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/68/7c6c8e2b65ad4a3ff5ef658c04a6c2802ff7fe55fc7eecacb6efee1abc40/tensorflow-2.3.0rc0-cp36-cp36m-manylinux2010_x86_64.whl (320.3MB)\n",
            "\u001b[K     |████████████████████████████████| 320.3MB 55kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (7.0.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python==4.1.1.26->-r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (0.35.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (3.12.4)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 56.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (1.12.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (1.33.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (1.15.0)\n",
            "Collecting tf-estimator-nightly<2.3.0.dev2020062302,>=2.3.0.dev2020062301\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/3b/fb9aafd734da258411bff2a600cabff65c7d201782318791b72422bd973d/tf_estimator_nightly-2.3.0.dev2020062301-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (3.3.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (2.10.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 7)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 7)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 7)) (2.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (50.3.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (3.3.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (2.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.3.0rc0->-r requirements.txt (line 4)) (3.4.0)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: opencv-python, tensorboard, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed opencv-python-4.1.1.26 tensorboard-2.2.2 tensorflow-2.3.0rc0 tf-estimator-nightly-2.3.0.dev2020062301\n",
            "Collecting tensorflow-gpu==2.3.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/53/8553b59f223aef72d0a24dbc15364c86a28440652e98504301fea2286a0c/tensorflow_gpu-2.3.0rc0-cp36-cp36m-manylinux2010_x86_64.whl (320.3MB)\n",
            "\u001b[K     |████████████████████████████████| 320.3MB 44kB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python==4.1.1.26 in /usr/local/lib/python3.6/dist-packages (from -r requirements-gpu.txt (line 2)) (4.1.1.26)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from -r requirements-gpu.txt (line 3)) (4.2.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements-gpu.txt (line 4)) (4.41.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from -r requirements-gpu.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements-gpu.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.6/dist-packages (from -r requirements-gpu.txt (line 7)) (1.9)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from -r requirements-gpu.txt (line 8)) (7.0.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (0.35.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (1.33.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: tf-estimator-nightly<2.3.0.dev2020062302,>=2.3.0.dev2020062301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (2.3.0.dev2020062301)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-gpu.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-gpu.txt (line 6)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-gpu.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-gpu.txt (line 6)) (2.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (50.3.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (3.3.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (2020.6.20)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.3.0rc0->-r requirements-gpu.txt (line 1)) (3.4.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.3.0rc0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE3CsWyLs0CT",
        "outputId": "2b983e0e-f351-4f1a-e593-dcc4b5239d7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Convert darknet weights to tensorflow\n",
        "## yolov4\n",
        "!python save_model.py --weights ./data/yolov4.weights --output ./checkpoints/yolov4-416 --input_size 416 --model yolov4 \n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-08 09:31:14.882782: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-08 09:31:16.168782: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-11-08 09:31:16.181991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:31:16.182574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-11-08 09:31:16.182611: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-08 09:31:16.184214: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-08 09:31:16.185983: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-08 09:31:16.186329: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-08 09:31:16.188022: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-08 09:31:16.189230: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-08 09:31:16.193535: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-08 09:31:16.193646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:31:16.194190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:31:16.194692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-08 09:31:16.195039: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-11-08 09:31:16.200025: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000175000 Hz\n",
            "2020-11-08 09:31:16.200205: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2872a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-08 09:31:16.200231: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-11-08 09:31:16.289095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:31:16.290179: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2872bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-08 09:31:16.290210: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-11-08 09:31:16.290441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:31:16.291101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-11-08 09:31:16.291171: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-08 09:31:16.291234: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-08 09:31:16.291300: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-08 09:31:16.291332: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-08 09:31:16.291355: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-08 09:31:16.291375: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-08 09:31:16.291396: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-08 09:31:16.291478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:31:16.292156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:31:16.292678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-08 09:31:16.292728: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-08 09:31:16.967436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-11-08 09:31:16.967494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-11-08 09:31:16.967510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-11-08 09:31:16.967722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:31:16.968419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-08 09:31:16.968962: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-11-08 09:31:16.969008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14951 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 416, 416, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 416, 416, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 416, 416, 32) 128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus (TensorFlo [(None, 416, 416, 32 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh (TensorFlowOpL [(None, 416, 416, 32 0           tf_op_layer_Softplus[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul (TensorFlowOpLa [(None, 416, 416, 32 0           batch_normalization[0][0]        \n",
            "                                                                 tf_op_layer_Tanh[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 417, 417, 32) 0           tf_op_layer_Mul[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 208, 208, 64) 18432       zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 208, 208, 64) 256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_1 (TensorF [(None, 208, 208, 64 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_1 (TensorFlowO [(None, 208, 208, 64 0           tf_op_layer_Softplus_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_1 (TensorFlowOp [(None, 208, 208, 64 0           batch_normalization_1[0][0]      \n",
            "                                                                 tf_op_layer_Tanh_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 208, 208, 64) 4096        tf_op_layer_Mul_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 208, 208, 64) 256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_3 (TensorF [(None, 208, 208, 64 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_3 (TensorFlowO [(None, 208, 208, 64 0           tf_op_layer_Softplus_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_3 (TensorFlowOp [(None, 208, 208, 64 0           batch_normalization_3[0][0]      \n",
            "                                                                 tf_op_layer_Tanh_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 208, 208, 32) 2048        tf_op_layer_Mul_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 208, 208, 32) 128         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_4 (TensorF [(None, 208, 208, 32 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_4 (TensorFlowO [(None, 208, 208, 32 0           tf_op_layer_Softplus_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_4 (TensorFlowOp [(None, 208, 208, 32 0           batch_normalization_4[0][0]      \n",
            "                                                                 tf_op_layer_Tanh_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 208, 208, 64) 18432       tf_op_layer_Mul_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 208, 208, 64) 256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_5 (TensorF [(None, 208, 208, 64 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_5 (TensorFlowO [(None, 208, 208, 64 0           tf_op_layer_Softplus_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_5 (TensorFlowOp [(None, 208, 208, 64 0           batch_normalization_5[0][0]      \n",
            "                                                                 tf_op_layer_Tanh_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2 (TensorFlowOp [(None, 208, 208, 64 0           tf_op_layer_Mul_3[0][0]          \n",
            "                                                                 tf_op_layer_Mul_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 208, 208, 64) 4096        tf_op_layer_AddV2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 208, 208, 64) 4096        tf_op_layer_Mul_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 208, 208, 64) 256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 208, 208, 64) 256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_6 (TensorF [(None, 208, 208, 64 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_2 (TensorF [(None, 208, 208, 64 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_6 (TensorFlowO [(None, 208, 208, 64 0           tf_op_layer_Softplus_6[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_2 (TensorFlowO [(None, 208, 208, 64 0           tf_op_layer_Softplus_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_6 (TensorFlowOp [(None, 208, 208, 64 0           batch_normalization_6[0][0]      \n",
            "                                                                 tf_op_layer_Tanh_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_2 (TensorFlowOp [(None, 208, 208, 64 0           batch_normalization_2[0][0]      \n",
            "                                                                 tf_op_layer_Tanh_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat (TensorFlowO [(None, 208, 208, 12 0           tf_op_layer_Mul_6[0][0]          \n",
            "                                                                 tf_op_layer_Mul_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 208, 208, 64) 8192        tf_op_layer_concat[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 208, 208, 64) 256         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_7 (TensorF [(None, 208, 208, 64 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_7 (TensorFlowO [(None, 208, 208, 64 0           tf_op_layer_Softplus_7[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_7 (TensorFlowOp [(None, 208, 208, 64 0           batch_normalization_7[0][0]      \n",
            "                                                                 tf_op_layer_Tanh_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 209, 209, 64) 0           tf_op_layer_Mul_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 104, 104, 128 73728       zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 104, 104, 128 512         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_8 (TensorF [(None, 104, 104, 12 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_8 (TensorFlowO [(None, 104, 104, 12 0           tf_op_layer_Softplus_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_8 (TensorFlowOp [(None, 104, 104, 12 0           batch_normalization_8[0][0]      \n",
            "                                                                 tf_op_layer_Tanh_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 104, 104, 64) 8192        tf_op_layer_Mul_8[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 104, 104, 64) 256         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_10 (Tensor [(None, 104, 104, 64 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_10 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_Softplus_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_10 (TensorFlowO [(None, 104, 104, 64 0           batch_normalization_10[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_10[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 104, 104, 64) 4096        tf_op_layer_Mul_10[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 104, 104, 64) 256         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_11 (Tensor [(None, 104, 104, 64 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_11 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_Softplus_11[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_11 (TensorFlowO [(None, 104, 104, 64 0           batch_normalization_11[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_11[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 104, 104, 64) 36864       tf_op_layer_Mul_11[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 104, 104, 64) 256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_12 (Tensor [(None, 104, 104, 64 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_12 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_Softplus_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_12 (TensorFlowO [(None, 104, 104, 64 0           batch_normalization_12[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_12[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_1 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_Mul_10[0][0]         \n",
            "                                                                 tf_op_layer_Mul_12[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 104, 104, 64) 4096        tf_op_layer_AddV2_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 104, 104, 64) 256         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_13 (Tensor [(None, 104, 104, 64 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_13 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_Softplus_13[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_13 (TensorFlowO [(None, 104, 104, 64 0           batch_normalization_13[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 104, 104, 64) 36864       tf_op_layer_Mul_13[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 104, 104, 64) 256         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_14 (Tensor [(None, 104, 104, 64 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_14 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_Softplus_14[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_14 (TensorFlowO [(None, 104, 104, 64 0           batch_normalization_14[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_14[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_2 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_AddV2_1[0][0]        \n",
            "                                                                 tf_op_layer_Mul_14[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 104, 104, 64) 4096        tf_op_layer_AddV2_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 104, 104, 64) 8192        tf_op_layer_Mul_8[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 104, 104, 64) 256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 104, 104, 64) 256         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_15 (Tensor [(None, 104, 104, 64 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_9 (TensorF [(None, 104, 104, 64 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_15 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_Softplus_15[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_9 (TensorFlowO [(None, 104, 104, 64 0           tf_op_layer_Softplus_9[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_15 (TensorFlowO [(None, 104, 104, 64 0           batch_normalization_15[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_9 (TensorFlowOp [(None, 104, 104, 64 0           batch_normalization_9[0][0]      \n",
            "                                                                 tf_op_layer_Tanh_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1 (TensorFlo [(None, 104, 104, 12 0           tf_op_layer_Mul_15[0][0]         \n",
            "                                                                 tf_op_layer_Mul_9[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 104, 104, 128 16384       tf_op_layer_concat_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 104, 104, 128 512         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_16 (Tensor [(None, 104, 104, 12 0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_16 (TensorFlow [(None, 104, 104, 12 0           tf_op_layer_Softplus_16[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_16 (TensorFlowO [(None, 104, 104, 12 0           batch_normalization_16[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_16[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPadding2D (None, 105, 105, 128 0           tf_op_layer_Mul_16[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 52, 52, 256)  294912      zero_padding2d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 52, 52, 256)  1024        conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_17 (Tensor [(None, 52, 52, 256) 0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_17 (TensorFlow [(None, 52, 52, 256) 0           tf_op_layer_Softplus_17[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_17 (TensorFlowO [(None, 52, 52, 256) 0           batch_normalization_17[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_17[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 52, 52, 128)  32768       tf_op_layer_Mul_17[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 52, 52, 128)  512         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_19 (Tensor [(None, 52, 52, 128) 0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_19 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_19[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_19 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_19[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_19[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_Mul_19[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 52, 52, 128)  512         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_20 (Tensor [(None, 52, 52, 128) 0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_20 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_20[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_20 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_20[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_20[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_20[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 52, 52, 128)  512         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_21 (Tensor [(None, 52, 52, 128) 0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_21 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_21[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_21 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_21[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_21[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_3 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Mul_19[0][0]         \n",
            "                                                                 tf_op_layer_Mul_21[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 52, 52, 128)  512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_22 (Tensor [(None, 52, 52, 128) 0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_22 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_22[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_22 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_22[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_22[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_22[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 52, 52, 128)  512         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_23 (Tensor [(None, 52, 52, 128) 0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_23 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_23[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_23 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_23[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_23[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_4 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_AddV2_3[0][0]        \n",
            "                                                                 tf_op_layer_Mul_23[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 52, 52, 128)  512         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_24 (Tensor [(None, 52, 52, 128) 0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_24 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_24[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_24 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_24[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_24[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_24[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 52, 52, 128)  512         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_25 (Tensor [(None, 52, 52, 128) 0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_25 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_25[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_25 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_25[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_25[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_5 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_AddV2_4[0][0]        \n",
            "                                                                 tf_op_layer_Mul_25[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 52, 52, 128)  512         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_26 (Tensor [(None, 52, 52, 128) 0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_26 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_26[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_26 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_26[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_26[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_26[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 52, 52, 128)  512         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_27 (Tensor [(None, 52, 52, 128) 0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_27 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_27[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_27 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_27[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_27[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_6 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_AddV2_5[0][0]        \n",
            "                                                                 tf_op_layer_Mul_27[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 52, 52, 128)  512         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_28 (Tensor [(None, 52, 52, 128) 0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_28 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_28[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_28 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_28[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_28[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_28[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 52, 52, 128)  512         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_29 (Tensor [(None, 52, 52, 128) 0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_29 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_29[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_29 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_29[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_29[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_7 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_AddV2_6[0][0]        \n",
            "                                                                 tf_op_layer_Mul_29[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 52, 52, 128)  512         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_30 (Tensor [(None, 52, 52, 128) 0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_30 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_30[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_30 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_30[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_30[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_30[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 52, 52, 128)  512         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_31 (Tensor [(None, 52, 52, 128) 0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_31 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_31[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_31 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_31[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_31[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_8 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_AddV2_7[0][0]        \n",
            "                                                                 tf_op_layer_Mul_31[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 52, 52, 128)  512         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_32 (Tensor [(None, 52, 52, 128) 0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_32 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_32[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_32 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_32[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_32[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_32[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 52, 52, 128)  512         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_33 (Tensor [(None, 52, 52, 128) 0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_33 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_33[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_33 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_33[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_33[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_9 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_AddV2_8[0][0]        \n",
            "                                                                 tf_op_layer_Mul_33[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 52, 52, 128)  512         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_34 (Tensor [(None, 52, 52, 128) 0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_34 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_34[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_34 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_34[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_34[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_34[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 52, 52, 128)  512         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_35 (Tensor [(None, 52, 52, 128) 0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_35 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_35[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_35 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_35[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_35[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_10 (TensorFlo [(None, 52, 52, 128) 0           tf_op_layer_AddV2_9[0][0]        \n",
            "                                                                 tf_op_layer_Mul_35[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_10[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 52, 52, 128)  32768       tf_op_layer_Mul_17[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 52, 52, 128)  512         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 52, 52, 128)  512         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_36 (Tensor [(None, 52, 52, 128) 0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_18 (Tensor [(None, 52, 52, 128) 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_36 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_36[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_18 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_18[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_36 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_36[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_36[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_18 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_18[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_18[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2 (TensorFlo [(None, 52, 52, 256) 0           tf_op_layer_Mul_36[0][0]         \n",
            "                                                                 tf_op_layer_Mul_18[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 52, 52, 256)  65536       tf_op_layer_concat_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 52, 52, 256)  1024        conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_37 (Tensor [(None, 52, 52, 256) 0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_37 (TensorFlow [(None, 52, 52, 256) 0           tf_op_layer_Softplus_37[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_37 (TensorFlowO [(None, 52, 52, 256) 0           batch_normalization_37[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_37[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_3 (ZeroPadding2D (None, 53, 53, 256)  0           tf_op_layer_Mul_37[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 26, 26, 512)  1179648     zero_padding2d_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 26, 26, 512)  2048        conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_38 (Tensor [(None, 26, 26, 512) 0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_38 (TensorFlow [(None, 26, 26, 512) 0           tf_op_layer_Softplus_38[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_38 (TensorFlowO [(None, 26, 26, 512) 0           batch_normalization_38[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_38[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_Mul_38[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 26, 26, 256)  1024        conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_40 (Tensor [(None, 26, 26, 256) 0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_40 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_40[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_40 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_40[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_40[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_Mul_40[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 26, 26, 256)  1024        conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_41 (Tensor [(None, 26, 26, 256) 0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_41 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_41[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_41 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_41[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_41[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_41[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 26, 26, 256)  1024        conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_42 (Tensor [(None, 26, 26, 256) 0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_42 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_42[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_42 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_42[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_42[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_11 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_Mul_40[0][0]         \n",
            "                                                                 tf_op_layer_Mul_42[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 26, 26, 256)  1024        conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_43 (Tensor [(None, 26, 26, 256) 0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_43 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_43[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_43 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_43[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_43[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_43[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 26, 26, 256)  1024        conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_44 (Tensor [(None, 26, 26, 256) 0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_44 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_44[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_44 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_44[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_44[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_12 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_AddV2_11[0][0]       \n",
            "                                                                 tf_op_layer_Mul_44[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 26, 26, 256)  1024        conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_45 (Tensor [(None, 26, 26, 256) 0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_45 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_45[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_45 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_45[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_45[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_45[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 26, 26, 256)  1024        conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_46 (Tensor [(None, 26, 26, 256) 0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_46 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_46[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_46 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_46[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_46[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_13 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_AddV2_12[0][0]       \n",
            "                                                                 tf_op_layer_Mul_46[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 26, 26, 256)  1024        conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_47 (Tensor [(None, 26, 26, 256) 0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_47 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_47[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_47 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_47[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_47[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_47[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 26, 26, 256)  1024        conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_48 (Tensor [(None, 26, 26, 256) 0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_48 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_48[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_48 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_48[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_48[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_14 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_AddV2_13[0][0]       \n",
            "                                                                 tf_op_layer_Mul_48[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_14[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 26, 26, 256)  1024        conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_49 (Tensor [(None, 26, 26, 256) 0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_49 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_49[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_49 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_49[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_49[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_49[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 26, 26, 256)  1024        conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_50 (Tensor [(None, 26, 26, 256) 0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_50 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_50[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_50 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_50[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_50[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_15 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_AddV2_14[0][0]       \n",
            "                                                                 tf_op_layer_Mul_50[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 26, 26, 256)  1024        conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_51 (Tensor [(None, 26, 26, 256) 0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_51 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_51[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_51 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_51[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_51[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_51[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 26, 26, 256)  1024        conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_52 (Tensor [(None, 26, 26, 256) 0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_52 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_52[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_52 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_52[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_52[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_16 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_AddV2_15[0][0]       \n",
            "                                                                 tf_op_layer_Mul_52[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_16[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 26, 26, 256)  1024        conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_53 (Tensor [(None, 26, 26, 256) 0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_53 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_53[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_53 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_53[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_53[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_53[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 26, 26, 256)  1024        conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_54 (Tensor [(None, 26, 26, 256) 0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_54 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_54[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_54 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_54[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_54[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_17 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_AddV2_16[0][0]       \n",
            "                                                                 tf_op_layer_Mul_54[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_17[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 26, 26, 256)  1024        conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_55 (Tensor [(None, 26, 26, 256) 0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_55 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_55[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_55 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_55[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_55[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_55[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 26, 26, 256)  1024        conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_56 (Tensor [(None, 26, 26, 256) 0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_56 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_56[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_56 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_56[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_56[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_18 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_AddV2_17[0][0]       \n",
            "                                                                 tf_op_layer_Mul_56[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_18[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_Mul_38[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 26, 26, 256)  1024        conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 26, 26, 256)  1024        conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_57 (Tensor [(None, 26, 26, 256) 0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_39 (Tensor [(None, 26, 26, 256) 0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_57 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_57[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_39 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_39[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_57 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_57[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_57[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_39 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_39[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_39[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3 (TensorFlo [(None, 26, 26, 512) 0           tf_op_layer_Mul_57[0][0]         \n",
            "                                                                 tf_op_layer_Mul_39[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 26, 26, 512)  262144      tf_op_layer_concat_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 26, 26, 512)  2048        conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_58 (Tensor [(None, 26, 26, 512) 0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_58 (TensorFlow [(None, 26, 26, 512) 0           tf_op_layer_Softplus_58[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_58 (TensorFlowO [(None, 26, 26, 512) 0           batch_normalization_58[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_58[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_4 (ZeroPadding2D (None, 27, 27, 512)  0           tf_op_layer_Mul_58[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 13, 13, 1024) 4718592     zero_padding2d_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 13, 13, 1024) 4096        conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_59 (Tensor [(None, 13, 13, 1024 0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_59 (TensorFlow [(None, 13, 13, 1024 0           tf_op_layer_Softplus_59[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_59 (TensorFlowO [(None, 13, 13, 1024 0           batch_normalization_59[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_59[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 13, 13, 512)  524288      tf_op_layer_Mul_59[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 13, 13, 512)  2048        conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_61 (Tensor [(None, 13, 13, 512) 0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_61 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_61[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_61 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_61[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_61[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 13, 13, 512)  262144      tf_op_layer_Mul_61[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 13, 13, 512)  2048        conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_62 (Tensor [(None, 13, 13, 512) 0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_62 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_62[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_62 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_62[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_62[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 13, 13, 512)  2359296     tf_op_layer_Mul_62[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 13, 13, 512)  2048        conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_63 (Tensor [(None, 13, 13, 512) 0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_63 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_63[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_63 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_63[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_63[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_19 (TensorFlo [(None, 13, 13, 512) 0           tf_op_layer_Mul_61[0][0]         \n",
            "                                                                 tf_op_layer_Mul_63[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 13, 13, 512)  262144      tf_op_layer_AddV2_19[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 13, 13, 512)  2048        conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_64 (Tensor [(None, 13, 13, 512) 0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_64 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_64[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_64 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_64[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_64[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 13, 13, 512)  2359296     tf_op_layer_Mul_64[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 13, 13, 512)  2048        conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_65 (Tensor [(None, 13, 13, 512) 0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_65 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_65[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_65 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_65[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_65[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_20 (TensorFlo [(None, 13, 13, 512) 0           tf_op_layer_AddV2_19[0][0]       \n",
            "                                                                 tf_op_layer_Mul_65[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 13, 13, 512)  262144      tf_op_layer_AddV2_20[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 13, 13, 512)  2048        conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_66 (Tensor [(None, 13, 13, 512) 0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_66 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_66[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_66 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_66[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_66[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 13, 13, 512)  2359296     tf_op_layer_Mul_66[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 13, 13, 512)  2048        conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_67 (Tensor [(None, 13, 13, 512) 0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_67 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_67[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_67 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_67[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_67[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_21 (TensorFlo [(None, 13, 13, 512) 0           tf_op_layer_AddV2_20[0][0]       \n",
            "                                                                 tf_op_layer_Mul_67[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 13, 13, 512)  262144      tf_op_layer_AddV2_21[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 13, 13, 512)  2048        conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_68 (Tensor [(None, 13, 13, 512) 0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_68 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_68[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_68 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_68[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_68[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 13, 13, 512)  2359296     tf_op_layer_Mul_68[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 13, 13, 512)  2048        conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_69 (Tensor [(None, 13, 13, 512) 0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_69 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_69[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_69 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_69[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_69[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_22 (TensorFlo [(None, 13, 13, 512) 0           tf_op_layer_AddV2_21[0][0]       \n",
            "                                                                 tf_op_layer_Mul_69[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 13, 13, 512)  262144      tf_op_layer_AddV2_22[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 13, 13, 512)  524288      tf_op_layer_Mul_59[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 13, 13, 512)  2048        conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 13, 13, 512)  2048        conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_70 (Tensor [(None, 13, 13, 512) 0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_60 (Tensor [(None, 13, 13, 512) 0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_70 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_70[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_60 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_60[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_70 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_70[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_70[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_60 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_60[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_60[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_4 (TensorFlo [(None, 13, 13, 1024 0           tf_op_layer_Mul_70[0][0]         \n",
            "                                                                 tf_op_layer_Mul_60[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 13, 13, 1024) 1048576     tf_op_layer_concat_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 13, 13, 1024) 4096        conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softplus_71 (Tensor [(None, 13, 13, 1024 0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tanh_71 (TensorFlow [(None, 13, 13, 1024 0           tf_op_layer_Softplus_71[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_71 (TensorFlowO [(None, 13, 13, 1024 0           batch_normalization_71[0][0]     \n",
            "                                                                 tf_op_layer_Tanh_71[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 13, 13, 512)  524288      tf_op_layer_Mul_71[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 13, 13, 512)  2048        conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu (TensorFl [(None, 13, 13, 512) 0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 13, 13, 1024) 4718592     tf_op_layer_LeakyRelu[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 13, 13, 1024) 4096        conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_1 (Tensor [(None, 13, 13, 1024 0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 13, 13, 512)  524288      tf_op_layer_LeakyRelu_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 13, 13, 512)  2048        conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_2 (Tensor [(None, 13, 13, 512) 0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_MaxPool (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_LeakyRelu_2[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_MaxPool_1 (TensorFl [(None, 13, 13, 512) 0           tf_op_layer_LeakyRelu_2[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_MaxPool_2 (TensorFl [(None, 13, 13, 512) 0           tf_op_layer_LeakyRelu_2[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_5 (TensorFlo [(None, 13, 13, 2048 0           tf_op_layer_MaxPool[0][0]        \n",
            "                                                                 tf_op_layer_MaxPool_1[0][0]      \n",
            "                                                                 tf_op_layer_MaxPool_2[0][0]      \n",
            "                                                                 tf_op_layer_LeakyRelu_2[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 13, 13, 512)  1048576     tf_op_layer_concat_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 13, 13, 512)  2048        conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_3 (Tensor [(None, 13, 13, 512) 0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 13, 13, 1024) 4718592     tf_op_layer_LeakyRelu_3[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 13, 13, 1024) 4096        conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_4 (Tensor [(None, 13, 13, 1024 0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 13, 13, 512)  524288      tf_op_layer_LeakyRelu_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 13, 13, 512)  2048        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_5 (Tensor [(None, 13, 13, 512) 0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 13, 13, 256)  131072      tf_op_layer_LeakyRelu_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_Mul_58[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 13, 13, 256)  1024        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 26, 26, 256)  1024        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_6 (Tensor [(None, 13, 13, 256) 0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_7 (Tensor [(None, 26, 26, 256) 0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ResizeBilinear (Ten [(None, 26, 26, 256) 0           tf_op_layer_LeakyRelu_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_6 (TensorFlo [(None, 26, 26, 512) 0           tf_op_layer_LeakyRelu_7[0][0]    \n",
            "                                                                 tf_op_layer_ResizeBilinear[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_concat_6[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 26, 26, 256)  1024        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_8 (Tensor [(None, 26, 26, 256) 0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 26, 26, 512)  1179648     tf_op_layer_LeakyRelu_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 26, 26, 512)  2048        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_9 (Tensor [(None, 26, 26, 512) 0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_LeakyRelu_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 26, 26, 256)  1024        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_10 (Tenso [(None, 26, 26, 256) 0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 26, 26, 512)  1179648     tf_op_layer_LeakyRelu_10[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 26, 26, 512)  2048        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_11 (Tenso [(None, 26, 26, 512) 0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_LeakyRelu_11[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 26, 26, 256)  1024        conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_12 (Tenso [(None, 26, 26, 256) 0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 26, 26, 128)  32768       tf_op_layer_LeakyRelu_12[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 52, 52, 128)  32768       tf_op_layer_Mul_37[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 26, 26, 128)  512         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 52, 52, 128)  512         conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_13 (Tenso [(None, 26, 26, 128) 0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_14 (Tenso [(None, 52, 52, 128) 0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ResizeBilinear_1 (T [(None, 52, 52, 128) 0           tf_op_layer_LeakyRelu_13[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_7 (TensorFlo [(None, 52, 52, 256) 0           tf_op_layer_LeakyRelu_14[0][0]   \n",
            "                                                                 tf_op_layer_ResizeBilinear_1[0][0\n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 52, 52, 128)  32768       tf_op_layer_concat_7[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 52, 52, 128)  512         conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_15 (Tenso [(None, 52, 52, 128) 0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 52, 52, 256)  294912      tf_op_layer_LeakyRelu_15[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 52, 52, 256)  1024        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_16 (Tenso [(None, 52, 52, 256) 0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 52, 52, 128)  32768       tf_op_layer_LeakyRelu_16[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 52, 52, 128)  512         conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_17 (Tenso [(None, 52, 52, 128) 0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 52, 52, 256)  294912      tf_op_layer_LeakyRelu_17[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 52, 52, 256)  1024        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_18 (Tenso [(None, 52, 52, 256) 0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 52, 52, 128)  32768       tf_op_layer_LeakyRelu_18[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 52, 52, 128)  512         conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_19 (Tenso [(None, 52, 52, 128) 0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_5 (ZeroPadding2D (None, 53, 53, 128)  0           tf_op_layer_LeakyRelu_19[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 26, 26, 256)  294912      zero_padding2d_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 26, 26, 256)  1024        conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_21 (Tenso [(None, 26, 26, 256) 0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_8 (TensorFlo [(None, 26, 26, 512) 0           tf_op_layer_LeakyRelu_21[0][0]   \n",
            "                                                                 tf_op_layer_LeakyRelu_12[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_concat_8[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 26, 26, 256)  1024        conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_22 (Tenso [(None, 26, 26, 256) 0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 26, 26, 512)  1179648     tf_op_layer_LeakyRelu_22[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 26, 26, 512)  2048        conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_23 (Tenso [(None, 26, 26, 512) 0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_LeakyRelu_23[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 26, 26, 256)  1024        conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_24 (Tenso [(None, 26, 26, 256) 0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 26, 26, 512)  1179648     tf_op_layer_LeakyRelu_24[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 26, 26, 512)  2048        conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_25 (Tenso [(None, 26, 26, 512) 0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_LeakyRelu_25[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 26, 26, 256)  1024        conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_26 (Tenso [(None, 26, 26, 256) 0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_6 (ZeroPadding2D (None, 27, 27, 256)  0           tf_op_layer_LeakyRelu_26[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 13, 13, 512)  1179648     zero_padding2d_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 13, 13, 512)  2048        conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_28 (Tenso [(None, 13, 13, 512) 0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_9 (TensorFlo [(None, 13, 13, 1024 0           tf_op_layer_LeakyRelu_28[0][0]   \n",
            "                                                                 tf_op_layer_LeakyRelu_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 13, 13, 512)  524288      tf_op_layer_concat_9[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 13, 13, 512)  2048        conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_29 (Tenso [(None, 13, 13, 512) 0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 13, 13, 1024) 4718592     tf_op_layer_LeakyRelu_29[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 13, 13, 1024) 4096        conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_30 (Tenso [(None, 13, 13, 1024 0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 13, 13, 512)  524288      tf_op_layer_LeakyRelu_30[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 13, 13, 512)  2048        conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_31 (Tenso [(None, 13, 13, 512) 0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 13, 13, 1024) 4718592     tf_op_layer_LeakyRelu_31[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 13, 13, 1024) 4096        conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_32 (Tenso [(None, 13, 13, 1024 0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 13, 13, 512)  524288      tf_op_layer_LeakyRelu_32[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 13, 13, 512)  2048        conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_33 (Tenso [(None, 13, 13, 512) 0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 52, 52, 256)  294912      tf_op_layer_LeakyRelu_19[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 26, 26, 512)  1179648     tf_op_layer_LeakyRelu_26[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 13, 13, 1024) 4718592     tf_op_layer_LeakyRelu_33[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 52, 52, 256)  1024        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 26, 26, 512)  2048        conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 13, 13, 1024) 4096        conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_20 (Tenso [(None, 52, 52, 256) 0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_27 (Tenso [(None, 26, 26, 512) 0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_LeakyRelu_34 (Tenso [(None, 13, 13, 1024 0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 52, 52, 255)  65535       tf_op_layer_LeakyRelu_20[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 26, 26, 255)  130815      tf_op_layer_LeakyRelu_27[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 13, 13, 255)  261375      tf_op_layer_LeakyRelu_34[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape (TensorFlowOp [(4,)]               0           conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_1 (TensorFlow [(4,)]               0           conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_2 (TensorFlow [(4,)]               0           conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [()]                 0           tf_op_layer_Shape[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_2 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape/shape (Tens [(5,)]               0           tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_3/shape (Te [(5,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_6/shape (Te [(5,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape (TensorFlow [(None, 52, 52, 3, 8 0           conv2d_93[0][0]                  \n",
            "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_3 (TensorFl [(None, 26, 26, 3, 8 0           conv2d_101[0][0]                 \n",
            "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_6 (TensorFl [(None, 13, 13, 3, 8 0           conv2d_109[0][0]                 \n",
            "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split (TensorFlowOp [(None, 52, 52, 3, 2 0           tf_op_layer_Reshape[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split_1 (TensorFlow [(None, 26, 26, 3, 2 0           tf_op_layer_Reshape_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split_2 (TensorFlow [(None, 13, 13, 3, 2 0           tf_op_layer_Reshape_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sigmoid (TensorFlow [(None, 52, 52, 3, 2 0           tf_op_layer_split[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tile/multiples (Ten [(5,)]               0           tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sigmoid_3 (TensorFl [(None, 26, 26, 3, 2 0           tf_op_layer_split_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tile_1/multiples (T [(5,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sigmoid_6 (TensorFl [(None, 13, 13, 3, 2 0           tf_op_layer_split_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tile_2/multiples (T [(5,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_72 (TensorFlowO [(None, 52, 52, 3, 2 0           tf_op_layer_Sigmoid[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tile (TensorFlowOpL [(None, 52, 52, 3, 2 0           tf_op_layer_Tile/multiples[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_76 (TensorFlowO [(None, 26, 26, 3, 2 0           tf_op_layer_Sigmoid_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tile_1 (TensorFlowO [(None, 26, 26, 3, 2 0           tf_op_layer_Tile_1/multiples[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_80 (TensorFlowO [(None, 13, 13, 3, 2 0           tf_op_layer_Sigmoid_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Tile_2 (TensorFlowO [(None, 13, 13, 3, 2 0           tf_op_layer_Tile_2/multiples[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sub (TensorFlowOpLa [(None, 52, 52, 3, 2 0           tf_op_layer_Mul_72[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Cast (TensorFlowOpL [(None, 52, 52, 3, 2 0           tf_op_layer_Tile[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sub_1 (TensorFlowOp [(None, 26, 26, 3, 2 0           tf_op_layer_Mul_76[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Cast_1 (TensorFlowO [(None, 26, 26, 3, 2 0           tf_op_layer_Tile_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sub_2 (TensorFlowOp [(None, 13, 13, 3, 2 0           tf_op_layer_Mul_80[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Cast_2 (TensorFlowO [(None, 13, 13, 3, 2 0           tf_op_layer_Tile_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_23 (TensorFlo [(None, 52, 52, 3, 2 0           tf_op_layer_Sub[0][0]            \n",
            "                                                                 tf_op_layer_Cast[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Exp (TensorFlowOpLa [(None, 52, 52, 3, 2 0           tf_op_layer_split[0][1]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_24 (TensorFlo [(None, 26, 26, 3, 2 0           tf_op_layer_Sub_1[0][0]          \n",
            "                                                                 tf_op_layer_Cast_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Exp_1 (TensorFlowOp [(None, 26, 26, 3, 2 0           tf_op_layer_split_1[0][1]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_25 (TensorFlo [(None, 13, 13, 3, 2 0           tf_op_layer_Sub_2[0][0]          \n",
            "                                                                 tf_op_layer_Cast_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Exp_2 (TensorFlowOp [(None, 13, 13, 3, 2 0           tf_op_layer_split_2[0][1]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_73 (TensorFlowO [(None, 52, 52, 3, 2 0           tf_op_layer_AddV2_23[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_74 (TensorFlowO [(None, 52, 52, 3, 2 0           tf_op_layer_Exp[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_77 (TensorFlowO [(None, 26, 26, 3, 2 0           tf_op_layer_AddV2_24[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_78 (TensorFlowO [(None, 26, 26, 3, 2 0           tf_op_layer_Exp_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_81 (TensorFlowO [(None, 13, 13, 3, 2 0           tf_op_layer_AddV2_25[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_82 (TensorFlowO [(None, 13, 13, 3, 2 0           tf_op_layer_Exp_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_10 (TensorFl [(None, 52, 52, 3, 4 0           tf_op_layer_Mul_73[0][0]         \n",
            "                                                                 tf_op_layer_Mul_74[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_11 (TensorFl [(None, 26, 26, 3, 4 0           tf_op_layer_Mul_77[0][0]         \n",
            "                                                                 tf_op_layer_Mul_78[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_12 (TensorFl [(None, 13, 13, 3, 4 0           tf_op_layer_Mul_81[0][0]         \n",
            "                                                                 tf_op_layer_Mul_82[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sigmoid_1 (TensorFl [(None, 52, 52, 3, 1 0           tf_op_layer_split[0][2]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sigmoid_2 (TensorFl [(None, 52, 52, 3, 8 0           tf_op_layer_split[0][3]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sigmoid_4 (TensorFl [(None, 26, 26, 3, 1 0           tf_op_layer_split_1[0][2]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sigmoid_5 (TensorFl [(None, 26, 26, 3, 8 0           tf_op_layer_split_1[0][3]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sigmoid_7 (TensorFl [(None, 13, 13, 3, 1 0           tf_op_layer_split_2[0][2]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sigmoid_8 (TensorFl [(None, 13, 13, 3, 8 0           tf_op_layer_split_2[0][3]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_2 (TensorFl [(None, None, 4)]    0           tf_op_layer_concat_10[0][0]      \n",
            "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_5 (TensorFl [(None, None, 4)]    0           tf_op_layer_concat_11[0][0]      \n",
            "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_8 (TensorFl [(None, None, 4)]    0           tf_op_layer_concat_12[0][0]      \n",
            "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_75 (TensorFlowO [(None, 52, 52, 3, 8 0           tf_op_layer_Sigmoid_1[0][0]      \n",
            "                                                                 tf_op_layer_Sigmoid_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_79 (TensorFlowO [(None, 26, 26, 3, 8 0           tf_op_layer_Sigmoid_4[0][0]      \n",
            "                                                                 tf_op_layer_Sigmoid_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Mul_83 (TensorFlowO [(None, 13, 13, 3, 8 0           tf_op_layer_Sigmoid_7[0][0]      \n",
            "                                                                 tf_op_layer_Sigmoid_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_13 (TensorFl [(None, None, 4)]    0           tf_op_layer_Reshape_2[0][0]      \n",
            "                                                                 tf_op_layer_Reshape_5[0][0]      \n",
            "                                                                 tf_op_layer_Reshape_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_1 (TensorFl [(None, None, 80)]   0           tf_op_layer_Mul_75[0][0]         \n",
            "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_4 (TensorFl [(None, None, 80)]   0           tf_op_layer_Mul_79[0][0]         \n",
            "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_7 (TensorFl [(None, None, 80)]   0           tf_op_layer_Mul_83[0][0]         \n",
            "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_3 (TensorFlow [(3,)]               0           tf_op_layer_concat_13[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_14 (TensorFl [(None, None, 80)]   0           tf_op_layer_Reshape_1[0][0]      \n",
            "                                                                 tf_op_layer_Reshape_4[0][0]      \n",
            "                                                                 tf_op_layer_Reshape_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_3 (Te [(2,)]               0           tf_op_layer_Shape_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Max (TensorFlowOpLa [(None, None)]       0           tf_op_layer_concat_14[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_4 (TensorFlow [(3,)]               0           tf_op_layer_concat_13[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Prod (TensorFlowOpL [()]                 0           tf_op_layer_strided_slice_3[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_5 (TensorFlow [(3,)]               0           tf_op_layer_concat_13[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_GreaterEqual (Tenso [(None, None)]       0           tf_op_layer_Max[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_4 (Te [(0,)]               0           tf_op_layer_Shape_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_15/values_1  [(1,)]               0           tf_op_layer_Prod[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_5 (Te [(1,)]               0           tf_op_layer_Shape_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_10 (TensorF [(None,)]            0           tf_op_layer_GreaterEqual[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_15 (TensorFl [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
            "                                                                 tf_op_layer_concat_15/values_1[0]\n",
            "                                                                 tf_op_layer_strided_slice_5[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Where (TensorFlowOp [(None, 1)]          0           tf_op_layer_Reshape_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_9 (TensorFl [(None, 4)]          0           tf_op_layer_concat_13[0][0]      \n",
            "                                                                 tf_op_layer_concat_15[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze (TensorFlow [(None,)]            0           tf_op_layer_Where[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_GatherV2 (TensorFlo [(None, 4)]          0           tf_op_layer_Reshape_9[0][0]      \n",
            "                                                                 tf_op_layer_Squeeze[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_9 (TensorFlow [(3,)]               0           tf_op_layer_concat_14[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_10 (TensorFlo [(2,)]               0           tf_op_layer_GatherV2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_6 (TensorFlow [(3,)]               0           tf_op_layer_concat_14[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_10 (T [()]                 0           tf_op_layer_Shape_10[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_6 (Te [(2,)]               0           tf_op_layer_Shape_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_13/shape (T [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
            "                                                                 tf_op_layer_strided_slice_10[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_7 (TensorFlow [(3,)]               0           tf_op_layer_concat_14[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Prod_1 (TensorFlowO [()]                 0           tf_op_layer_strided_slice_6[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_8 (TensorFlow [(3,)]               0           tf_op_layer_concat_14[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_13 (TensorF [(None, None, None)] 0           tf_op_layer_GatherV2[0][0]       \n",
            "                                                                 tf_op_layer_Reshape_13/shape[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_7 (Te [(0,)]               0           tf_op_layer_Shape_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_16/values_1  [(1,)]               0           tf_op_layer_Prod_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_8 (Te [(1,)]               0           tf_op_layer_Shape_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_12 (TensorF [(None,)]            0           tf_op_layer_GreaterEqual[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split_3 (TensorFlow [(None, None, 2), (N 0           tf_op_layer_Reshape_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_16 (TensorFl [(2,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
            "                                                                 tf_op_layer_concat_16/values_1[0]\n",
            "                                                                 tf_op_layer_strided_slice_8[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Where_1 (TensorFlow [(None, 1)]          0           tf_op_layer_Reshape_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_14 (T [(None, None, 2)]    0           tf_op_layer_split_3[0][1]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_11 (TensorF [(None, 80)]         0           tf_op_layer_concat_14[0][0]      \n",
            "                                                                 tf_op_layer_concat_16[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze_1 (TensorFl [(None,)]            0           tf_op_layer_Where_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_13 (T [(None, None, 2)]    0           tf_op_layer_split_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_RealDiv (TensorFlow [(None, None, 2)]    0           tf_op_layer_strided_slice_14[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_RealDiv_2 (TensorFl [(None, None, 2)]    0           tf_op_layer_strided_slice_14[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_GatherV2_1 (TensorF [(None, 80)]         0           tf_op_layer_Reshape_11[0][0]     \n",
            "                                                                 tf_op_layer_Squeeze_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sub_3 (TensorFlowOp [(None, None, 2)]    0           tf_op_layer_strided_slice_13[0][0\n",
            "                                                                 tf_op_layer_RealDiv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_AddV2_26 (TensorFlo [(None, None, 2)]    0           tf_op_layer_strided_slice_13[0][0\n",
            "                                                                 tf_op_layer_RealDiv_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_11 (TensorFlo [(3,)]               0           tf_op_layer_concat_14[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape_12 (TensorFlo [(2,)]               0           tf_op_layer_GatherV2_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_RealDiv_1 (TensorFl [(None, None, 2)]    0           tf_op_layer_Sub_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_RealDiv_3 (TensorFl [(None, None, 2)]    0           tf_op_layer_AddV2_26[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_12 (T [()]                 0           tf_op_layer_Shape_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_15 (T [(None, None, 1)]    0           tf_op_layer_RealDiv_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_16 (T [(None, None, 1)]    0           tf_op_layer_RealDiv_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_17 (T [(None, None, 1)]    0           tf_op_layer_RealDiv_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_18 (T [(None, None, 1)]    0           tf_op_layer_RealDiv_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_14/shape (T [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
            "                                                                 tf_op_layer_strided_slice_12[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_17 (TensorFl [(None, None, 4)]    0           tf_op_layer_strided_slice_15[0][0\n",
            "                                                                 tf_op_layer_strided_slice_16[0][0\n",
            "                                                                 tf_op_layer_strided_slice_17[0][0\n",
            "                                                                 tf_op_layer_strided_slice_18[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Reshape_14 (TensorF [(None, None, None)] 0           tf_op_layer_GatherV2_1[0][0]     \n",
            "                                                                 tf_op_layer_Reshape_14/shape[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_18 (TensorFl [(None, None, None)] 0           tf_op_layer_concat_17[0][0]      \n",
            "                                                                 tf_op_layer_Reshape_14[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 64,429,405\n",
            "Trainable params: 64,363,101\n",
            "Non-trainable params: 66,304\n",
            "__________________________________________________________________________________________________\n",
            "2020-11-08 09:31:47.105416: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "INFO:tensorflow:Assets written to: ./checkpoints/yolov4-416/assets\n",
            "I1108 09:32:13.895797 139701700589440 builder_impl.py:775] Assets written to: ./checkpoints/yolov4-416/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daORdWIPtREF",
        "outputId": "60e9c022-2457-4bb5-fedb-30850ff952eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!gdown --id 1cewMfusmPjYWbrnuJRuKhPMwRe_b9PaT &\n",
        "!mv  yolov4.weights ./scripts/\n",
        "# then move the downloaded file into ./data/yolov4.weights"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cewMfusmPjYWbrnuJRuKhPMwRe_b9PaT\n",
            "To: /content/tensorflow-yolov4-tflite/yolov4.weights\n",
            "258MB [00:01, 209MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WosWH0ps_DD",
        "outputId": "4459f026-e22c-4d9d-d283-e1f526fe14bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install gdown"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQFuSZ61r25m",
        "outputId": "b9a1f0c1-7063-49f3-c8ba-a9989b387fdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd .."
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tensorflow-yolov4-tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLBuwt7Cm63x",
        "outputId": "9c64e2f4-51f3-44fd-a1b2-88817da1d047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python coco_annotation.py --coco_path ../coco "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ1Gy8pbmtvr",
        "outputId": "f9ececa3-5bc0-4324-b86a-d7fe0e0baadc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python coco_convert.py --input ../coco/annotations/annotations/instances_val2017.json --output val2017.pkl"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mKết quả truyền trực tuyến bị cắt bớt đến 5000 dòng cuối.\u001b[0m\n",
            "31782/36781 total: 86.41\n",
            "31783/36781 total: 86.41\n",
            "31784/36781 total: 86.41\n",
            "31785/36781 total: 86.42\n",
            "31786/36781 total: 86.42\n",
            "31787/36781 total: 86.42\n",
            "31788/36781 total: 86.43\n",
            "31789/36781 total: 86.43\n",
            "31790/36781 total: 86.43\n",
            "31791/36781 total: 86.43\n",
            "31792/36781 total: 86.44\n",
            "31793/36781 total: 86.44\n",
            "31794/36781 total: 86.44\n",
            "31795/36781 total: 86.44\n",
            "31796/36781 total: 86.45\n",
            "31797/36781 total: 86.45\n",
            "31798/36781 total: 86.45\n",
            "31799/36781 total: 86.45\n",
            "31800/36781 total: 86.46\n",
            "31801/36781 total: 86.46\n",
            "31802/36781 total: 86.46\n",
            "31803/36781 total: 86.47\n",
            "31804/36781 total: 86.47\n",
            "31805/36781 total: 86.47\n",
            "31806/36781 total: 86.47\n",
            "31807/36781 total: 86.48\n",
            "31808/36781 total: 86.48\n",
            "31809/36781 total: 86.48\n",
            "31810/36781 total: 86.48\n",
            "31811/36781 total: 86.49\n",
            "31812/36781 total: 86.49\n",
            "31813/36781 total: 86.49\n",
            "31814/36781 total: 86.5\n",
            "31815/36781 total: 86.5\n",
            "31816/36781 total: 86.5\n",
            "31817/36781 total: 86.5\n",
            "31818/36781 total: 86.51\n",
            "31819/36781 total: 86.51\n",
            "31820/36781 total: 86.51\n",
            "31821/36781 total: 86.51\n",
            "31822/36781 total: 86.52\n",
            "31823/36781 total: 86.52\n",
            "31824/36781 total: 86.52\n",
            "31825/36781 total: 86.53\n",
            "31826/36781 total: 86.53\n",
            "31827/36781 total: 86.53\n",
            "31828/36781 total: 86.53\n",
            "31829/36781 total: 86.54\n",
            "31830/36781 total: 86.54\n",
            "31831/36781 total: 86.54\n",
            "31832/36781 total: 86.54\n",
            "31833/36781 total: 86.55\n",
            "31834/36781 total: 86.55\n",
            "31835/36781 total: 86.55\n",
            "31836/36781 total: 86.56\n",
            "31837/36781 total: 86.56\n",
            "31838/36781 total: 86.56\n",
            "31839/36781 total: 86.56\n",
            "31840/36781 total: 86.57\n",
            "31841/36781 total: 86.57\n",
            "31842/36781 total: 86.57\n",
            "31843/36781 total: 86.57\n",
            "31844/36781 total: 86.58\n",
            "31845/36781 total: 86.58\n",
            "31846/36781 total: 86.58\n",
            "31847/36781 total: 86.59\n",
            "31848/36781 total: 86.59\n",
            "31849/36781 total: 86.59\n",
            "31850/36781 total: 86.59\n",
            "31851/36781 total: 86.6\n",
            "31852/36781 total: 86.6\n",
            "31853/36781 total: 86.6\n",
            "31854/36781 total: 86.6\n",
            "31855/36781 total: 86.61\n",
            "31856/36781 total: 86.61\n",
            "31857/36781 total: 86.61\n",
            "31858/36781 total: 86.62\n",
            "31859/36781 total: 86.62\n",
            "31860/36781 total: 86.62\n",
            "31861/36781 total: 86.62\n",
            "31862/36781 total: 86.63\n",
            "31863/36781 total: 86.63\n",
            "31864/36781 total: 86.63\n",
            "31865/36781 total: 86.63\n",
            "31866/36781 total: 86.64\n",
            "31867/36781 total: 86.64\n",
            "31868/36781 total: 86.64\n",
            "31869/36781 total: 86.65\n",
            "31870/36781 total: 86.65\n",
            "31871/36781 total: 86.65\n",
            "31872/36781 total: 86.65\n",
            "31873/36781 total: 86.66\n",
            "31874/36781 total: 86.66\n",
            "31875/36781 total: 86.66\n",
            "31876/36781 total: 86.66\n",
            "31877/36781 total: 86.67\n",
            "31878/36781 total: 86.67\n",
            "31879/36781 total: 86.67\n",
            "31880/36781 total: 86.68\n",
            "31881/36781 total: 86.68\n",
            "31882/36781 total: 86.68\n",
            "31883/36781 total: 86.68\n",
            "31884/36781 total: 86.69\n",
            "31885/36781 total: 86.69\n",
            "31886/36781 total: 86.69\n",
            "31887/36781 total: 86.69\n",
            "31888/36781 total: 86.7\n",
            "31889/36781 total: 86.7\n",
            "31890/36781 total: 86.7\n",
            "31891/36781 total: 86.71\n",
            "31892/36781 total: 86.71\n",
            "31893/36781 total: 86.71\n",
            "31894/36781 total: 86.71\n",
            "31895/36781 total: 86.72\n",
            "31896/36781 total: 86.72\n",
            "31897/36781 total: 86.72\n",
            "31898/36781 total: 86.72\n",
            "31899/36781 total: 86.73\n",
            "31900/36781 total: 86.73\n",
            "31901/36781 total: 86.73\n",
            "31902/36781 total: 86.73\n",
            "31903/36781 total: 86.74\n",
            "31904/36781 total: 86.74\n",
            "31905/36781 total: 86.74\n",
            "31906/36781 total: 86.75\n",
            "31907/36781 total: 86.75\n",
            "31908/36781 total: 86.75\n",
            "31909/36781 total: 86.75\n",
            "31910/36781 total: 86.76\n",
            "31911/36781 total: 86.76\n",
            "31912/36781 total: 86.76\n",
            "31913/36781 total: 86.76\n",
            "31914/36781 total: 86.77\n",
            "31915/36781 total: 86.77\n",
            "31916/36781 total: 86.77\n",
            "31917/36781 total: 86.78\n",
            "31918/36781 total: 86.78\n",
            "31919/36781 total: 86.78\n",
            "31920/36781 total: 86.78\n",
            "31921/36781 total: 86.79\n",
            "31922/36781 total: 86.79\n",
            "31923/36781 total: 86.79\n",
            "31924/36781 total: 86.79\n",
            "31925/36781 total: 86.8\n",
            "31926/36781 total: 86.8\n",
            "31927/36781 total: 86.8\n",
            "31928/36781 total: 86.81\n",
            "31929/36781 total: 86.81\n",
            "31930/36781 total: 86.81\n",
            "31931/36781 total: 86.81\n",
            "31932/36781 total: 86.82\n",
            "31933/36781 total: 86.82\n",
            "31934/36781 total: 86.82\n",
            "31935/36781 total: 86.82\n",
            "31936/36781 total: 86.83\n",
            "31937/36781 total: 86.83\n",
            "31938/36781 total: 86.83\n",
            "31939/36781 total: 86.84\n",
            "31940/36781 total: 86.84\n",
            "31941/36781 total: 86.84\n",
            "31942/36781 total: 86.84\n",
            "31943/36781 total: 86.85\n",
            "31944/36781 total: 86.85\n",
            "31945/36781 total: 86.85\n",
            "31946/36781 total: 86.85\n",
            "31947/36781 total: 86.86\n",
            "31948/36781 total: 86.86\n",
            "31949/36781 total: 86.86\n",
            "31950/36781 total: 86.87\n",
            "31951/36781 total: 86.87\n",
            "31952/36781 total: 86.87\n",
            "31953/36781 total: 86.87\n",
            "31954/36781 total: 86.88\n",
            "31955/36781 total: 86.88\n",
            "31956/36781 total: 86.88\n",
            "31957/36781 total: 86.88\n",
            "31958/36781 total: 86.89\n",
            "31959/36781 total: 86.89\n",
            "31960/36781 total: 86.89\n",
            "31961/36781 total: 86.9\n",
            "31962/36781 total: 86.9\n",
            "31963/36781 total: 86.9\n",
            "31964/36781 total: 86.9\n",
            "31965/36781 total: 86.91\n",
            "31966/36781 total: 86.91\n",
            "31967/36781 total: 86.91\n",
            "31968/36781 total: 86.91\n",
            "31969/36781 total: 86.92\n",
            "31970/36781 total: 86.92\n",
            "31971/36781 total: 86.92\n",
            "31972/36781 total: 86.93\n",
            "31973/36781 total: 86.93\n",
            "31974/36781 total: 86.93\n",
            "31975/36781 total: 86.93\n",
            "31976/36781 total: 86.94\n",
            "31977/36781 total: 86.94\n",
            "31978/36781 total: 86.94\n",
            "31979/36781 total: 86.94\n",
            "31980/36781 total: 86.95\n",
            "31981/36781 total: 86.95\n",
            "31982/36781 total: 86.95\n",
            "31983/36781 total: 86.96\n",
            "31984/36781 total: 86.96\n",
            "31985/36781 total: 86.96\n",
            "31986/36781 total: 86.96\n",
            "31987/36781 total: 86.97\n",
            "31988/36781 total: 86.97\n",
            "31989/36781 total: 86.97\n",
            "31990/36781 total: 86.97\n",
            "31991/36781 total: 86.98\n",
            "31992/36781 total: 86.98\n",
            "31993/36781 total: 86.98\n",
            "31994/36781 total: 86.99\n",
            "31995/36781 total: 86.99\n",
            "31996/36781 total: 86.99\n",
            "31997/36781 total: 86.99\n",
            "31998/36781 total: 87.0\n",
            "31999/36781 total: 87.0\n",
            "32000/36781 total: 87.0\n",
            "32001/36781 total: 87.0\n",
            "32002/36781 total: 87.01\n",
            "32003/36781 total: 87.01\n",
            "32004/36781 total: 87.01\n",
            "32005/36781 total: 87.02\n",
            "32006/36781 total: 87.02\n",
            "32007/36781 total: 87.02\n",
            "32008/36781 total: 87.02\n",
            "32009/36781 total: 87.03\n",
            "32010/36781 total: 87.03\n",
            "32011/36781 total: 87.03\n",
            "32012/36781 total: 87.03\n",
            "32013/36781 total: 87.04\n",
            "32014/36781 total: 87.04\n",
            "32015/36781 total: 87.04\n",
            "32016/36781 total: 87.04\n",
            "32017/36781 total: 87.05\n",
            "32018/36781 total: 87.05\n",
            "32019/36781 total: 87.05\n",
            "32020/36781 total: 87.06\n",
            "32021/36781 total: 87.06\n",
            "32022/36781 total: 87.06\n",
            "32023/36781 total: 87.06\n",
            "32024/36781 total: 87.07\n",
            "32025/36781 total: 87.07\n",
            "32026/36781 total: 87.07\n",
            "32027/36781 total: 87.07\n",
            "32028/36781 total: 87.08\n",
            "32029/36781 total: 87.08\n",
            "32030/36781 total: 87.08\n",
            "32031/36781 total: 87.09\n",
            "32032/36781 total: 87.09\n",
            "32033/36781 total: 87.09\n",
            "32034/36781 total: 87.09\n",
            "32035/36781 total: 87.1\n",
            "32036/36781 total: 87.1\n",
            "32037/36781 total: 87.1\n",
            "32038/36781 total: 87.1\n",
            "32039/36781 total: 87.11\n",
            "32040/36781 total: 87.11\n",
            "32041/36781 total: 87.11\n",
            "32042/36781 total: 87.12\n",
            "32043/36781 total: 87.12\n",
            "32044/36781 total: 87.12\n",
            "32045/36781 total: 87.12\n",
            "32046/36781 total: 87.13\n",
            "32047/36781 total: 87.13\n",
            "32048/36781 total: 87.13\n",
            "32049/36781 total: 87.13\n",
            "32050/36781 total: 87.14\n",
            "32051/36781 total: 87.14\n",
            "32052/36781 total: 87.14\n",
            "32053/36781 total: 87.15\n",
            "32054/36781 total: 87.15\n",
            "32055/36781 total: 87.15\n",
            "32056/36781 total: 87.15\n",
            "32057/36781 total: 87.16\n",
            "32058/36781 total: 87.16\n",
            "32059/36781 total: 87.16\n",
            "32060/36781 total: 87.16\n",
            "32061/36781 total: 87.17\n",
            "32062/36781 total: 87.17\n",
            "32063/36781 total: 87.17\n",
            "32064/36781 total: 87.18\n",
            "32065/36781 total: 87.18\n",
            "32066/36781 total: 87.18\n",
            "32067/36781 total: 87.18\n",
            "32068/36781 total: 87.19\n",
            "32069/36781 total: 87.19\n",
            "32070/36781 total: 87.19\n",
            "32071/36781 total: 87.19\n",
            "32072/36781 total: 87.2\n",
            "32073/36781 total: 87.2\n",
            "32074/36781 total: 87.2\n",
            "32075/36781 total: 87.21\n",
            "32076/36781 total: 87.21\n",
            "32077/36781 total: 87.21\n",
            "32078/36781 total: 87.21\n",
            "32079/36781 total: 87.22\n",
            "32080/36781 total: 87.22\n",
            "32081/36781 total: 87.22\n",
            "32082/36781 total: 87.22\n",
            "32083/36781 total: 87.23\n",
            "32084/36781 total: 87.23\n",
            "32085/36781 total: 87.23\n",
            "32086/36781 total: 87.24\n",
            "32087/36781 total: 87.24\n",
            "32088/36781 total: 87.24\n",
            "32089/36781 total: 87.24\n",
            "32090/36781 total: 87.25\n",
            "32091/36781 total: 87.25\n",
            "32092/36781 total: 87.25\n",
            "32093/36781 total: 87.25\n",
            "32094/36781 total: 87.26\n",
            "32095/36781 total: 87.26\n",
            "32096/36781 total: 87.26\n",
            "32097/36781 total: 87.27\n",
            "32098/36781 total: 87.27\n",
            "32099/36781 total: 87.27\n",
            "32100/36781 total: 87.27\n",
            "32101/36781 total: 87.28\n",
            "32102/36781 total: 87.28\n",
            "32103/36781 total: 87.28\n",
            "32104/36781 total: 87.28\n",
            "32105/36781 total: 87.29\n",
            "32106/36781 total: 87.29\n",
            "32107/36781 total: 87.29\n",
            "32108/36781 total: 87.3\n",
            "32109/36781 total: 87.3\n",
            "32110/36781 total: 87.3\n",
            "32111/36781 total: 87.3\n",
            "32112/36781 total: 87.31\n",
            "32113/36781 total: 87.31\n",
            "32114/36781 total: 87.31\n",
            "32115/36781 total: 87.31\n",
            "32116/36781 total: 87.32\n",
            "32117/36781 total: 87.32\n",
            "32118/36781 total: 87.32\n",
            "32119/36781 total: 87.32\n",
            "32120/36781 total: 87.33\n",
            "32121/36781 total: 87.33\n",
            "32122/36781 total: 87.33\n",
            "32123/36781 total: 87.34\n",
            "32124/36781 total: 87.34\n",
            "32125/36781 total: 87.34\n",
            "32126/36781 total: 87.34\n",
            "32127/36781 total: 87.35\n",
            "32128/36781 total: 87.35\n",
            "32129/36781 total: 87.35\n",
            "32130/36781 total: 87.35\n",
            "32131/36781 total: 87.36\n",
            "32132/36781 total: 87.36\n",
            "32133/36781 total: 87.36\n",
            "32134/36781 total: 87.37\n",
            "32135/36781 total: 87.37\n",
            "32136/36781 total: 87.37\n",
            "32137/36781 total: 87.37\n",
            "32138/36781 total: 87.38\n",
            "32139/36781 total: 87.38\n",
            "32140/36781 total: 87.38\n",
            "32141/36781 total: 87.38\n",
            "32142/36781 total: 87.39\n",
            "32143/36781 total: 87.39\n",
            "32144/36781 total: 87.39\n",
            "32145/36781 total: 87.4\n",
            "32146/36781 total: 87.4\n",
            "32147/36781 total: 87.4\n",
            "32148/36781 total: 87.4\n",
            "32149/36781 total: 87.41\n",
            "32150/36781 total: 87.41\n",
            "32151/36781 total: 87.41\n",
            "32152/36781 total: 87.41\n",
            "32153/36781 total: 87.42\n",
            "32154/36781 total: 87.42\n",
            "32155/36781 total: 87.42\n",
            "32156/36781 total: 87.43\n",
            "32157/36781 total: 87.43\n",
            "32158/36781 total: 87.43\n",
            "32159/36781 total: 87.43\n",
            "32160/36781 total: 87.44\n",
            "32161/36781 total: 87.44\n",
            "32162/36781 total: 87.44\n",
            "32163/36781 total: 87.44\n",
            "32164/36781 total: 87.45\n",
            "32165/36781 total: 87.45\n",
            "32166/36781 total: 87.45\n",
            "32167/36781 total: 87.46\n",
            "32168/36781 total: 87.46\n",
            "32169/36781 total: 87.46\n",
            "32170/36781 total: 87.46\n",
            "32171/36781 total: 87.47\n",
            "32172/36781 total: 87.47\n",
            "32173/36781 total: 87.47\n",
            "32174/36781 total: 87.47\n",
            "32175/36781 total: 87.48\n",
            "32176/36781 total: 87.48\n",
            "32177/36781 total: 87.48\n",
            "32178/36781 total: 87.49\n",
            "32179/36781 total: 87.49\n",
            "32180/36781 total: 87.49\n",
            "32181/36781 total: 87.49\n",
            "32182/36781 total: 87.5\n",
            "32183/36781 total: 87.5\n",
            "32184/36781 total: 87.5\n",
            "32185/36781 total: 87.5\n",
            "32186/36781 total: 87.51\n",
            "32187/36781 total: 87.51\n",
            "32188/36781 total: 87.51\n",
            "32189/36781 total: 87.52\n",
            "32190/36781 total: 87.52\n",
            "32191/36781 total: 87.52\n",
            "32192/36781 total: 87.52\n",
            "32193/36781 total: 87.53\n",
            "32194/36781 total: 87.53\n",
            "32195/36781 total: 87.53\n",
            "32196/36781 total: 87.53\n",
            "32197/36781 total: 87.54\n",
            "32198/36781 total: 87.54\n",
            "32199/36781 total: 87.54\n",
            "32200/36781 total: 87.55\n",
            "32201/36781 total: 87.55\n",
            "32202/36781 total: 87.55\n",
            "32203/36781 total: 87.55\n",
            "32204/36781 total: 87.56\n",
            "32205/36781 total: 87.56\n",
            "32206/36781 total: 87.56\n",
            "32207/36781 total: 87.56\n",
            "32208/36781 total: 87.57\n",
            "32209/36781 total: 87.57\n",
            "32210/36781 total: 87.57\n",
            "32211/36781 total: 87.58\n",
            "32212/36781 total: 87.58\n",
            "32213/36781 total: 87.58\n",
            "32214/36781 total: 87.58\n",
            "32215/36781 total: 87.59\n",
            "32216/36781 total: 87.59\n",
            "32217/36781 total: 87.59\n",
            "32218/36781 total: 87.59\n",
            "32219/36781 total: 87.6\n",
            "32220/36781 total: 87.6\n",
            "32221/36781 total: 87.6\n",
            "32222/36781 total: 87.61\n",
            "32223/36781 total: 87.61\n",
            "32224/36781 total: 87.61\n",
            "32225/36781 total: 87.61\n",
            "32226/36781 total: 87.62\n",
            "32227/36781 total: 87.62\n",
            "32228/36781 total: 87.62\n",
            "32229/36781 total: 87.62\n",
            "32230/36781 total: 87.63\n",
            "32231/36781 total: 87.63\n",
            "32232/36781 total: 87.63\n",
            "32233/36781 total: 87.63\n",
            "32234/36781 total: 87.64\n",
            "32235/36781 total: 87.64\n",
            "32236/36781 total: 87.64\n",
            "32237/36781 total: 87.65\n",
            "32238/36781 total: 87.65\n",
            "32239/36781 total: 87.65\n",
            "32240/36781 total: 87.65\n",
            "32241/36781 total: 87.66\n",
            "32242/36781 total: 87.66\n",
            "32243/36781 total: 87.66\n",
            "32244/36781 total: 87.66\n",
            "32245/36781 total: 87.67\n",
            "32246/36781 total: 87.67\n",
            "32247/36781 total: 87.67\n",
            "32248/36781 total: 87.68\n",
            "32249/36781 total: 87.68\n",
            "32250/36781 total: 87.68\n",
            "32251/36781 total: 87.68\n",
            "32252/36781 total: 87.69\n",
            "32253/36781 total: 87.69\n",
            "32254/36781 total: 87.69\n",
            "32255/36781 total: 87.69\n",
            "32256/36781 total: 87.7\n",
            "32257/36781 total: 87.7\n",
            "32258/36781 total: 87.7\n",
            "32259/36781 total: 87.71\n",
            "32260/36781 total: 87.71\n",
            "32261/36781 total: 87.71\n",
            "32262/36781 total: 87.71\n",
            "32263/36781 total: 87.72\n",
            "32264/36781 total: 87.72\n",
            "32265/36781 total: 87.72\n",
            "32266/36781 total: 87.72\n",
            "32267/36781 total: 87.73\n",
            "32268/36781 total: 87.73\n",
            "32269/36781 total: 87.73\n",
            "32270/36781 total: 87.74\n",
            "32271/36781 total: 87.74\n",
            "32272/36781 total: 87.74\n",
            "32273/36781 total: 87.74\n",
            "32274/36781 total: 87.75\n",
            "32275/36781 total: 87.75\n",
            "32276/36781 total: 87.75\n",
            "32277/36781 total: 87.75\n",
            "32278/36781 total: 87.76\n",
            "32279/36781 total: 87.76\n",
            "32280/36781 total: 87.76\n",
            "32281/36781 total: 87.77\n",
            "32282/36781 total: 87.77\n",
            "32283/36781 total: 87.77\n",
            "32284/36781 total: 87.77\n",
            "32285/36781 total: 87.78\n",
            "32286/36781 total: 87.78\n",
            "32287/36781 total: 87.78\n",
            "32288/36781 total: 87.78\n",
            "32289/36781 total: 87.79\n",
            "32290/36781 total: 87.79\n",
            "32291/36781 total: 87.79\n",
            "32292/36781 total: 87.8\n",
            "32293/36781 total: 87.8\n",
            "32294/36781 total: 87.8\n",
            "32295/36781 total: 87.8\n",
            "32296/36781 total: 87.81\n",
            "32297/36781 total: 87.81\n",
            "32298/36781 total: 87.81\n",
            "32299/36781 total: 87.81\n",
            "32300/36781 total: 87.82\n",
            "32301/36781 total: 87.82\n",
            "32302/36781 total: 87.82\n",
            "32303/36781 total: 87.83\n",
            "32304/36781 total: 87.83\n",
            "32305/36781 total: 87.83\n",
            "32306/36781 total: 87.83\n",
            "32307/36781 total: 87.84\n",
            "32308/36781 total: 87.84\n",
            "32309/36781 total: 87.84\n",
            "32310/36781 total: 87.84\n",
            "32311/36781 total: 87.85\n",
            "32312/36781 total: 87.85\n",
            "32313/36781 total: 87.85\n",
            "32314/36781 total: 87.86\n",
            "32315/36781 total: 87.86\n",
            "32316/36781 total: 87.86\n",
            "32317/36781 total: 87.86\n",
            "32318/36781 total: 87.87\n",
            "32319/36781 total: 87.87\n",
            "32320/36781 total: 87.87\n",
            "32321/36781 total: 87.87\n",
            "32322/36781 total: 87.88\n",
            "32323/36781 total: 87.88\n",
            "32324/36781 total: 87.88\n",
            "32325/36781 total: 87.89\n",
            "32326/36781 total: 87.89\n",
            "32327/36781 total: 87.89\n",
            "32328/36781 total: 87.89\n",
            "32329/36781 total: 87.9\n",
            "32330/36781 total: 87.9\n",
            "32331/36781 total: 87.9\n",
            "32332/36781 total: 87.9\n",
            "32333/36781 total: 87.91\n",
            "32334/36781 total: 87.91\n",
            "32335/36781 total: 87.91\n",
            "32336/36781 total: 87.91\n",
            "32337/36781 total: 87.92\n",
            "32338/36781 total: 87.92\n",
            "32339/36781 total: 87.92\n",
            "32340/36781 total: 87.93\n",
            "32341/36781 total: 87.93\n",
            "32342/36781 total: 87.93\n",
            "32343/36781 total: 87.93\n",
            "32344/36781 total: 87.94\n",
            "32345/36781 total: 87.94\n",
            "32346/36781 total: 87.94\n",
            "32347/36781 total: 87.94\n",
            "32348/36781 total: 87.95\n",
            "32349/36781 total: 87.95\n",
            "32350/36781 total: 87.95\n",
            "32351/36781 total: 87.96\n",
            "32352/36781 total: 87.96\n",
            "32353/36781 total: 87.96\n",
            "32354/36781 total: 87.96\n",
            "32355/36781 total: 87.97\n",
            "32356/36781 total: 87.97\n",
            "32357/36781 total: 87.97\n",
            "32358/36781 total: 87.97\n",
            "32359/36781 total: 87.98\n",
            "32360/36781 total: 87.98\n",
            "32361/36781 total: 87.98\n",
            "32362/36781 total: 87.99\n",
            "32363/36781 total: 87.99\n",
            "32364/36781 total: 87.99\n",
            "32365/36781 total: 87.99\n",
            "32366/36781 total: 88.0\n",
            "32367/36781 total: 88.0\n",
            "32368/36781 total: 88.0\n",
            "32369/36781 total: 88.0\n",
            "32370/36781 total: 88.01\n",
            "32371/36781 total: 88.01\n",
            "32372/36781 total: 88.01\n",
            "32373/36781 total: 88.02\n",
            "32374/36781 total: 88.02\n",
            "32375/36781 total: 88.02\n",
            "32376/36781 total: 88.02\n",
            "32377/36781 total: 88.03\n",
            "32378/36781 total: 88.03\n",
            "32379/36781 total: 88.03\n",
            "32380/36781 total: 88.03\n",
            "32381/36781 total: 88.04\n",
            "32382/36781 total: 88.04\n",
            "32383/36781 total: 88.04\n",
            "32384/36781 total: 88.05\n",
            "32385/36781 total: 88.05\n",
            "32386/36781 total: 88.05\n",
            "32387/36781 total: 88.05\n",
            "32388/36781 total: 88.06\n",
            "32389/36781 total: 88.06\n",
            "32390/36781 total: 88.06\n",
            "32391/36781 total: 88.06\n",
            "32392/36781 total: 88.07\n",
            "32393/36781 total: 88.07\n",
            "32394/36781 total: 88.07\n",
            "32395/36781 total: 88.08\n",
            "32396/36781 total: 88.08\n",
            "32397/36781 total: 88.08\n",
            "32398/36781 total: 88.08\n",
            "32399/36781 total: 88.09\n",
            "32400/36781 total: 88.09\n",
            "32401/36781 total: 88.09\n",
            "32402/36781 total: 88.09\n",
            "32403/36781 total: 88.1\n",
            "32404/36781 total: 88.1\n",
            "32405/36781 total: 88.1\n",
            "32406/36781 total: 88.11\n",
            "32407/36781 total: 88.11\n",
            "32408/36781 total: 88.11\n",
            "32409/36781 total: 88.11\n",
            "32410/36781 total: 88.12\n",
            "32411/36781 total: 88.12\n",
            "32412/36781 total: 88.12\n",
            "32413/36781 total: 88.12\n",
            "32414/36781 total: 88.13\n",
            "32415/36781 total: 88.13\n",
            "32416/36781 total: 88.13\n",
            "32417/36781 total: 88.14\n",
            "32418/36781 total: 88.14\n",
            "32419/36781 total: 88.14\n",
            "32420/36781 total: 88.14\n",
            "32421/36781 total: 88.15\n",
            "32422/36781 total: 88.15\n",
            "32423/36781 total: 88.15\n",
            "32424/36781 total: 88.15\n",
            "32425/36781 total: 88.16\n",
            "32426/36781 total: 88.16\n",
            "32427/36781 total: 88.16\n",
            "32428/36781 total: 88.17\n",
            "32429/36781 total: 88.17\n",
            "32430/36781 total: 88.17\n",
            "32431/36781 total: 88.17\n",
            "32432/36781 total: 88.18\n",
            "32433/36781 total: 88.18\n",
            "32434/36781 total: 88.18\n",
            "32435/36781 total: 88.18\n",
            "32436/36781 total: 88.19\n",
            "32437/36781 total: 88.19\n",
            "32438/36781 total: 88.19\n",
            "32439/36781 total: 88.19\n",
            "32440/36781 total: 88.2\n",
            "32441/36781 total: 88.2\n",
            "32442/36781 total: 88.2\n",
            "32443/36781 total: 88.21\n",
            "32444/36781 total: 88.21\n",
            "32445/36781 total: 88.21\n",
            "32446/36781 total: 88.21\n",
            "32447/36781 total: 88.22\n",
            "32448/36781 total: 88.22\n",
            "32449/36781 total: 88.22\n",
            "32450/36781 total: 88.22\n",
            "32451/36781 total: 88.23\n",
            "32452/36781 total: 88.23\n",
            "32453/36781 total: 88.23\n",
            "32454/36781 total: 88.24\n",
            "32455/36781 total: 88.24\n",
            "32456/36781 total: 88.24\n",
            "32457/36781 total: 88.24\n",
            "32458/36781 total: 88.25\n",
            "32459/36781 total: 88.25\n",
            "32460/36781 total: 88.25\n",
            "32461/36781 total: 88.25\n",
            "32462/36781 total: 88.26\n",
            "32463/36781 total: 88.26\n",
            "32464/36781 total: 88.26\n",
            "32465/36781 total: 88.27\n",
            "32466/36781 total: 88.27\n",
            "32467/36781 total: 88.27\n",
            "32468/36781 total: 88.27\n",
            "32469/36781 total: 88.28\n",
            "32470/36781 total: 88.28\n",
            "32471/36781 total: 88.28\n",
            "32472/36781 total: 88.28\n",
            "32473/36781 total: 88.29\n",
            "32474/36781 total: 88.29\n",
            "32475/36781 total: 88.29\n",
            "32476/36781 total: 88.3\n",
            "32477/36781 total: 88.3\n",
            "32478/36781 total: 88.3\n",
            "32479/36781 total: 88.3\n",
            "32480/36781 total: 88.31\n",
            "32481/36781 total: 88.31\n",
            "32482/36781 total: 88.31\n",
            "32483/36781 total: 88.31\n",
            "32484/36781 total: 88.32\n",
            "32485/36781 total: 88.32\n",
            "32486/36781 total: 88.32\n",
            "32487/36781 total: 88.33\n",
            "32488/36781 total: 88.33\n",
            "32489/36781 total: 88.33\n",
            "32490/36781 total: 88.33\n",
            "32491/36781 total: 88.34\n",
            "32492/36781 total: 88.34\n",
            "32493/36781 total: 88.34\n",
            "32494/36781 total: 88.34\n",
            "32495/36781 total: 88.35\n",
            "32496/36781 total: 88.35\n",
            "32497/36781 total: 88.35\n",
            "32498/36781 total: 88.36\n",
            "32499/36781 total: 88.36\n",
            "32500/36781 total: 88.36\n",
            "32501/36781 total: 88.36\n",
            "32502/36781 total: 88.37\n",
            "32503/36781 total: 88.37\n",
            "32504/36781 total: 88.37\n",
            "32505/36781 total: 88.37\n",
            "32506/36781 total: 88.38\n",
            "32507/36781 total: 88.38\n",
            "32508/36781 total: 88.38\n",
            "32509/36781 total: 88.39\n",
            "32510/36781 total: 88.39\n",
            "32511/36781 total: 88.39\n",
            "32512/36781 total: 88.39\n",
            "32513/36781 total: 88.4\n",
            "32514/36781 total: 88.4\n",
            "32515/36781 total: 88.4\n",
            "32516/36781 total: 88.4\n",
            "32517/36781 total: 88.41\n",
            "32518/36781 total: 88.41\n",
            "32519/36781 total: 88.41\n",
            "32520/36781 total: 88.42\n",
            "32521/36781 total: 88.42\n",
            "32522/36781 total: 88.42\n",
            "32523/36781 total: 88.42\n",
            "32524/36781 total: 88.43\n",
            "32525/36781 total: 88.43\n",
            "32526/36781 total: 88.43\n",
            "32527/36781 total: 88.43\n",
            "32528/36781 total: 88.44\n",
            "32529/36781 total: 88.44\n",
            "32530/36781 total: 88.44\n",
            "32531/36781 total: 88.45\n",
            "32532/36781 total: 88.45\n",
            "32533/36781 total: 88.45\n",
            "32534/36781 total: 88.45\n",
            "32535/36781 total: 88.46\n",
            "32536/36781 total: 88.46\n",
            "32537/36781 total: 88.46\n",
            "32538/36781 total: 88.46\n",
            "32539/36781 total: 88.47\n",
            "32540/36781 total: 88.47\n",
            "32541/36781 total: 88.47\n",
            "32542/36781 total: 88.48\n",
            "32543/36781 total: 88.48\n",
            "32544/36781 total: 88.48\n",
            "32545/36781 total: 88.48\n",
            "32546/36781 total: 88.49\n",
            "32547/36781 total: 88.49\n",
            "32548/36781 total: 88.49\n",
            "32549/36781 total: 88.49\n",
            "32550/36781 total: 88.5\n",
            "32551/36781 total: 88.5\n",
            "32552/36781 total: 88.5\n",
            "32553/36781 total: 88.5\n",
            "32554/36781 total: 88.51\n",
            "32555/36781 total: 88.51\n",
            "32556/36781 total: 88.51\n",
            "32557/36781 total: 88.52\n",
            "32558/36781 total: 88.52\n",
            "32559/36781 total: 88.52\n",
            "32560/36781 total: 88.52\n",
            "32561/36781 total: 88.53\n",
            "32562/36781 total: 88.53\n",
            "32563/36781 total: 88.53\n",
            "32564/36781 total: 88.53\n",
            "32565/36781 total: 88.54\n",
            "32566/36781 total: 88.54\n",
            "32567/36781 total: 88.54\n",
            "32568/36781 total: 88.55\n",
            "32569/36781 total: 88.55\n",
            "32570/36781 total: 88.55\n",
            "32571/36781 total: 88.55\n",
            "32572/36781 total: 88.56\n",
            "32573/36781 total: 88.56\n",
            "32574/36781 total: 88.56\n",
            "32575/36781 total: 88.56\n",
            "32576/36781 total: 88.57\n",
            "32577/36781 total: 88.57\n",
            "32578/36781 total: 88.57\n",
            "32579/36781 total: 88.58\n",
            "32580/36781 total: 88.58\n",
            "32581/36781 total: 88.58\n",
            "32582/36781 total: 88.58\n",
            "32583/36781 total: 88.59\n",
            "32584/36781 total: 88.59\n",
            "32585/36781 total: 88.59\n",
            "32586/36781 total: 88.59\n",
            "32587/36781 total: 88.6\n",
            "32588/36781 total: 88.6\n",
            "32589/36781 total: 88.6\n",
            "32590/36781 total: 88.61\n",
            "32591/36781 total: 88.61\n",
            "32592/36781 total: 88.61\n",
            "32593/36781 total: 88.61\n",
            "32594/36781 total: 88.62\n",
            "32595/36781 total: 88.62\n",
            "32596/36781 total: 88.62\n",
            "32597/36781 total: 88.62\n",
            "32598/36781 total: 88.63\n",
            "32599/36781 total: 88.63\n",
            "32600/36781 total: 88.63\n",
            "32601/36781 total: 88.64\n",
            "32602/36781 total: 88.64\n",
            "32603/36781 total: 88.64\n",
            "32604/36781 total: 88.64\n",
            "32605/36781 total: 88.65\n",
            "32606/36781 total: 88.65\n",
            "32607/36781 total: 88.65\n",
            "32608/36781 total: 88.65\n",
            "32609/36781 total: 88.66\n",
            "32610/36781 total: 88.66\n",
            "32611/36781 total: 88.66\n",
            "32612/36781 total: 88.67\n",
            "32613/36781 total: 88.67\n",
            "32614/36781 total: 88.67\n",
            "32615/36781 total: 88.67\n",
            "32616/36781 total: 88.68\n",
            "32617/36781 total: 88.68\n",
            "32618/36781 total: 88.68\n",
            "32619/36781 total: 88.68\n",
            "32620/36781 total: 88.69\n",
            "32621/36781 total: 88.69\n",
            "32622/36781 total: 88.69\n",
            "32623/36781 total: 88.7\n",
            "32624/36781 total: 88.7\n",
            "32625/36781 total: 88.7\n",
            "32626/36781 total: 88.7\n",
            "32627/36781 total: 88.71\n",
            "32628/36781 total: 88.71\n",
            "32629/36781 total: 88.71\n",
            "32630/36781 total: 88.71\n",
            "32631/36781 total: 88.72\n",
            "32632/36781 total: 88.72\n",
            "32633/36781 total: 88.72\n",
            "32634/36781 total: 88.73\n",
            "32635/36781 total: 88.73\n",
            "32636/36781 total: 88.73\n",
            "32637/36781 total: 88.73\n",
            "32638/36781 total: 88.74\n",
            "32639/36781 total: 88.74\n",
            "32640/36781 total: 88.74\n",
            "32641/36781 total: 88.74\n",
            "32642/36781 total: 88.75\n",
            "32643/36781 total: 88.75\n",
            "32644/36781 total: 88.75\n",
            "32645/36781 total: 88.76\n",
            "32646/36781 total: 88.76\n",
            "32647/36781 total: 88.76\n",
            "32648/36781 total: 88.76\n",
            "32649/36781 total: 88.77\n",
            "32650/36781 total: 88.77\n",
            "32651/36781 total: 88.77\n",
            "32652/36781 total: 88.77\n",
            "32653/36781 total: 88.78\n",
            "32654/36781 total: 88.78\n",
            "32655/36781 total: 88.78\n",
            "32656/36781 total: 88.78\n",
            "32657/36781 total: 88.79\n",
            "32658/36781 total: 88.79\n",
            "32659/36781 total: 88.79\n",
            "32660/36781 total: 88.8\n",
            "32661/36781 total: 88.8\n",
            "32662/36781 total: 88.8\n",
            "32663/36781 total: 88.8\n",
            "32664/36781 total: 88.81\n",
            "32665/36781 total: 88.81\n",
            "32666/36781 total: 88.81\n",
            "32667/36781 total: 88.81\n",
            "32668/36781 total: 88.82\n",
            "32669/36781 total: 88.82\n",
            "32670/36781 total: 88.82\n",
            "32671/36781 total: 88.83\n",
            "32672/36781 total: 88.83\n",
            "32673/36781 total: 88.83\n",
            "32674/36781 total: 88.83\n",
            "32675/36781 total: 88.84\n",
            "32676/36781 total: 88.84\n",
            "32677/36781 total: 88.84\n",
            "32678/36781 total: 88.84\n",
            "32679/36781 total: 88.85\n",
            "32680/36781 total: 88.85\n",
            "32681/36781 total: 88.85\n",
            "32682/36781 total: 88.86\n",
            "32683/36781 total: 88.86\n",
            "32684/36781 total: 88.86\n",
            "32685/36781 total: 88.86\n",
            "32686/36781 total: 88.87\n",
            "32687/36781 total: 88.87\n",
            "32688/36781 total: 88.87\n",
            "32689/36781 total: 88.87\n",
            "32690/36781 total: 88.88\n",
            "32691/36781 total: 88.88\n",
            "32692/36781 total: 88.88\n",
            "32693/36781 total: 88.89\n",
            "32694/36781 total: 88.89\n",
            "32695/36781 total: 88.89\n",
            "32696/36781 total: 88.89\n",
            "32697/36781 total: 88.9\n",
            "32698/36781 total: 88.9\n",
            "32699/36781 total: 88.9\n",
            "32700/36781 total: 88.9\n",
            "32701/36781 total: 88.91\n",
            "32702/36781 total: 88.91\n",
            "32703/36781 total: 88.91\n",
            "32704/36781 total: 88.92\n",
            "32705/36781 total: 88.92\n",
            "32706/36781 total: 88.92\n",
            "32707/36781 total: 88.92\n",
            "32708/36781 total: 88.93\n",
            "32709/36781 total: 88.93\n",
            "32710/36781 total: 88.93\n",
            "32711/36781 total: 88.93\n",
            "32712/36781 total: 88.94\n",
            "32713/36781 total: 88.94\n",
            "32714/36781 total: 88.94\n",
            "32715/36781 total: 88.95\n",
            "32716/36781 total: 88.95\n",
            "32717/36781 total: 88.95\n",
            "32718/36781 total: 88.95\n",
            "32719/36781 total: 88.96\n",
            "32720/36781 total: 88.96\n",
            "32721/36781 total: 88.96\n",
            "32722/36781 total: 88.96\n",
            "32723/36781 total: 88.97\n",
            "32724/36781 total: 88.97\n",
            "32725/36781 total: 88.97\n",
            "32726/36781 total: 88.98\n",
            "32727/36781 total: 88.98\n",
            "32728/36781 total: 88.98\n",
            "32729/36781 total: 88.98\n",
            "32730/36781 total: 88.99\n",
            "32731/36781 total: 88.99\n",
            "32732/36781 total: 88.99\n",
            "32733/36781 total: 88.99\n",
            "32734/36781 total: 89.0\n",
            "32735/36781 total: 89.0\n",
            "32736/36781 total: 89.0\n",
            "32737/36781 total: 89.01\n",
            "32738/36781 total: 89.01\n",
            "32739/36781 total: 89.01\n",
            "32740/36781 total: 89.01\n",
            "32741/36781 total: 89.02\n",
            "32742/36781 total: 89.02\n",
            "32743/36781 total: 89.02\n",
            "32744/36781 total: 89.02\n",
            "32745/36781 total: 89.03\n",
            "32746/36781 total: 89.03\n",
            "32747/36781 total: 89.03\n",
            "32748/36781 total: 89.04\n",
            "32749/36781 total: 89.04\n",
            "32750/36781 total: 89.04\n",
            "32751/36781 total: 89.04\n",
            "32752/36781 total: 89.05\n",
            "32753/36781 total: 89.05\n",
            "32754/36781 total: 89.05\n",
            "32755/36781 total: 89.05\n",
            "32756/36781 total: 89.06\n",
            "32757/36781 total: 89.06\n",
            "32758/36781 total: 89.06\n",
            "32759/36781 total: 89.07\n",
            "32760/36781 total: 89.07\n",
            "32761/36781 total: 89.07\n",
            "32762/36781 total: 89.07\n",
            "32763/36781 total: 89.08\n",
            "32764/36781 total: 89.08\n",
            "32765/36781 total: 89.08\n",
            "32766/36781 total: 89.08\n",
            "32767/36781 total: 89.09\n",
            "32768/36781 total: 89.09\n",
            "32769/36781 total: 89.09\n",
            "32770/36781 total: 89.09\n",
            "32771/36781 total: 89.1\n",
            "32772/36781 total: 89.1\n",
            "32773/36781 total: 89.1\n",
            "32774/36781 total: 89.11\n",
            "32775/36781 total: 89.11\n",
            "32776/36781 total: 89.11\n",
            "32777/36781 total: 89.11\n",
            "32778/36781 total: 89.12\n",
            "32779/36781 total: 89.12\n",
            "32780/36781 total: 89.12\n",
            "32781/36781 total: 89.12\n",
            "32782/36781 total: 89.13\n",
            "32783/36781 total: 89.13\n",
            "32784/36781 total: 89.13\n",
            "32785/36781 total: 89.14\n",
            "32786/36781 total: 89.14\n",
            "32787/36781 total: 89.14\n",
            "32788/36781 total: 89.14\n",
            "32789/36781 total: 89.15\n",
            "32790/36781 total: 89.15\n",
            "32791/36781 total: 89.15\n",
            "32792/36781 total: 89.15\n",
            "32793/36781 total: 89.16\n",
            "32794/36781 total: 89.16\n",
            "32795/36781 total: 89.16\n",
            "32796/36781 total: 89.17\n",
            "32797/36781 total: 89.17\n",
            "32798/36781 total: 89.17\n",
            "32799/36781 total: 89.17\n",
            "32800/36781 total: 89.18\n",
            "32801/36781 total: 89.18\n",
            "32802/36781 total: 89.18\n",
            "32803/36781 total: 89.18\n",
            "32804/36781 total: 89.19\n",
            "32805/36781 total: 89.19\n",
            "32806/36781 total: 89.19\n",
            "32807/36781 total: 89.2\n",
            "32808/36781 total: 89.2\n",
            "32809/36781 total: 89.2\n",
            "32810/36781 total: 89.2\n",
            "32811/36781 total: 89.21\n",
            "32812/36781 total: 89.21\n",
            "32813/36781 total: 89.21\n",
            "32814/36781 total: 89.21\n",
            "32815/36781 total: 89.22\n",
            "32816/36781 total: 89.22\n",
            "32817/36781 total: 89.22\n",
            "32818/36781 total: 89.23\n",
            "32819/36781 total: 89.23\n",
            "32820/36781 total: 89.23\n",
            "32821/36781 total: 89.23\n",
            "32822/36781 total: 89.24\n",
            "32823/36781 total: 89.24\n",
            "32824/36781 total: 89.24\n",
            "32825/36781 total: 89.24\n",
            "32826/36781 total: 89.25\n",
            "32827/36781 total: 89.25\n",
            "32828/36781 total: 89.25\n",
            "32829/36781 total: 89.26\n",
            "32830/36781 total: 89.26\n",
            "32831/36781 total: 89.26\n",
            "32832/36781 total: 89.26\n",
            "32833/36781 total: 89.27\n",
            "32834/36781 total: 89.27\n",
            "32835/36781 total: 89.27\n",
            "32836/36781 total: 89.27\n",
            "32837/36781 total: 89.28\n",
            "32838/36781 total: 89.28\n",
            "32839/36781 total: 89.28\n",
            "32840/36781 total: 89.29\n",
            "32841/36781 total: 89.29\n",
            "32842/36781 total: 89.29\n",
            "32843/36781 total: 89.29\n",
            "32844/36781 total: 89.3\n",
            "32845/36781 total: 89.3\n",
            "32846/36781 total: 89.3\n",
            "32847/36781 total: 89.3\n",
            "32848/36781 total: 89.31\n",
            "32849/36781 total: 89.31\n",
            "32850/36781 total: 89.31\n",
            "32851/36781 total: 89.32\n",
            "32852/36781 total: 89.32\n",
            "32853/36781 total: 89.32\n",
            "32854/36781 total: 89.32\n",
            "32855/36781 total: 89.33\n",
            "32856/36781 total: 89.33\n",
            "32857/36781 total: 89.33\n",
            "32858/36781 total: 89.33\n",
            "32859/36781 total: 89.34\n",
            "32860/36781 total: 89.34\n",
            "32861/36781 total: 89.34\n",
            "32862/36781 total: 89.35\n",
            "32863/36781 total: 89.35\n",
            "32864/36781 total: 89.35\n",
            "32865/36781 total: 89.35\n",
            "32866/36781 total: 89.36\n",
            "32867/36781 total: 89.36\n",
            "32868/36781 total: 89.36\n",
            "32869/36781 total: 89.36\n",
            "32870/36781 total: 89.37\n",
            "32871/36781 total: 89.37\n",
            "32872/36781 total: 89.37\n",
            "32873/36781 total: 89.37\n",
            "32874/36781 total: 89.38\n",
            "32875/36781 total: 89.38\n",
            "32876/36781 total: 89.38\n",
            "32877/36781 total: 89.39\n",
            "32878/36781 total: 89.39\n",
            "32879/36781 total: 89.39\n",
            "32880/36781 total: 89.39\n",
            "32881/36781 total: 89.4\n",
            "32882/36781 total: 89.4\n",
            "32883/36781 total: 89.4\n",
            "32884/36781 total: 89.4\n",
            "32885/36781 total: 89.41\n",
            "32886/36781 total: 89.41\n",
            "32887/36781 total: 89.41\n",
            "32888/36781 total: 89.42\n",
            "32889/36781 total: 89.42\n",
            "32890/36781 total: 89.42\n",
            "32891/36781 total: 89.42\n",
            "32892/36781 total: 89.43\n",
            "32893/36781 total: 89.43\n",
            "32894/36781 total: 89.43\n",
            "32895/36781 total: 89.43\n",
            "32896/36781 total: 89.44\n",
            "32897/36781 total: 89.44\n",
            "32898/36781 total: 89.44\n",
            "32899/36781 total: 89.45\n",
            "32900/36781 total: 89.45\n",
            "32901/36781 total: 89.45\n",
            "32902/36781 total: 89.45\n",
            "32903/36781 total: 89.46\n",
            "32904/36781 total: 89.46\n",
            "32905/36781 total: 89.46\n",
            "32906/36781 total: 89.46\n",
            "32907/36781 total: 89.47\n",
            "32908/36781 total: 89.47\n",
            "32909/36781 total: 89.47\n",
            "32910/36781 total: 89.48\n",
            "32911/36781 total: 89.48\n",
            "32912/36781 total: 89.48\n",
            "32913/36781 total: 89.48\n",
            "32914/36781 total: 89.49\n",
            "32915/36781 total: 89.49\n",
            "32916/36781 total: 89.49\n",
            "32917/36781 total: 89.49\n",
            "32918/36781 total: 89.5\n",
            "32919/36781 total: 89.5\n",
            "32920/36781 total: 89.5\n",
            "32921/36781 total: 89.51\n",
            "32922/36781 total: 89.51\n",
            "32923/36781 total: 89.51\n",
            "32924/36781 total: 89.51\n",
            "32925/36781 total: 89.52\n",
            "32926/36781 total: 89.52\n",
            "32927/36781 total: 89.52\n",
            "32928/36781 total: 89.52\n",
            "32929/36781 total: 89.53\n",
            "32930/36781 total: 89.53\n",
            "32931/36781 total: 89.53\n",
            "32932/36781 total: 89.54\n",
            "32933/36781 total: 89.54\n",
            "32934/36781 total: 89.54\n",
            "32935/36781 total: 89.54\n",
            "32936/36781 total: 89.55\n",
            "32937/36781 total: 89.55\n",
            "32938/36781 total: 89.55\n",
            "32939/36781 total: 89.55\n",
            "32940/36781 total: 89.56\n",
            "32941/36781 total: 89.56\n",
            "32942/36781 total: 89.56\n",
            "32943/36781 total: 89.57\n",
            "32944/36781 total: 89.57\n",
            "32945/36781 total: 89.57\n",
            "32946/36781 total: 89.57\n",
            "32947/36781 total: 89.58\n",
            "32948/36781 total: 89.58\n",
            "32949/36781 total: 89.58\n",
            "32950/36781 total: 89.58\n",
            "32951/36781 total: 89.59\n",
            "32952/36781 total: 89.59\n",
            "32953/36781 total: 89.59\n",
            "32954/36781 total: 89.6\n",
            "32955/36781 total: 89.6\n",
            "32956/36781 total: 89.6\n",
            "32957/36781 total: 89.6\n",
            "32958/36781 total: 89.61\n",
            "32959/36781 total: 89.61\n",
            "32960/36781 total: 89.61\n",
            "32961/36781 total: 89.61\n",
            "32962/36781 total: 89.62\n",
            "32963/36781 total: 89.62\n",
            "32964/36781 total: 89.62\n",
            "32965/36781 total: 89.63\n",
            "32966/36781 total: 89.63\n",
            "32967/36781 total: 89.63\n",
            "32968/36781 total: 89.63\n",
            "32969/36781 total: 89.64\n",
            "32970/36781 total: 89.64\n",
            "32971/36781 total: 89.64\n",
            "32972/36781 total: 89.64\n",
            "32973/36781 total: 89.65\n",
            "32974/36781 total: 89.65\n",
            "32975/36781 total: 89.65\n",
            "32976/36781 total: 89.65\n",
            "32977/36781 total: 89.66\n",
            "32978/36781 total: 89.66\n",
            "32979/36781 total: 89.66\n",
            "32980/36781 total: 89.67\n",
            "32981/36781 total: 89.67\n",
            "32982/36781 total: 89.67\n",
            "32983/36781 total: 89.67\n",
            "32984/36781 total: 89.68\n",
            "32985/36781 total: 89.68\n",
            "32986/36781 total: 89.68\n",
            "32987/36781 total: 89.68\n",
            "32988/36781 total: 89.69\n",
            "32989/36781 total: 89.69\n",
            "32990/36781 total: 89.69\n",
            "32991/36781 total: 89.7\n",
            "32992/36781 total: 89.7\n",
            "32993/36781 total: 89.7\n",
            "32994/36781 total: 89.7\n",
            "32995/36781 total: 89.71\n",
            "32996/36781 total: 89.71\n",
            "32997/36781 total: 89.71\n",
            "32998/36781 total: 89.71\n",
            "32999/36781 total: 89.72\n",
            "33000/36781 total: 89.72\n",
            "33001/36781 total: 89.72\n",
            "33002/36781 total: 89.73\n",
            "33003/36781 total: 89.73\n",
            "33004/36781 total: 89.73\n",
            "33005/36781 total: 89.73\n",
            "33006/36781 total: 89.74\n",
            "33007/36781 total: 89.74\n",
            "33008/36781 total: 89.74\n",
            "33009/36781 total: 89.74\n",
            "33010/36781 total: 89.75\n",
            "33011/36781 total: 89.75\n",
            "33012/36781 total: 89.75\n",
            "33013/36781 total: 89.76\n",
            "33014/36781 total: 89.76\n",
            "33015/36781 total: 89.76\n",
            "33016/36781 total: 89.76\n",
            "33017/36781 total: 89.77\n",
            "33018/36781 total: 89.77\n",
            "33019/36781 total: 89.77\n",
            "33020/36781 total: 89.77\n",
            "33021/36781 total: 89.78\n",
            "33022/36781 total: 89.78\n",
            "33023/36781 total: 89.78\n",
            "33024/36781 total: 89.79\n",
            "33025/36781 total: 89.79\n",
            "33026/36781 total: 89.79\n",
            "33027/36781 total: 89.79\n",
            "33028/36781 total: 89.8\n",
            "33029/36781 total: 89.8\n",
            "33030/36781 total: 89.8\n",
            "33031/36781 total: 89.8\n",
            "33032/36781 total: 89.81\n",
            "33033/36781 total: 89.81\n",
            "33034/36781 total: 89.81\n",
            "33035/36781 total: 89.82\n",
            "33036/36781 total: 89.82\n",
            "33037/36781 total: 89.82\n",
            "33038/36781 total: 89.82\n",
            "33039/36781 total: 89.83\n",
            "33040/36781 total: 89.83\n",
            "33041/36781 total: 89.83\n",
            "33042/36781 total: 89.83\n",
            "33043/36781 total: 89.84\n",
            "33044/36781 total: 89.84\n",
            "33045/36781 total: 89.84\n",
            "33046/36781 total: 89.85\n",
            "33047/36781 total: 89.85\n",
            "33048/36781 total: 89.85\n",
            "33049/36781 total: 89.85\n",
            "33050/36781 total: 89.86\n",
            "33051/36781 total: 89.86\n",
            "33052/36781 total: 89.86\n",
            "33053/36781 total: 89.86\n",
            "33054/36781 total: 89.87\n",
            "33055/36781 total: 89.87\n",
            "33056/36781 total: 89.87\n",
            "33057/36781 total: 89.88\n",
            "33058/36781 total: 89.88\n",
            "33059/36781 total: 89.88\n",
            "33060/36781 total: 89.88\n",
            "33061/36781 total: 89.89\n",
            "33062/36781 total: 89.89\n",
            "33063/36781 total: 89.89\n",
            "33064/36781 total: 89.89\n",
            "33065/36781 total: 89.9\n",
            "33066/36781 total: 89.9\n",
            "33067/36781 total: 89.9\n",
            "33068/36781 total: 89.91\n",
            "33069/36781 total: 89.91\n",
            "33070/36781 total: 89.91\n",
            "33071/36781 total: 89.91\n",
            "33072/36781 total: 89.92\n",
            "33073/36781 total: 89.92\n",
            "33074/36781 total: 89.92\n",
            "33075/36781 total: 89.92\n",
            "33076/36781 total: 89.93\n",
            "33077/36781 total: 89.93\n",
            "33078/36781 total: 89.93\n",
            "33079/36781 total: 89.94\n",
            "33080/36781 total: 89.94\n",
            "33081/36781 total: 89.94\n",
            "33082/36781 total: 89.94\n",
            "33083/36781 total: 89.95\n",
            "33084/36781 total: 89.95\n",
            "33085/36781 total: 89.95\n",
            "33086/36781 total: 89.95\n",
            "33087/36781 total: 89.96\n",
            "33088/36781 total: 89.96\n",
            "33089/36781 total: 89.96\n",
            "33090/36781 total: 89.96\n",
            "33091/36781 total: 89.97\n",
            "33092/36781 total: 89.97\n",
            "33093/36781 total: 89.97\n",
            "33094/36781 total: 89.98\n",
            "33095/36781 total: 89.98\n",
            "33096/36781 total: 89.98\n",
            "33097/36781 total: 89.98\n",
            "33098/36781 total: 89.99\n",
            "33099/36781 total: 89.99\n",
            "33100/36781 total: 89.99\n",
            "33101/36781 total: 89.99\n",
            "33102/36781 total: 90.0\n",
            "33103/36781 total: 90.0\n",
            "33104/36781 total: 90.0\n",
            "33105/36781 total: 90.01\n",
            "33106/36781 total: 90.01\n",
            "33107/36781 total: 90.01\n",
            "33108/36781 total: 90.01\n",
            "33109/36781 total: 90.02\n",
            "33110/36781 total: 90.02\n",
            "33111/36781 total: 90.02\n",
            "33112/36781 total: 90.02\n",
            "33113/36781 total: 90.03\n",
            "33114/36781 total: 90.03\n",
            "33115/36781 total: 90.03\n",
            "33116/36781 total: 90.04\n",
            "33117/36781 total: 90.04\n",
            "33118/36781 total: 90.04\n",
            "33119/36781 total: 90.04\n",
            "33120/36781 total: 90.05\n",
            "33121/36781 total: 90.05\n",
            "33122/36781 total: 90.05\n",
            "33123/36781 total: 90.05\n",
            "33124/36781 total: 90.06\n",
            "33125/36781 total: 90.06\n",
            "33126/36781 total: 90.06\n",
            "33127/36781 total: 90.07\n",
            "33128/36781 total: 90.07\n",
            "33129/36781 total: 90.07\n",
            "33130/36781 total: 90.07\n",
            "33131/36781 total: 90.08\n",
            "33132/36781 total: 90.08\n",
            "33133/36781 total: 90.08\n",
            "33134/36781 total: 90.08\n",
            "33135/36781 total: 90.09\n",
            "33136/36781 total: 90.09\n",
            "33137/36781 total: 90.09\n",
            "33138/36781 total: 90.1\n",
            "33139/36781 total: 90.1\n",
            "33140/36781 total: 90.1\n",
            "33141/36781 total: 90.1\n",
            "33142/36781 total: 90.11\n",
            "33143/36781 total: 90.11\n",
            "33144/36781 total: 90.11\n",
            "33145/36781 total: 90.11\n",
            "33146/36781 total: 90.12\n",
            "33147/36781 total: 90.12\n",
            "33148/36781 total: 90.12\n",
            "33149/36781 total: 90.13\n",
            "33150/36781 total: 90.13\n",
            "33151/36781 total: 90.13\n",
            "33152/36781 total: 90.13\n",
            "33153/36781 total: 90.14\n",
            "33154/36781 total: 90.14\n",
            "33155/36781 total: 90.14\n",
            "33156/36781 total: 90.14\n",
            "33157/36781 total: 90.15\n",
            "33158/36781 total: 90.15\n",
            "33159/36781 total: 90.15\n",
            "33160/36781 total: 90.16\n",
            "33161/36781 total: 90.16\n",
            "33162/36781 total: 90.16\n",
            "33163/36781 total: 90.16\n",
            "33164/36781 total: 90.17\n",
            "33165/36781 total: 90.17\n",
            "33166/36781 total: 90.17\n",
            "33167/36781 total: 90.17\n",
            "33168/36781 total: 90.18\n",
            "33169/36781 total: 90.18\n",
            "33170/36781 total: 90.18\n",
            "33171/36781 total: 90.19\n",
            "33172/36781 total: 90.19\n",
            "33173/36781 total: 90.19\n",
            "33174/36781 total: 90.19\n",
            "33175/36781 total: 90.2\n",
            "33176/36781 total: 90.2\n",
            "33177/36781 total: 90.2\n",
            "33178/36781 total: 90.2\n",
            "33179/36781 total: 90.21\n",
            "33180/36781 total: 90.21\n",
            "33181/36781 total: 90.21\n",
            "33182/36781 total: 90.22\n",
            "33183/36781 total: 90.22\n",
            "33184/36781 total: 90.22\n",
            "33185/36781 total: 90.22\n",
            "33186/36781 total: 90.23\n",
            "33187/36781 total: 90.23\n",
            "33188/36781 total: 90.23\n",
            "33189/36781 total: 90.23\n",
            "33190/36781 total: 90.24\n",
            "33191/36781 total: 90.24\n",
            "33192/36781 total: 90.24\n",
            "33193/36781 total: 90.24\n",
            "33194/36781 total: 90.25\n",
            "33195/36781 total: 90.25\n",
            "33196/36781 total: 90.25\n",
            "33197/36781 total: 90.26\n",
            "33198/36781 total: 90.26\n",
            "33199/36781 total: 90.26\n",
            "33200/36781 total: 90.26\n",
            "33201/36781 total: 90.27\n",
            "33202/36781 total: 90.27\n",
            "33203/36781 total: 90.27\n",
            "33204/36781 total: 90.27\n",
            "33205/36781 total: 90.28\n",
            "33206/36781 total: 90.28\n",
            "33207/36781 total: 90.28\n",
            "33208/36781 total: 90.29\n",
            "33209/36781 total: 90.29\n",
            "33210/36781 total: 90.29\n",
            "33211/36781 total: 90.29\n",
            "33212/36781 total: 90.3\n",
            "33213/36781 total: 90.3\n",
            "33214/36781 total: 90.3\n",
            "33215/36781 total: 90.3\n",
            "33216/36781 total: 90.31\n",
            "33217/36781 total: 90.31\n",
            "33218/36781 total: 90.31\n",
            "33219/36781 total: 90.32\n",
            "33220/36781 total: 90.32\n",
            "33221/36781 total: 90.32\n",
            "33222/36781 total: 90.32\n",
            "33223/36781 total: 90.33\n",
            "33224/36781 total: 90.33\n",
            "33225/36781 total: 90.33\n",
            "33226/36781 total: 90.33\n",
            "33227/36781 total: 90.34\n",
            "33228/36781 total: 90.34\n",
            "33229/36781 total: 90.34\n",
            "33230/36781 total: 90.35\n",
            "33231/36781 total: 90.35\n",
            "33232/36781 total: 90.35\n",
            "33233/36781 total: 90.35\n",
            "33234/36781 total: 90.36\n",
            "33235/36781 total: 90.36\n",
            "33236/36781 total: 90.36\n",
            "33237/36781 total: 90.36\n",
            "33238/36781 total: 90.37\n",
            "33239/36781 total: 90.37\n",
            "33240/36781 total: 90.37\n",
            "33241/36781 total: 90.38\n",
            "33242/36781 total: 90.38\n",
            "33243/36781 total: 90.38\n",
            "33244/36781 total: 90.38\n",
            "33245/36781 total: 90.39\n",
            "33246/36781 total: 90.39\n",
            "33247/36781 total: 90.39\n",
            "33248/36781 total: 90.39\n",
            "33249/36781 total: 90.4\n",
            "33250/36781 total: 90.4\n",
            "33251/36781 total: 90.4\n",
            "33252/36781 total: 90.41\n",
            "33253/36781 total: 90.41\n",
            "33254/36781 total: 90.41\n",
            "33255/36781 total: 90.41\n",
            "33256/36781 total: 90.42\n",
            "33257/36781 total: 90.42\n",
            "33258/36781 total: 90.42\n",
            "33259/36781 total: 90.42\n",
            "33260/36781 total: 90.43\n",
            "33261/36781 total: 90.43\n",
            "33262/36781 total: 90.43\n",
            "33263/36781 total: 90.44\n",
            "33264/36781 total: 90.44\n",
            "33265/36781 total: 90.44\n",
            "33266/36781 total: 90.44\n",
            "33267/36781 total: 90.45\n",
            "33268/36781 total: 90.45\n",
            "33269/36781 total: 90.45\n",
            "33270/36781 total: 90.45\n",
            "33271/36781 total: 90.46\n",
            "33272/36781 total: 90.46\n",
            "33273/36781 total: 90.46\n",
            "33274/36781 total: 90.47\n",
            "33275/36781 total: 90.47\n",
            "33276/36781 total: 90.47\n",
            "33277/36781 total: 90.47\n",
            "33278/36781 total: 90.48\n",
            "33279/36781 total: 90.48\n",
            "33280/36781 total: 90.48\n",
            "33281/36781 total: 90.48\n",
            "33282/36781 total: 90.49\n",
            "33283/36781 total: 90.49\n",
            "33284/36781 total: 90.49\n",
            "33285/36781 total: 90.5\n",
            "33286/36781 total: 90.5\n",
            "33287/36781 total: 90.5\n",
            "33288/36781 total: 90.5\n",
            "33289/36781 total: 90.51\n",
            "33290/36781 total: 90.51\n",
            "33291/36781 total: 90.51\n",
            "33292/36781 total: 90.51\n",
            "33293/36781 total: 90.52\n",
            "33294/36781 total: 90.52\n",
            "33295/36781 total: 90.52\n",
            "33296/36781 total: 90.52\n",
            "33297/36781 total: 90.53\n",
            "33298/36781 total: 90.53\n",
            "33299/36781 total: 90.53\n",
            "33300/36781 total: 90.54\n",
            "33301/36781 total: 90.54\n",
            "33302/36781 total: 90.54\n",
            "33303/36781 total: 90.54\n",
            "33304/36781 total: 90.55\n",
            "33305/36781 total: 90.55\n",
            "33306/36781 total: 90.55\n",
            "33307/36781 total: 90.55\n",
            "33308/36781 total: 90.56\n",
            "33309/36781 total: 90.56\n",
            "33310/36781 total: 90.56\n",
            "33311/36781 total: 90.57\n",
            "33312/36781 total: 90.57\n",
            "33313/36781 total: 90.57\n",
            "33314/36781 total: 90.57\n",
            "33315/36781 total: 90.58\n",
            "33316/36781 total: 90.58\n",
            "33317/36781 total: 90.58\n",
            "33318/36781 total: 90.58\n",
            "33319/36781 total: 90.59\n",
            "33320/36781 total: 90.59\n",
            "33321/36781 total: 90.59\n",
            "33322/36781 total: 90.6\n",
            "33323/36781 total: 90.6\n",
            "33324/36781 total: 90.6\n",
            "33325/36781 total: 90.6\n",
            "33326/36781 total: 90.61\n",
            "33327/36781 total: 90.61\n",
            "33328/36781 total: 90.61\n",
            "33329/36781 total: 90.61\n",
            "33330/36781 total: 90.62\n",
            "33331/36781 total: 90.62\n",
            "33332/36781 total: 90.62\n",
            "33333/36781 total: 90.63\n",
            "33334/36781 total: 90.63\n",
            "33335/36781 total: 90.63\n",
            "33336/36781 total: 90.63\n",
            "33337/36781 total: 90.64\n",
            "33338/36781 total: 90.64\n",
            "33339/36781 total: 90.64\n",
            "33340/36781 total: 90.64\n",
            "33341/36781 total: 90.65\n",
            "33342/36781 total: 90.65\n",
            "33343/36781 total: 90.65\n",
            "33344/36781 total: 90.66\n",
            "33345/36781 total: 90.66\n",
            "33346/36781 total: 90.66\n",
            "33347/36781 total: 90.66\n",
            "33348/36781 total: 90.67\n",
            "33349/36781 total: 90.67\n",
            "33350/36781 total: 90.67\n",
            "33351/36781 total: 90.67\n",
            "33352/36781 total: 90.68\n",
            "33353/36781 total: 90.68\n",
            "33354/36781 total: 90.68\n",
            "33355/36781 total: 90.69\n",
            "33356/36781 total: 90.69\n",
            "33357/36781 total: 90.69\n",
            "33358/36781 total: 90.69\n",
            "33359/36781 total: 90.7\n",
            "33360/36781 total: 90.7\n",
            "33361/36781 total: 90.7\n",
            "33362/36781 total: 90.7\n",
            "33363/36781 total: 90.71\n",
            "33364/36781 total: 90.71\n",
            "33365/36781 total: 90.71\n",
            "33366/36781 total: 90.72\n",
            "33367/36781 total: 90.72\n",
            "33368/36781 total: 90.72\n",
            "33369/36781 total: 90.72\n",
            "33370/36781 total: 90.73\n",
            "33371/36781 total: 90.73\n",
            "33372/36781 total: 90.73\n",
            "33373/36781 total: 90.73\n",
            "33374/36781 total: 90.74\n",
            "33375/36781 total: 90.74\n",
            "33376/36781 total: 90.74\n",
            "33377/36781 total: 90.75\n",
            "33378/36781 total: 90.75\n",
            "33379/36781 total: 90.75\n",
            "33380/36781 total: 90.75\n",
            "33381/36781 total: 90.76\n",
            "33382/36781 total: 90.76\n",
            "33383/36781 total: 90.76\n",
            "33384/36781 total: 90.76\n",
            "33385/36781 total: 90.77\n",
            "33386/36781 total: 90.77\n",
            "33387/36781 total: 90.77\n",
            "33388/36781 total: 90.78\n",
            "33389/36781 total: 90.78\n",
            "33390/36781 total: 90.78\n",
            "33391/36781 total: 90.78\n",
            "33392/36781 total: 90.79\n",
            "33393/36781 total: 90.79\n",
            "33394/36781 total: 90.79\n",
            "33395/36781 total: 90.79\n",
            "33396/36781 total: 90.8\n",
            "33397/36781 total: 90.8\n",
            "33398/36781 total: 90.8\n",
            "33399/36781 total: 90.81\n",
            "33400/36781 total: 90.81\n",
            "33401/36781 total: 90.81\n",
            "33402/36781 total: 90.81\n",
            "33403/36781 total: 90.82\n",
            "33404/36781 total: 90.82\n",
            "33405/36781 total: 90.82\n",
            "33406/36781 total: 90.82\n",
            "33407/36781 total: 90.83\n",
            "33408/36781 total: 90.83\n",
            "33409/36781 total: 90.83\n",
            "33410/36781 total: 90.83\n",
            "33411/36781 total: 90.84\n",
            "33412/36781 total: 90.84\n",
            "33413/36781 total: 90.84\n",
            "33414/36781 total: 90.85\n",
            "33415/36781 total: 90.85\n",
            "33416/36781 total: 90.85\n",
            "33417/36781 total: 90.85\n",
            "33418/36781 total: 90.86\n",
            "33419/36781 total: 90.86\n",
            "33420/36781 total: 90.86\n",
            "33421/36781 total: 90.86\n",
            "33422/36781 total: 90.87\n",
            "33423/36781 total: 90.87\n",
            "33424/36781 total: 90.87\n",
            "33425/36781 total: 90.88\n",
            "33426/36781 total: 90.88\n",
            "33427/36781 total: 90.88\n",
            "33428/36781 total: 90.88\n",
            "33429/36781 total: 90.89\n",
            "33430/36781 total: 90.89\n",
            "33431/36781 total: 90.89\n",
            "33432/36781 total: 90.89\n",
            "33433/36781 total: 90.9\n",
            "33434/36781 total: 90.9\n",
            "33435/36781 total: 90.9\n",
            "33436/36781 total: 90.91\n",
            "33437/36781 total: 90.91\n",
            "33438/36781 total: 90.91\n",
            "33439/36781 total: 90.91\n",
            "33440/36781 total: 90.92\n",
            "33441/36781 total: 90.92\n",
            "33442/36781 total: 90.92\n",
            "33443/36781 total: 90.92\n",
            "33444/36781 total: 90.93\n",
            "33445/36781 total: 90.93\n",
            "33446/36781 total: 90.93\n",
            "33447/36781 total: 90.94\n",
            "33448/36781 total: 90.94\n",
            "33449/36781 total: 90.94\n",
            "33450/36781 total: 90.94\n",
            "33451/36781 total: 90.95\n",
            "33452/36781 total: 90.95\n",
            "33453/36781 total: 90.95\n",
            "33454/36781 total: 90.95\n",
            "33455/36781 total: 90.96\n",
            "33456/36781 total: 90.96\n",
            "33457/36781 total: 90.96\n",
            "33458/36781 total: 90.97\n",
            "33459/36781 total: 90.97\n",
            "33460/36781 total: 90.97\n",
            "33461/36781 total: 90.97\n",
            "33462/36781 total: 90.98\n",
            "33463/36781 total: 90.98\n",
            "33464/36781 total: 90.98\n",
            "33465/36781 total: 90.98\n",
            "33466/36781 total: 90.99\n",
            "33467/36781 total: 90.99\n",
            "33468/36781 total: 90.99\n",
            "33469/36781 total: 91.0\n",
            "33470/36781 total: 91.0\n",
            "33471/36781 total: 91.0\n",
            "33472/36781 total: 91.0\n",
            "33473/36781 total: 91.01\n",
            "33474/36781 total: 91.01\n",
            "33475/36781 total: 91.01\n",
            "33476/36781 total: 91.01\n",
            "33477/36781 total: 91.02\n",
            "33478/36781 total: 91.02\n",
            "33479/36781 total: 91.02\n",
            "33480/36781 total: 91.03\n",
            "33481/36781 total: 91.03\n",
            "33482/36781 total: 91.03\n",
            "33483/36781 total: 91.03\n",
            "33484/36781 total: 91.04\n",
            "33485/36781 total: 91.04\n",
            "33486/36781 total: 91.04\n",
            "33487/36781 total: 91.04\n",
            "33488/36781 total: 91.05\n",
            "33489/36781 total: 91.05\n",
            "33490/36781 total: 91.05\n",
            "33491/36781 total: 91.06\n",
            "33492/36781 total: 91.06\n",
            "33493/36781 total: 91.06\n",
            "33494/36781 total: 91.06\n",
            "33495/36781 total: 91.07\n",
            "33496/36781 total: 91.07\n",
            "33497/36781 total: 91.07\n",
            "33498/36781 total: 91.07\n",
            "33499/36781 total: 91.08\n",
            "33500/36781 total: 91.08\n",
            "33501/36781 total: 91.08\n",
            "33502/36781 total: 91.09\n",
            "33503/36781 total: 91.09\n",
            "33504/36781 total: 91.09\n",
            "33505/36781 total: 91.09\n",
            "33506/36781 total: 91.1\n",
            "33507/36781 total: 91.1\n",
            "33508/36781 total: 91.1\n",
            "33509/36781 total: 91.1\n",
            "33510/36781 total: 91.11\n",
            "33511/36781 total: 91.11\n",
            "33512/36781 total: 91.11\n",
            "33513/36781 total: 91.11\n",
            "33514/36781 total: 91.12\n",
            "33515/36781 total: 91.12\n",
            "33516/36781 total: 91.12\n",
            "33517/36781 total: 91.13\n",
            "33518/36781 total: 91.13\n",
            "33519/36781 total: 91.13\n",
            "33520/36781 total: 91.13\n",
            "33521/36781 total: 91.14\n",
            "33522/36781 total: 91.14\n",
            "33523/36781 total: 91.14\n",
            "33524/36781 total: 91.14\n",
            "33525/36781 total: 91.15\n",
            "33526/36781 total: 91.15\n",
            "33527/36781 total: 91.15\n",
            "33528/36781 total: 91.16\n",
            "33529/36781 total: 91.16\n",
            "33530/36781 total: 91.16\n",
            "33531/36781 total: 91.16\n",
            "33532/36781 total: 91.17\n",
            "33533/36781 total: 91.17\n",
            "33534/36781 total: 91.17\n",
            "33535/36781 total: 91.17\n",
            "33536/36781 total: 91.18\n",
            "33537/36781 total: 91.18\n",
            "33538/36781 total: 91.18\n",
            "33539/36781 total: 91.19\n",
            "33540/36781 total: 91.19\n",
            "33541/36781 total: 91.19\n",
            "33542/36781 total: 91.19\n",
            "33543/36781 total: 91.2\n",
            "33544/36781 total: 91.2\n",
            "33545/36781 total: 91.2\n",
            "33546/36781 total: 91.2\n",
            "33547/36781 total: 91.21\n",
            "33548/36781 total: 91.21\n",
            "33549/36781 total: 91.21\n",
            "33550/36781 total: 91.22\n",
            "33551/36781 total: 91.22\n",
            "33552/36781 total: 91.22\n",
            "33553/36781 total: 91.22\n",
            "33554/36781 total: 91.23\n",
            "33555/36781 total: 91.23\n",
            "33556/36781 total: 91.23\n",
            "33557/36781 total: 91.23\n",
            "33558/36781 total: 91.24\n",
            "33559/36781 total: 91.24\n",
            "33560/36781 total: 91.24\n",
            "33561/36781 total: 91.25\n",
            "33562/36781 total: 91.25\n",
            "33563/36781 total: 91.25\n",
            "33564/36781 total: 91.25\n",
            "33565/36781 total: 91.26\n",
            "33566/36781 total: 91.26\n",
            "33567/36781 total: 91.26\n",
            "33568/36781 total: 91.26\n",
            "33569/36781 total: 91.27\n",
            "33570/36781 total: 91.27\n",
            "33571/36781 total: 91.27\n",
            "33572/36781 total: 91.28\n",
            "33573/36781 total: 91.28\n",
            "33574/36781 total: 91.28\n",
            "33575/36781 total: 91.28\n",
            "33576/36781 total: 91.29\n",
            "33577/36781 total: 91.29\n",
            "33578/36781 total: 91.29\n",
            "33579/36781 total: 91.29\n",
            "33580/36781 total: 91.3\n",
            "33581/36781 total: 91.3\n",
            "33582/36781 total: 91.3\n",
            "33583/36781 total: 91.31\n",
            "33584/36781 total: 91.31\n",
            "33585/36781 total: 91.31\n",
            "33586/36781 total: 91.31\n",
            "33587/36781 total: 91.32\n",
            "33588/36781 total: 91.32\n",
            "33589/36781 total: 91.32\n",
            "33590/36781 total: 91.32\n",
            "33591/36781 total: 91.33\n",
            "33592/36781 total: 91.33\n",
            "33593/36781 total: 91.33\n",
            "33594/36781 total: 91.34\n",
            "33595/36781 total: 91.34\n",
            "33596/36781 total: 91.34\n",
            "33597/36781 total: 91.34\n",
            "33598/36781 total: 91.35\n",
            "33599/36781 total: 91.35\n",
            "33600/36781 total: 91.35\n",
            "33601/36781 total: 91.35\n",
            "33602/36781 total: 91.36\n",
            "33603/36781 total: 91.36\n",
            "33604/36781 total: 91.36\n",
            "33605/36781 total: 91.37\n",
            "33606/36781 total: 91.37\n",
            "33607/36781 total: 91.37\n",
            "33608/36781 total: 91.37\n",
            "33609/36781 total: 91.38\n",
            "33610/36781 total: 91.38\n",
            "33611/36781 total: 91.38\n",
            "33612/36781 total: 91.38\n",
            "33613/36781 total: 91.39\n",
            "33614/36781 total: 91.39\n",
            "33615/36781 total: 91.39\n",
            "33616/36781 total: 91.4\n",
            "33617/36781 total: 91.4\n",
            "33618/36781 total: 91.4\n",
            "33619/36781 total: 91.4\n",
            "33620/36781 total: 91.41\n",
            "33621/36781 total: 91.41\n",
            "33622/36781 total: 91.41\n",
            "33623/36781 total: 91.41\n",
            "33624/36781 total: 91.42\n",
            "33625/36781 total: 91.42\n",
            "33626/36781 total: 91.42\n",
            "33627/36781 total: 91.42\n",
            "33628/36781 total: 91.43\n",
            "33629/36781 total: 91.43\n",
            "33630/36781 total: 91.43\n",
            "33631/36781 total: 91.44\n",
            "33632/36781 total: 91.44\n",
            "33633/36781 total: 91.44\n",
            "33634/36781 total: 91.44\n",
            "33635/36781 total: 91.45\n",
            "33636/36781 total: 91.45\n",
            "33637/36781 total: 91.45\n",
            "33638/36781 total: 91.45\n",
            "33639/36781 total: 91.46\n",
            "33640/36781 total: 91.46\n",
            "33641/36781 total: 91.46\n",
            "33642/36781 total: 91.47\n",
            "33643/36781 total: 91.47\n",
            "33644/36781 total: 91.47\n",
            "33645/36781 total: 91.47\n",
            "33646/36781 total: 91.48\n",
            "33647/36781 total: 91.48\n",
            "33648/36781 total: 91.48\n",
            "33649/36781 total: 91.48\n",
            "33650/36781 total: 91.49\n",
            "33651/36781 total: 91.49\n",
            "33652/36781 total: 91.49\n",
            "33653/36781 total: 91.5\n",
            "33654/36781 total: 91.5\n",
            "33655/36781 total: 91.5\n",
            "33656/36781 total: 91.5\n",
            "33657/36781 total: 91.51\n",
            "33658/36781 total: 91.51\n",
            "33659/36781 total: 91.51\n",
            "33660/36781 total: 91.51\n",
            "33661/36781 total: 91.52\n",
            "33662/36781 total: 91.52\n",
            "33663/36781 total: 91.52\n",
            "33664/36781 total: 91.53\n",
            "33665/36781 total: 91.53\n",
            "33666/36781 total: 91.53\n",
            "33667/36781 total: 91.53\n",
            "33668/36781 total: 91.54\n",
            "33669/36781 total: 91.54\n",
            "33670/36781 total: 91.54\n",
            "33671/36781 total: 91.54\n",
            "33672/36781 total: 91.55\n",
            "33673/36781 total: 91.55\n",
            "33674/36781 total: 91.55\n",
            "33675/36781 total: 91.56\n",
            "33676/36781 total: 91.56\n",
            "33677/36781 total: 91.56\n",
            "33678/36781 total: 91.56\n",
            "33679/36781 total: 91.57\n",
            "33680/36781 total: 91.57\n",
            "33681/36781 total: 91.57\n",
            "33682/36781 total: 91.57\n",
            "33683/36781 total: 91.58\n",
            "33684/36781 total: 91.58\n",
            "33685/36781 total: 91.58\n",
            "33686/36781 total: 91.59\n",
            "33687/36781 total: 91.59\n",
            "33688/36781 total: 91.59\n",
            "33689/36781 total: 91.59\n",
            "33690/36781 total: 91.6\n",
            "33691/36781 total: 91.6\n",
            "33692/36781 total: 91.6\n",
            "33693/36781 total: 91.6\n",
            "33694/36781 total: 91.61\n",
            "33695/36781 total: 91.61\n",
            "33696/36781 total: 91.61\n",
            "33697/36781 total: 91.62\n",
            "33698/36781 total: 91.62\n",
            "33699/36781 total: 91.62\n",
            "33700/36781 total: 91.62\n",
            "33701/36781 total: 91.63\n",
            "33702/36781 total: 91.63\n",
            "33703/36781 total: 91.63\n",
            "33704/36781 total: 91.63\n",
            "33705/36781 total: 91.64\n",
            "33706/36781 total: 91.64\n",
            "33707/36781 total: 91.64\n",
            "33708/36781 total: 91.65\n",
            "33709/36781 total: 91.65\n",
            "33710/36781 total: 91.65\n",
            "33711/36781 total: 91.65\n",
            "33712/36781 total: 91.66\n",
            "33713/36781 total: 91.66\n",
            "33714/36781 total: 91.66\n",
            "33715/36781 total: 91.66\n",
            "33716/36781 total: 91.67\n",
            "33717/36781 total: 91.67\n",
            "33718/36781 total: 91.67\n",
            "33719/36781 total: 91.68\n",
            "33720/36781 total: 91.68\n",
            "33721/36781 total: 91.68\n",
            "33722/36781 total: 91.68\n",
            "33723/36781 total: 91.69\n",
            "33724/36781 total: 91.69\n",
            "33725/36781 total: 91.69\n",
            "33726/36781 total: 91.69\n",
            "33727/36781 total: 91.7\n",
            "33728/36781 total: 91.7\n",
            "33729/36781 total: 91.7\n",
            "33730/36781 total: 91.7\n",
            "33731/36781 total: 91.71\n",
            "33732/36781 total: 91.71\n",
            "33733/36781 total: 91.71\n",
            "33734/36781 total: 91.72\n",
            "33735/36781 total: 91.72\n",
            "33736/36781 total: 91.72\n",
            "33737/36781 total: 91.72\n",
            "33738/36781 total: 91.73\n",
            "33739/36781 total: 91.73\n",
            "33740/36781 total: 91.73\n",
            "33741/36781 total: 91.73\n",
            "33742/36781 total: 91.74\n",
            "33743/36781 total: 91.74\n",
            "33744/36781 total: 91.74\n",
            "33745/36781 total: 91.75\n",
            "33746/36781 total: 91.75\n",
            "33747/36781 total: 91.75\n",
            "33748/36781 total: 91.75\n",
            "33749/36781 total: 91.76\n",
            "33750/36781 total: 91.76\n",
            "33751/36781 total: 91.76\n",
            "33752/36781 total: 91.76\n",
            "33753/36781 total: 91.77\n",
            "33754/36781 total: 91.77\n",
            "33755/36781 total: 91.77\n",
            "33756/36781 total: 91.78\n",
            "33757/36781 total: 91.78\n",
            "33758/36781 total: 91.78\n",
            "33759/36781 total: 91.78\n",
            "33760/36781 total: 91.79\n",
            "33761/36781 total: 91.79\n",
            "33762/36781 total: 91.79\n",
            "33763/36781 total: 91.79\n",
            "33764/36781 total: 91.8\n",
            "33765/36781 total: 91.8\n",
            "33766/36781 total: 91.8\n",
            "33767/36781 total: 91.81\n",
            "33768/36781 total: 91.81\n",
            "33769/36781 total: 91.81\n",
            "33770/36781 total: 91.81\n",
            "33771/36781 total: 91.82\n",
            "33772/36781 total: 91.82\n",
            "33773/36781 total: 91.82\n",
            "33774/36781 total: 91.82\n",
            "33775/36781 total: 91.83\n",
            "33776/36781 total: 91.83\n",
            "33777/36781 total: 91.83\n",
            "33778/36781 total: 91.84\n",
            "33779/36781 total: 91.84\n",
            "33780/36781 total: 91.84\n",
            "33781/36781 total: 91.84\n",
            "33782/36781 total: 91.85\n",
            "33783/36781 total: 91.85\n",
            "33784/36781 total: 91.85\n",
            "33785/36781 total: 91.85\n",
            "33786/36781 total: 91.86\n",
            "33787/36781 total: 91.86\n",
            "33788/36781 total: 91.86\n",
            "33789/36781 total: 91.87\n",
            "33790/36781 total: 91.87\n",
            "33791/36781 total: 91.87\n",
            "33792/36781 total: 91.87\n",
            "33793/36781 total: 91.88\n",
            "33794/36781 total: 91.88\n",
            "33795/36781 total: 91.88\n",
            "33796/36781 total: 91.88\n",
            "33797/36781 total: 91.89\n",
            "33798/36781 total: 91.89\n",
            "33799/36781 total: 91.89\n",
            "33800/36781 total: 91.9\n",
            "33801/36781 total: 91.9\n",
            "33802/36781 total: 91.9\n",
            "33803/36781 total: 91.9\n",
            "33804/36781 total: 91.91\n",
            "33805/36781 total: 91.91\n",
            "33806/36781 total: 91.91\n",
            "33807/36781 total: 91.91\n",
            "33808/36781 total: 91.92\n",
            "33809/36781 total: 91.92\n",
            "33810/36781 total: 91.92\n",
            "33811/36781 total: 91.93\n",
            "33812/36781 total: 91.93\n",
            "33813/36781 total: 91.93\n",
            "33814/36781 total: 91.93\n",
            "33815/36781 total: 91.94\n",
            "33816/36781 total: 91.94\n",
            "33817/36781 total: 91.94\n",
            "33818/36781 total: 91.94\n",
            "33819/36781 total: 91.95\n",
            "33820/36781 total: 91.95\n",
            "33821/36781 total: 91.95\n",
            "33822/36781 total: 91.96\n",
            "33823/36781 total: 91.96\n",
            "33824/36781 total: 91.96\n",
            "33825/36781 total: 91.96\n",
            "33826/36781 total: 91.97\n",
            "33827/36781 total: 91.97\n",
            "33828/36781 total: 91.97\n",
            "33829/36781 total: 91.97\n",
            "33830/36781 total: 91.98\n",
            "33831/36781 total: 91.98\n",
            "33832/36781 total: 91.98\n",
            "33833/36781 total: 91.98\n",
            "33834/36781 total: 91.99\n",
            "33835/36781 total: 91.99\n",
            "33836/36781 total: 91.99\n",
            "33837/36781 total: 92.0\n",
            "33838/36781 total: 92.0\n",
            "33839/36781 total: 92.0\n",
            "33840/36781 total: 92.0\n",
            "33841/36781 total: 92.01\n",
            "33842/36781 total: 92.01\n",
            "33843/36781 total: 92.01\n",
            "33844/36781 total: 92.01\n",
            "33845/36781 total: 92.02\n",
            "33846/36781 total: 92.02\n",
            "33847/36781 total: 92.02\n",
            "33848/36781 total: 92.03\n",
            "33849/36781 total: 92.03\n",
            "33850/36781 total: 92.03\n",
            "33851/36781 total: 92.03\n",
            "33852/36781 total: 92.04\n",
            "33853/36781 total: 92.04\n",
            "33854/36781 total: 92.04\n",
            "33855/36781 total: 92.04\n",
            "33856/36781 total: 92.05\n",
            "33857/36781 total: 92.05\n",
            "33858/36781 total: 92.05\n",
            "33859/36781 total: 92.06\n",
            "33860/36781 total: 92.06\n",
            "33861/36781 total: 92.06\n",
            "33862/36781 total: 92.06\n",
            "33863/36781 total: 92.07\n",
            "33864/36781 total: 92.07\n",
            "33865/36781 total: 92.07\n",
            "33866/36781 total: 92.07\n",
            "33867/36781 total: 92.08\n",
            "33868/36781 total: 92.08\n",
            "33869/36781 total: 92.08\n",
            "33870/36781 total: 92.09\n",
            "33871/36781 total: 92.09\n",
            "33872/36781 total: 92.09\n",
            "33873/36781 total: 92.09\n",
            "33874/36781 total: 92.1\n",
            "33875/36781 total: 92.1\n",
            "33876/36781 total: 92.1\n",
            "33877/36781 total: 92.1\n",
            "33878/36781 total: 92.11\n",
            "33879/36781 total: 92.11\n",
            "33880/36781 total: 92.11\n",
            "33881/36781 total: 92.12\n",
            "33882/36781 total: 92.12\n",
            "33883/36781 total: 92.12\n",
            "33884/36781 total: 92.12\n",
            "33885/36781 total: 92.13\n",
            "33886/36781 total: 92.13\n",
            "33887/36781 total: 92.13\n",
            "33888/36781 total: 92.13\n",
            "33889/36781 total: 92.14\n",
            "33890/36781 total: 92.14\n",
            "33891/36781 total: 92.14\n",
            "33892/36781 total: 92.15\n",
            "33893/36781 total: 92.15\n",
            "33894/36781 total: 92.15\n",
            "33895/36781 total: 92.15\n",
            "33896/36781 total: 92.16\n",
            "33897/36781 total: 92.16\n",
            "33898/36781 total: 92.16\n",
            "33899/36781 total: 92.16\n",
            "33900/36781 total: 92.17\n",
            "33901/36781 total: 92.17\n",
            "33902/36781 total: 92.17\n",
            "33903/36781 total: 92.18\n",
            "33904/36781 total: 92.18\n",
            "33905/36781 total: 92.18\n",
            "33906/36781 total: 92.18\n",
            "33907/36781 total: 92.19\n",
            "33908/36781 total: 92.19\n",
            "33909/36781 total: 92.19\n",
            "33910/36781 total: 92.19\n",
            "33911/36781 total: 92.2\n",
            "33912/36781 total: 92.2\n",
            "33913/36781 total: 92.2\n",
            "33914/36781 total: 92.21\n",
            "33915/36781 total: 92.21\n",
            "33916/36781 total: 92.21\n",
            "33917/36781 total: 92.21\n",
            "33918/36781 total: 92.22\n",
            "33919/36781 total: 92.22\n",
            "33920/36781 total: 92.22\n",
            "33921/36781 total: 92.22\n",
            "33922/36781 total: 92.23\n",
            "33923/36781 total: 92.23\n",
            "33924/36781 total: 92.23\n",
            "33925/36781 total: 92.24\n",
            "33926/36781 total: 92.24\n",
            "33927/36781 total: 92.24\n",
            "33928/36781 total: 92.24\n",
            "33929/36781 total: 92.25\n",
            "33930/36781 total: 92.25\n",
            "33931/36781 total: 92.25\n",
            "33932/36781 total: 92.25\n",
            "33933/36781 total: 92.26\n",
            "33934/36781 total: 92.26\n",
            "33935/36781 total: 92.26\n",
            "33936/36781 total: 92.27\n",
            "33937/36781 total: 92.27\n",
            "33938/36781 total: 92.27\n",
            "33939/36781 total: 92.27\n",
            "33940/36781 total: 92.28\n",
            "33941/36781 total: 92.28\n",
            "33942/36781 total: 92.28\n",
            "33943/36781 total: 92.28\n",
            "33944/36781 total: 92.29\n",
            "33945/36781 total: 92.29\n",
            "33946/36781 total: 92.29\n",
            "33947/36781 total: 92.29\n",
            "33948/36781 total: 92.3\n",
            "33949/36781 total: 92.3\n",
            "33950/36781 total: 92.3\n",
            "33951/36781 total: 92.31\n",
            "33952/36781 total: 92.31\n",
            "33953/36781 total: 92.31\n",
            "33954/36781 total: 92.31\n",
            "33955/36781 total: 92.32\n",
            "33956/36781 total: 92.32\n",
            "33957/36781 total: 92.32\n",
            "33958/36781 total: 92.32\n",
            "33959/36781 total: 92.33\n",
            "33960/36781 total: 92.33\n",
            "33961/36781 total: 92.33\n",
            "33962/36781 total: 92.34\n",
            "33963/36781 total: 92.34\n",
            "33964/36781 total: 92.34\n",
            "33965/36781 total: 92.34\n",
            "33966/36781 total: 92.35\n",
            "33967/36781 total: 92.35\n",
            "33968/36781 total: 92.35\n",
            "33969/36781 total: 92.35\n",
            "33970/36781 total: 92.36\n",
            "33971/36781 total: 92.36\n",
            "33972/36781 total: 92.36\n",
            "33973/36781 total: 92.37\n",
            "33974/36781 total: 92.37\n",
            "33975/36781 total: 92.37\n",
            "33976/36781 total: 92.37\n",
            "33977/36781 total: 92.38\n",
            "33978/36781 total: 92.38\n",
            "33979/36781 total: 92.38\n",
            "33980/36781 total: 92.38\n",
            "33981/36781 total: 92.39\n",
            "33982/36781 total: 92.39\n",
            "33983/36781 total: 92.39\n",
            "33984/36781 total: 92.4\n",
            "33985/36781 total: 92.4\n",
            "33986/36781 total: 92.4\n",
            "33987/36781 total: 92.4\n",
            "33988/36781 total: 92.41\n",
            "33989/36781 total: 92.41\n",
            "33990/36781 total: 92.41\n",
            "33991/36781 total: 92.41\n",
            "33992/36781 total: 92.42\n",
            "33993/36781 total: 92.42\n",
            "33994/36781 total: 92.42\n",
            "33995/36781 total: 92.43\n",
            "33996/36781 total: 92.43\n",
            "33997/36781 total: 92.43\n",
            "33998/36781 total: 92.43\n",
            "33999/36781 total: 92.44\n",
            "34000/36781 total: 92.44\n",
            "34001/36781 total: 92.44\n",
            "34002/36781 total: 92.44\n",
            "34003/36781 total: 92.45\n",
            "34004/36781 total: 92.45\n",
            "34005/36781 total: 92.45\n",
            "34006/36781 total: 92.46\n",
            "34007/36781 total: 92.46\n",
            "34008/36781 total: 92.46\n",
            "34009/36781 total: 92.46\n",
            "34010/36781 total: 92.47\n",
            "34011/36781 total: 92.47\n",
            "34012/36781 total: 92.47\n",
            "34013/36781 total: 92.47\n",
            "34014/36781 total: 92.48\n",
            "34015/36781 total: 92.48\n",
            "34016/36781 total: 92.48\n",
            "34017/36781 total: 92.49\n",
            "34018/36781 total: 92.49\n",
            "34019/36781 total: 92.49\n",
            "34020/36781 total: 92.49\n",
            "34021/36781 total: 92.5\n",
            "34022/36781 total: 92.5\n",
            "34023/36781 total: 92.5\n",
            "34024/36781 total: 92.5\n",
            "34025/36781 total: 92.51\n",
            "34026/36781 total: 92.51\n",
            "34027/36781 total: 92.51\n",
            "34028/36781 total: 92.52\n",
            "34029/36781 total: 92.52\n",
            "34030/36781 total: 92.52\n",
            "34031/36781 total: 92.52\n",
            "34032/36781 total: 92.53\n",
            "34033/36781 total: 92.53\n",
            "34034/36781 total: 92.53\n",
            "34035/36781 total: 92.53\n",
            "34036/36781 total: 92.54\n",
            "34037/36781 total: 92.54\n",
            "34038/36781 total: 92.54\n",
            "34039/36781 total: 92.55\n",
            "34040/36781 total: 92.55\n",
            "34041/36781 total: 92.55\n",
            "34042/36781 total: 92.55\n",
            "34043/36781 total: 92.56\n",
            "34044/36781 total: 92.56\n",
            "34045/36781 total: 92.56\n",
            "34046/36781 total: 92.56\n",
            "34047/36781 total: 92.57\n",
            "34048/36781 total: 92.57\n",
            "34049/36781 total: 92.57\n",
            "34050/36781 total: 92.57\n",
            "34051/36781 total: 92.58\n",
            "34052/36781 total: 92.58\n",
            "34053/36781 total: 92.58\n",
            "34054/36781 total: 92.59\n",
            "34055/36781 total: 92.59\n",
            "34056/36781 total: 92.59\n",
            "34057/36781 total: 92.59\n",
            "34058/36781 total: 92.6\n",
            "34059/36781 total: 92.6\n",
            "34060/36781 total: 92.6\n",
            "34061/36781 total: 92.6\n",
            "34062/36781 total: 92.61\n",
            "34063/36781 total: 92.61\n",
            "34064/36781 total: 92.61\n",
            "34065/36781 total: 92.62\n",
            "34066/36781 total: 92.62\n",
            "34067/36781 total: 92.62\n",
            "34068/36781 total: 92.62\n",
            "34069/36781 total: 92.63\n",
            "34070/36781 total: 92.63\n",
            "34071/36781 total: 92.63\n",
            "34072/36781 total: 92.63\n",
            "34073/36781 total: 92.64\n",
            "34074/36781 total: 92.64\n",
            "34075/36781 total: 92.64\n",
            "34076/36781 total: 92.65\n",
            "34077/36781 total: 92.65\n",
            "34078/36781 total: 92.65\n",
            "34079/36781 total: 92.65\n",
            "34080/36781 total: 92.66\n",
            "34081/36781 total: 92.66\n",
            "34082/36781 total: 92.66\n",
            "34083/36781 total: 92.66\n",
            "34084/36781 total: 92.67\n",
            "34085/36781 total: 92.67\n",
            "34086/36781 total: 92.67\n",
            "34087/36781 total: 92.68\n",
            "34088/36781 total: 92.68\n",
            "34089/36781 total: 92.68\n",
            "34090/36781 total: 92.68\n",
            "34091/36781 total: 92.69\n",
            "34092/36781 total: 92.69\n",
            "34093/36781 total: 92.69\n",
            "34094/36781 total: 92.69\n",
            "34095/36781 total: 92.7\n",
            "34096/36781 total: 92.7\n",
            "34097/36781 total: 92.7\n",
            "34098/36781 total: 92.71\n",
            "34099/36781 total: 92.71\n",
            "34100/36781 total: 92.71\n",
            "34101/36781 total: 92.71\n",
            "34102/36781 total: 92.72\n",
            "34103/36781 total: 92.72\n",
            "34104/36781 total: 92.72\n",
            "34105/36781 total: 92.72\n",
            "34106/36781 total: 92.73\n",
            "34107/36781 total: 92.73\n",
            "34108/36781 total: 92.73\n",
            "34109/36781 total: 92.74\n",
            "34110/36781 total: 92.74\n",
            "34111/36781 total: 92.74\n",
            "34112/36781 total: 92.74\n",
            "34113/36781 total: 92.75\n",
            "34114/36781 total: 92.75\n",
            "34115/36781 total: 92.75\n",
            "34116/36781 total: 92.75\n",
            "34117/36781 total: 92.76\n",
            "34118/36781 total: 92.76\n",
            "34119/36781 total: 92.76\n",
            "34120/36781 total: 92.77\n",
            "34121/36781 total: 92.77\n",
            "34122/36781 total: 92.77\n",
            "34123/36781 total: 92.77\n",
            "34124/36781 total: 92.78\n",
            "34125/36781 total: 92.78\n",
            "34126/36781 total: 92.78\n",
            "34127/36781 total: 92.78\n",
            "34128/36781 total: 92.79\n",
            "34129/36781 total: 92.79\n",
            "34130/36781 total: 92.79\n",
            "34131/36781 total: 92.8\n",
            "34132/36781 total: 92.8\n",
            "34133/36781 total: 92.8\n",
            "34134/36781 total: 92.8\n",
            "34135/36781 total: 92.81\n",
            "34136/36781 total: 92.81\n",
            "34137/36781 total: 92.81\n",
            "34138/36781 total: 92.81\n",
            "34139/36781 total: 92.82\n",
            "34140/36781 total: 92.82\n",
            "34141/36781 total: 92.82\n",
            "34142/36781 total: 92.83\n",
            "34143/36781 total: 92.83\n",
            "34144/36781 total: 92.83\n",
            "34145/36781 total: 92.83\n",
            "34146/36781 total: 92.84\n",
            "34147/36781 total: 92.84\n",
            "34148/36781 total: 92.84\n",
            "34149/36781 total: 92.84\n",
            "34150/36781 total: 92.85\n",
            "34151/36781 total: 92.85\n",
            "34152/36781 total: 92.85\n",
            "34153/36781 total: 92.86\n",
            "34154/36781 total: 92.86\n",
            "34155/36781 total: 92.86\n",
            "34156/36781 total: 92.86\n",
            "34157/36781 total: 92.87\n",
            "34158/36781 total: 92.87\n",
            "34159/36781 total: 92.87\n",
            "34160/36781 total: 92.87\n",
            "34161/36781 total: 92.88\n",
            "34162/36781 total: 92.88\n",
            "34163/36781 total: 92.88\n",
            "34164/36781 total: 92.88\n",
            "34165/36781 total: 92.89\n",
            "34166/36781 total: 92.89\n",
            "34167/36781 total: 92.89\n",
            "34168/36781 total: 92.9\n",
            "34169/36781 total: 92.9\n",
            "34170/36781 total: 92.9\n",
            "34171/36781 total: 92.9\n",
            "34172/36781 total: 92.91\n",
            "34173/36781 total: 92.91\n",
            "34174/36781 total: 92.91\n",
            "34175/36781 total: 92.91\n",
            "34176/36781 total: 92.92\n",
            "34177/36781 total: 92.92\n",
            "34178/36781 total: 92.92\n",
            "34179/36781 total: 92.93\n",
            "34180/36781 total: 92.93\n",
            "34181/36781 total: 92.93\n",
            "34182/36781 total: 92.93\n",
            "34183/36781 total: 92.94\n",
            "34184/36781 total: 92.94\n",
            "34185/36781 total: 92.94\n",
            "34186/36781 total: 92.94\n",
            "34187/36781 total: 92.95\n",
            "34188/36781 total: 92.95\n",
            "34189/36781 total: 92.95\n",
            "34190/36781 total: 92.96\n",
            "34191/36781 total: 92.96\n",
            "34192/36781 total: 92.96\n",
            "34193/36781 total: 92.96\n",
            "34194/36781 total: 92.97\n",
            "34195/36781 total: 92.97\n",
            "34196/36781 total: 92.97\n",
            "34197/36781 total: 92.97\n",
            "34198/36781 total: 92.98\n",
            "34199/36781 total: 92.98\n",
            "34200/36781 total: 92.98\n",
            "34201/36781 total: 92.99\n",
            "34202/36781 total: 92.99\n",
            "34203/36781 total: 92.99\n",
            "34204/36781 total: 92.99\n",
            "34205/36781 total: 93.0\n",
            "34206/36781 total: 93.0\n",
            "34207/36781 total: 93.0\n",
            "34208/36781 total: 93.0\n",
            "34209/36781 total: 93.01\n",
            "34210/36781 total: 93.01\n",
            "34211/36781 total: 93.01\n",
            "34212/36781 total: 93.02\n",
            "34213/36781 total: 93.02\n",
            "34214/36781 total: 93.02\n",
            "34215/36781 total: 93.02\n",
            "34216/36781 total: 93.03\n",
            "34217/36781 total: 93.03\n",
            "34218/36781 total: 93.03\n",
            "34219/36781 total: 93.03\n",
            "34220/36781 total: 93.04\n",
            "34221/36781 total: 93.04\n",
            "34222/36781 total: 93.04\n",
            "34223/36781 total: 93.05\n",
            "34224/36781 total: 93.05\n",
            "34225/36781 total: 93.05\n",
            "34226/36781 total: 93.05\n",
            "34227/36781 total: 93.06\n",
            "34228/36781 total: 93.06\n",
            "34229/36781 total: 93.06\n",
            "34230/36781 total: 93.06\n",
            "34231/36781 total: 93.07\n",
            "34232/36781 total: 93.07\n",
            "34233/36781 total: 93.07\n",
            "34234/36781 total: 93.08\n",
            "34235/36781 total: 93.08\n",
            "34236/36781 total: 93.08\n",
            "34237/36781 total: 93.08\n",
            "34238/36781 total: 93.09\n",
            "34239/36781 total: 93.09\n",
            "34240/36781 total: 93.09\n",
            "34241/36781 total: 93.09\n",
            "34242/36781 total: 93.1\n",
            "34243/36781 total: 93.1\n",
            "34244/36781 total: 93.1\n",
            "34245/36781 total: 93.11\n",
            "34246/36781 total: 93.11\n",
            "34247/36781 total: 93.11\n",
            "34248/36781 total: 93.11\n",
            "34249/36781 total: 93.12\n",
            "34250/36781 total: 93.12\n",
            "34251/36781 total: 93.12\n",
            "34252/36781 total: 93.12\n",
            "34253/36781 total: 93.13\n",
            "34254/36781 total: 93.13\n",
            "34255/36781 total: 93.13\n",
            "34256/36781 total: 93.14\n",
            "34257/36781 total: 93.14\n",
            "34258/36781 total: 93.14\n",
            "34259/36781 total: 93.14\n",
            "34260/36781 total: 93.15\n",
            "34261/36781 total: 93.15\n",
            "34262/36781 total: 93.15\n",
            "34263/36781 total: 93.15\n",
            "34264/36781 total: 93.16\n",
            "34265/36781 total: 93.16\n",
            "34266/36781 total: 93.16\n",
            "34267/36781 total: 93.16\n",
            "34268/36781 total: 93.17\n",
            "34269/36781 total: 93.17\n",
            "34270/36781 total: 93.17\n",
            "34271/36781 total: 93.18\n",
            "34272/36781 total: 93.18\n",
            "34273/36781 total: 93.18\n",
            "34274/36781 total: 93.18\n",
            "34275/36781 total: 93.19\n",
            "34276/36781 total: 93.19\n",
            "34277/36781 total: 93.19\n",
            "34278/36781 total: 93.19\n",
            "34279/36781 total: 93.2\n",
            "34280/36781 total: 93.2\n",
            "34281/36781 total: 93.2\n",
            "34282/36781 total: 93.21\n",
            "34283/36781 total: 93.21\n",
            "34284/36781 total: 93.21\n",
            "34285/36781 total: 93.21\n",
            "34286/36781 total: 93.22\n",
            "34287/36781 total: 93.22\n",
            "34288/36781 total: 93.22\n",
            "34289/36781 total: 93.22\n",
            "34290/36781 total: 93.23\n",
            "34291/36781 total: 93.23\n",
            "34292/36781 total: 93.23\n",
            "34293/36781 total: 93.24\n",
            "34294/36781 total: 93.24\n",
            "34295/36781 total: 93.24\n",
            "34296/36781 total: 93.24\n",
            "34297/36781 total: 93.25\n",
            "34298/36781 total: 93.25\n",
            "34299/36781 total: 93.25\n",
            "34300/36781 total: 93.25\n",
            "34301/36781 total: 93.26\n",
            "34302/36781 total: 93.26\n",
            "34303/36781 total: 93.26\n",
            "34304/36781 total: 93.27\n",
            "34305/36781 total: 93.27\n",
            "34306/36781 total: 93.27\n",
            "34307/36781 total: 93.27\n",
            "34308/36781 total: 93.28\n",
            "34309/36781 total: 93.28\n",
            "34310/36781 total: 93.28\n",
            "34311/36781 total: 93.28\n",
            "34312/36781 total: 93.29\n",
            "34313/36781 total: 93.29\n",
            "34314/36781 total: 93.29\n",
            "34315/36781 total: 93.3\n",
            "34316/36781 total: 93.3\n",
            "34317/36781 total: 93.3\n",
            "34318/36781 total: 93.3\n",
            "34319/36781 total: 93.31\n",
            "34320/36781 total: 93.31\n",
            "34321/36781 total: 93.31\n",
            "34322/36781 total: 93.31\n",
            "34323/36781 total: 93.32\n",
            "34324/36781 total: 93.32\n",
            "34325/36781 total: 93.32\n",
            "34326/36781 total: 93.33\n",
            "34327/36781 total: 93.33\n",
            "34328/36781 total: 93.33\n",
            "34329/36781 total: 93.33\n",
            "34330/36781 total: 93.34\n",
            "34331/36781 total: 93.34\n",
            "34332/36781 total: 93.34\n",
            "34333/36781 total: 93.34\n",
            "34334/36781 total: 93.35\n",
            "34335/36781 total: 93.35\n",
            "34336/36781 total: 93.35\n",
            "34337/36781 total: 93.36\n",
            "34338/36781 total: 93.36\n",
            "34339/36781 total: 93.36\n",
            "34340/36781 total: 93.36\n",
            "34341/36781 total: 93.37\n",
            "34342/36781 total: 93.37\n",
            "34343/36781 total: 93.37\n",
            "34344/36781 total: 93.37\n",
            "34345/36781 total: 93.38\n",
            "34346/36781 total: 93.38\n",
            "34347/36781 total: 93.38\n",
            "34348/36781 total: 93.39\n",
            "34349/36781 total: 93.39\n",
            "34350/36781 total: 93.39\n",
            "34351/36781 total: 93.39\n",
            "34352/36781 total: 93.4\n",
            "34353/36781 total: 93.4\n",
            "34354/36781 total: 93.4\n",
            "34355/36781 total: 93.4\n",
            "34356/36781 total: 93.41\n",
            "34357/36781 total: 93.41\n",
            "34358/36781 total: 93.41\n",
            "34359/36781 total: 93.42\n",
            "34360/36781 total: 93.42\n",
            "34361/36781 total: 93.42\n",
            "34362/36781 total: 93.42\n",
            "34363/36781 total: 93.43\n",
            "34364/36781 total: 93.43\n",
            "34365/36781 total: 93.43\n",
            "34366/36781 total: 93.43\n",
            "34367/36781 total: 93.44\n",
            "34368/36781 total: 93.44\n",
            "34369/36781 total: 93.44\n",
            "34370/36781 total: 93.44\n",
            "34371/36781 total: 93.45\n",
            "34372/36781 total: 93.45\n",
            "34373/36781 total: 93.45\n",
            "34374/36781 total: 93.46\n",
            "34375/36781 total: 93.46\n",
            "34376/36781 total: 93.46\n",
            "34377/36781 total: 93.46\n",
            "34378/36781 total: 93.47\n",
            "34379/36781 total: 93.47\n",
            "34380/36781 total: 93.47\n",
            "34381/36781 total: 93.47\n",
            "34382/36781 total: 93.48\n",
            "34383/36781 total: 93.48\n",
            "34384/36781 total: 93.48\n",
            "34385/36781 total: 93.49\n",
            "34386/36781 total: 93.49\n",
            "34387/36781 total: 93.49\n",
            "34388/36781 total: 93.49\n",
            "34389/36781 total: 93.5\n",
            "34390/36781 total: 93.5\n",
            "34391/36781 total: 93.5\n",
            "34392/36781 total: 93.5\n",
            "34393/36781 total: 93.51\n",
            "34394/36781 total: 93.51\n",
            "34395/36781 total: 93.51\n",
            "34396/36781 total: 93.52\n",
            "34397/36781 total: 93.52\n",
            "34398/36781 total: 93.52\n",
            "34399/36781 total: 93.52\n",
            "34400/36781 total: 93.53\n",
            "34401/36781 total: 93.53\n",
            "34402/36781 total: 93.53\n",
            "34403/36781 total: 93.53\n",
            "34404/36781 total: 93.54\n",
            "34405/36781 total: 93.54\n",
            "34406/36781 total: 93.54\n",
            "34407/36781 total: 93.55\n",
            "34408/36781 total: 93.55\n",
            "34409/36781 total: 93.55\n",
            "34410/36781 total: 93.55\n",
            "34411/36781 total: 93.56\n",
            "34412/36781 total: 93.56\n",
            "34413/36781 total: 93.56\n",
            "34414/36781 total: 93.56\n",
            "34415/36781 total: 93.57\n",
            "34416/36781 total: 93.57\n",
            "34417/36781 total: 93.57\n",
            "34418/36781 total: 93.58\n",
            "34419/36781 total: 93.58\n",
            "34420/36781 total: 93.58\n",
            "34421/36781 total: 93.58\n",
            "34422/36781 total: 93.59\n",
            "34423/36781 total: 93.59\n",
            "34424/36781 total: 93.59\n",
            "34425/36781 total: 93.59\n",
            "34426/36781 total: 93.6\n",
            "34427/36781 total: 93.6\n",
            "34428/36781 total: 93.6\n",
            "34429/36781 total: 93.61\n",
            "34430/36781 total: 93.61\n",
            "34431/36781 total: 93.61\n",
            "34432/36781 total: 93.61\n",
            "34433/36781 total: 93.62\n",
            "34434/36781 total: 93.62\n",
            "34435/36781 total: 93.62\n",
            "34436/36781 total: 93.62\n",
            "34437/36781 total: 93.63\n",
            "34438/36781 total: 93.63\n",
            "34439/36781 total: 93.63\n",
            "34440/36781 total: 93.64\n",
            "34441/36781 total: 93.64\n",
            "34442/36781 total: 93.64\n",
            "34443/36781 total: 93.64\n",
            "34444/36781 total: 93.65\n",
            "34445/36781 total: 93.65\n",
            "34446/36781 total: 93.65\n",
            "34447/36781 total: 93.65\n",
            "34448/36781 total: 93.66\n",
            "34449/36781 total: 93.66\n",
            "34450/36781 total: 93.66\n",
            "34451/36781 total: 93.67\n",
            "34452/36781 total: 93.67\n",
            "34453/36781 total: 93.67\n",
            "34454/36781 total: 93.67\n",
            "34455/36781 total: 93.68\n",
            "34456/36781 total: 93.68\n",
            "34457/36781 total: 93.68\n",
            "34458/36781 total: 93.68\n",
            "34459/36781 total: 93.69\n",
            "34460/36781 total: 93.69\n",
            "34461/36781 total: 93.69\n",
            "34462/36781 total: 93.7\n",
            "34463/36781 total: 93.7\n",
            "34464/36781 total: 93.7\n",
            "34465/36781 total: 93.7\n",
            "34466/36781 total: 93.71\n",
            "34467/36781 total: 93.71\n",
            "34468/36781 total: 93.71\n",
            "34469/36781 total: 93.71\n",
            "34470/36781 total: 93.72\n",
            "34471/36781 total: 93.72\n",
            "34472/36781 total: 93.72\n",
            "34473/36781 total: 93.73\n",
            "34474/36781 total: 93.73\n",
            "34475/36781 total: 93.73\n",
            "34476/36781 total: 93.73\n",
            "34477/36781 total: 93.74\n",
            "34478/36781 total: 93.74\n",
            "34479/36781 total: 93.74\n",
            "34480/36781 total: 93.74\n",
            "34481/36781 total: 93.75\n",
            "34482/36781 total: 93.75\n",
            "34483/36781 total: 93.75\n",
            "34484/36781 total: 93.75\n",
            "34485/36781 total: 93.76\n",
            "34486/36781 total: 93.76\n",
            "34487/36781 total: 93.76\n",
            "34488/36781 total: 93.77\n",
            "34489/36781 total: 93.77\n",
            "34490/36781 total: 93.77\n",
            "34491/36781 total: 93.77\n",
            "34492/36781 total: 93.78\n",
            "34493/36781 total: 93.78\n",
            "34494/36781 total: 93.78\n",
            "34495/36781 total: 93.78\n",
            "34496/36781 total: 93.79\n",
            "34497/36781 total: 93.79\n",
            "34498/36781 total: 93.79\n",
            "34499/36781 total: 93.8\n",
            "34500/36781 total: 93.8\n",
            "34501/36781 total: 93.8\n",
            "34502/36781 total: 93.8\n",
            "34503/36781 total: 93.81\n",
            "34504/36781 total: 93.81\n",
            "34505/36781 total: 93.81\n",
            "34506/36781 total: 93.81\n",
            "34507/36781 total: 93.82\n",
            "34508/36781 total: 93.82\n",
            "34509/36781 total: 93.82\n",
            "34510/36781 total: 93.83\n",
            "34511/36781 total: 93.83\n",
            "34512/36781 total: 93.83\n",
            "34513/36781 total: 93.83\n",
            "34514/36781 total: 93.84\n",
            "34515/36781 total: 93.84\n",
            "34516/36781 total: 93.84\n",
            "34517/36781 total: 93.84\n",
            "34518/36781 total: 93.85\n",
            "34519/36781 total: 93.85\n",
            "34520/36781 total: 93.85\n",
            "34521/36781 total: 93.86\n",
            "34522/36781 total: 93.86\n",
            "34523/36781 total: 93.86\n",
            "34524/36781 total: 93.86\n",
            "34525/36781 total: 93.87\n",
            "34526/36781 total: 93.87\n",
            "34527/36781 total: 93.87\n",
            "34528/36781 total: 93.87\n",
            "34529/36781 total: 93.88\n",
            "34530/36781 total: 93.88\n",
            "34531/36781 total: 93.88\n",
            "34532/36781 total: 93.89\n",
            "34533/36781 total: 93.89\n",
            "34534/36781 total: 93.89\n",
            "34535/36781 total: 93.89\n",
            "34536/36781 total: 93.9\n",
            "34537/36781 total: 93.9\n",
            "34538/36781 total: 93.9\n",
            "34539/36781 total: 93.9\n",
            "34540/36781 total: 93.91\n",
            "34541/36781 total: 93.91\n",
            "34542/36781 total: 93.91\n",
            "34543/36781 total: 93.92\n",
            "34544/36781 total: 93.92\n",
            "34545/36781 total: 93.92\n",
            "34546/36781 total: 93.92\n",
            "34547/36781 total: 93.93\n",
            "34548/36781 total: 93.93\n",
            "34549/36781 total: 93.93\n",
            "34550/36781 total: 93.93\n",
            "34551/36781 total: 93.94\n",
            "34552/36781 total: 93.94\n",
            "34553/36781 total: 93.94\n",
            "34554/36781 total: 93.95\n",
            "34555/36781 total: 93.95\n",
            "34556/36781 total: 93.95\n",
            "34557/36781 total: 93.95\n",
            "34558/36781 total: 93.96\n",
            "34559/36781 total: 93.96\n",
            "34560/36781 total: 93.96\n",
            "34561/36781 total: 93.96\n",
            "34562/36781 total: 93.97\n",
            "34563/36781 total: 93.97\n",
            "34564/36781 total: 93.97\n",
            "34565/36781 total: 93.98\n",
            "34566/36781 total: 93.98\n",
            "34567/36781 total: 93.98\n",
            "34568/36781 total: 93.98\n",
            "34569/36781 total: 93.99\n",
            "34570/36781 total: 93.99\n",
            "34571/36781 total: 93.99\n",
            "34572/36781 total: 93.99\n",
            "34573/36781 total: 94.0\n",
            "34574/36781 total: 94.0\n",
            "34575/36781 total: 94.0\n",
            "34576/36781 total: 94.01\n",
            "34577/36781 total: 94.01\n",
            "34578/36781 total: 94.01\n",
            "34579/36781 total: 94.01\n",
            "34580/36781 total: 94.02\n",
            "34581/36781 total: 94.02\n",
            "34582/36781 total: 94.02\n",
            "34583/36781 total: 94.02\n",
            "34584/36781 total: 94.03\n",
            "34585/36781 total: 94.03\n",
            "34586/36781 total: 94.03\n",
            "34587/36781 total: 94.03\n",
            "34588/36781 total: 94.04\n",
            "34589/36781 total: 94.04\n",
            "34590/36781 total: 94.04\n",
            "34591/36781 total: 94.05\n",
            "34592/36781 total: 94.05\n",
            "34593/36781 total: 94.05\n",
            "34594/36781 total: 94.05\n",
            "34595/36781 total: 94.06\n",
            "34596/36781 total: 94.06\n",
            "34597/36781 total: 94.06\n",
            "34598/36781 total: 94.06\n",
            "34599/36781 total: 94.07\n",
            "34600/36781 total: 94.07\n",
            "34601/36781 total: 94.07\n",
            "34602/36781 total: 94.08\n",
            "34603/36781 total: 94.08\n",
            "34604/36781 total: 94.08\n",
            "34605/36781 total: 94.08\n",
            "34606/36781 total: 94.09\n",
            "34607/36781 total: 94.09\n",
            "34608/36781 total: 94.09\n",
            "34609/36781 total: 94.09\n",
            "34610/36781 total: 94.1\n",
            "34611/36781 total: 94.1\n",
            "34612/36781 total: 94.1\n",
            "34613/36781 total: 94.11\n",
            "34614/36781 total: 94.11\n",
            "34615/36781 total: 94.11\n",
            "34616/36781 total: 94.11\n",
            "34617/36781 total: 94.12\n",
            "34618/36781 total: 94.12\n",
            "34619/36781 total: 94.12\n",
            "34620/36781 total: 94.12\n",
            "34621/36781 total: 94.13\n",
            "34622/36781 total: 94.13\n",
            "34623/36781 total: 94.13\n",
            "34624/36781 total: 94.14\n",
            "34625/36781 total: 94.14\n",
            "34626/36781 total: 94.14\n",
            "34627/36781 total: 94.14\n",
            "34628/36781 total: 94.15\n",
            "34629/36781 total: 94.15\n",
            "34630/36781 total: 94.15\n",
            "34631/36781 total: 94.15\n",
            "34632/36781 total: 94.16\n",
            "34633/36781 total: 94.16\n",
            "34634/36781 total: 94.16\n",
            "34635/36781 total: 94.17\n",
            "34636/36781 total: 94.17\n",
            "34637/36781 total: 94.17\n",
            "34638/36781 total: 94.17\n",
            "34639/36781 total: 94.18\n",
            "34640/36781 total: 94.18\n",
            "34641/36781 total: 94.18\n",
            "34642/36781 total: 94.18\n",
            "34643/36781 total: 94.19\n",
            "34644/36781 total: 94.19\n",
            "34645/36781 total: 94.19\n",
            "34646/36781 total: 94.2\n",
            "34647/36781 total: 94.2\n",
            "34648/36781 total: 94.2\n",
            "34649/36781 total: 94.2\n",
            "34650/36781 total: 94.21\n",
            "34651/36781 total: 94.21\n",
            "34652/36781 total: 94.21\n",
            "34653/36781 total: 94.21\n",
            "34654/36781 total: 94.22\n",
            "34655/36781 total: 94.22\n",
            "34656/36781 total: 94.22\n",
            "34657/36781 total: 94.23\n",
            "34658/36781 total: 94.23\n",
            "34659/36781 total: 94.23\n",
            "34660/36781 total: 94.23\n",
            "34661/36781 total: 94.24\n",
            "34662/36781 total: 94.24\n",
            "34663/36781 total: 94.24\n",
            "34664/36781 total: 94.24\n",
            "34665/36781 total: 94.25\n",
            "34666/36781 total: 94.25\n",
            "34667/36781 total: 94.25\n",
            "34668/36781 total: 94.26\n",
            "34669/36781 total: 94.26\n",
            "34670/36781 total: 94.26\n",
            "34671/36781 total: 94.26\n",
            "34672/36781 total: 94.27\n",
            "34673/36781 total: 94.27\n",
            "34674/36781 total: 94.27\n",
            "34675/36781 total: 94.27\n",
            "34676/36781 total: 94.28\n",
            "34677/36781 total: 94.28\n",
            "34678/36781 total: 94.28\n",
            "34679/36781 total: 94.29\n",
            "34680/36781 total: 94.29\n",
            "34681/36781 total: 94.29\n",
            "34682/36781 total: 94.29\n",
            "34683/36781 total: 94.3\n",
            "34684/36781 total: 94.3\n",
            "34685/36781 total: 94.3\n",
            "34686/36781 total: 94.3\n",
            "34687/36781 total: 94.31\n",
            "34688/36781 total: 94.31\n",
            "34689/36781 total: 94.31\n",
            "34690/36781 total: 94.31\n",
            "34691/36781 total: 94.32\n",
            "34692/36781 total: 94.32\n",
            "34693/36781 total: 94.32\n",
            "34694/36781 total: 94.33\n",
            "34695/36781 total: 94.33\n",
            "34696/36781 total: 94.33\n",
            "34697/36781 total: 94.33\n",
            "34698/36781 total: 94.34\n",
            "34699/36781 total: 94.34\n",
            "34700/36781 total: 94.34\n",
            "34701/36781 total: 94.34\n",
            "34702/36781 total: 94.35\n",
            "34703/36781 total: 94.35\n",
            "34704/36781 total: 94.35\n",
            "34705/36781 total: 94.36\n",
            "34706/36781 total: 94.36\n",
            "34707/36781 total: 94.36\n",
            "34708/36781 total: 94.36\n",
            "34709/36781 total: 94.37\n",
            "34710/36781 total: 94.37\n",
            "34711/36781 total: 94.37\n",
            "34712/36781 total: 94.37\n",
            "34713/36781 total: 94.38\n",
            "34714/36781 total: 94.38\n",
            "34715/36781 total: 94.38\n",
            "34716/36781 total: 94.39\n",
            "34717/36781 total: 94.39\n",
            "34718/36781 total: 94.39\n",
            "34719/36781 total: 94.39\n",
            "34720/36781 total: 94.4\n",
            "34721/36781 total: 94.4\n",
            "34722/36781 total: 94.4\n",
            "34723/36781 total: 94.4\n",
            "34724/36781 total: 94.41\n",
            "34725/36781 total: 94.41\n",
            "34726/36781 total: 94.41\n",
            "34727/36781 total: 94.42\n",
            "34728/36781 total: 94.42\n",
            "34729/36781 total: 94.42\n",
            "34730/36781 total: 94.42\n",
            "34731/36781 total: 94.43\n",
            "34732/36781 total: 94.43\n",
            "34733/36781 total: 94.43\n",
            "34734/36781 total: 94.43\n",
            "34735/36781 total: 94.44\n",
            "34736/36781 total: 94.44\n",
            "34737/36781 total: 94.44\n",
            "34738/36781 total: 94.45\n",
            "34739/36781 total: 94.45\n",
            "34740/36781 total: 94.45\n",
            "34741/36781 total: 94.45\n",
            "34742/36781 total: 94.46\n",
            "34743/36781 total: 94.46\n",
            "34744/36781 total: 94.46\n",
            "34745/36781 total: 94.46\n",
            "34746/36781 total: 94.47\n",
            "34747/36781 total: 94.47\n",
            "34748/36781 total: 94.47\n",
            "34749/36781 total: 94.48\n",
            "34750/36781 total: 94.48\n",
            "34751/36781 total: 94.48\n",
            "34752/36781 total: 94.48\n",
            "34753/36781 total: 94.49\n",
            "34754/36781 total: 94.49\n",
            "34755/36781 total: 94.49\n",
            "34756/36781 total: 94.49\n",
            "34757/36781 total: 94.5\n",
            "34758/36781 total: 94.5\n",
            "34759/36781 total: 94.5\n",
            "34760/36781 total: 94.51\n",
            "34761/36781 total: 94.51\n",
            "34762/36781 total: 94.51\n",
            "34763/36781 total: 94.51\n",
            "34764/36781 total: 94.52\n",
            "34765/36781 total: 94.52\n",
            "34766/36781 total: 94.52\n",
            "34767/36781 total: 94.52\n",
            "34768/36781 total: 94.53\n",
            "34769/36781 total: 94.53\n",
            "34770/36781 total: 94.53\n",
            "34771/36781 total: 94.54\n",
            "34772/36781 total: 94.54\n",
            "34773/36781 total: 94.54\n",
            "34774/36781 total: 94.54\n",
            "34775/36781 total: 94.55\n",
            "34776/36781 total: 94.55\n",
            "34777/36781 total: 94.55\n",
            "34778/36781 total: 94.55\n",
            "34779/36781 total: 94.56\n",
            "34780/36781 total: 94.56\n",
            "34781/36781 total: 94.56\n",
            "34782/36781 total: 94.57\n",
            "34783/36781 total: 94.57\n",
            "34784/36781 total: 94.57\n",
            "34785/36781 total: 94.57\n",
            "34786/36781 total: 94.58\n",
            "34787/36781 total: 94.58\n",
            "34788/36781 total: 94.58\n",
            "34789/36781 total: 94.58\n",
            "34790/36781 total: 94.59\n",
            "34791/36781 total: 94.59\n",
            "34792/36781 total: 94.59\n",
            "34793/36781 total: 94.6\n",
            "34794/36781 total: 94.6\n",
            "34795/36781 total: 94.6\n",
            "34796/36781 total: 94.6\n",
            "34797/36781 total: 94.61\n",
            "34798/36781 total: 94.61\n",
            "34799/36781 total: 94.61\n",
            "34800/36781 total: 94.61\n",
            "34801/36781 total: 94.62\n",
            "34802/36781 total: 94.62\n",
            "34803/36781 total: 94.62\n",
            "34804/36781 total: 94.62\n",
            "34805/36781 total: 94.63\n",
            "34806/36781 total: 94.63\n",
            "34807/36781 total: 94.63\n",
            "34808/36781 total: 94.64\n",
            "34809/36781 total: 94.64\n",
            "34810/36781 total: 94.64\n",
            "34811/36781 total: 94.64\n",
            "34812/36781 total: 94.65\n",
            "34813/36781 total: 94.65\n",
            "34814/36781 total: 94.65\n",
            "34815/36781 total: 94.65\n",
            "34816/36781 total: 94.66\n",
            "34817/36781 total: 94.66\n",
            "34818/36781 total: 94.66\n",
            "34819/36781 total: 94.67\n",
            "34820/36781 total: 94.67\n",
            "34821/36781 total: 94.67\n",
            "34822/36781 total: 94.67\n",
            "34823/36781 total: 94.68\n",
            "34824/36781 total: 94.68\n",
            "34825/36781 total: 94.68\n",
            "34826/36781 total: 94.68\n",
            "34827/36781 total: 94.69\n",
            "34828/36781 total: 94.69\n",
            "34829/36781 total: 94.69\n",
            "34830/36781 total: 94.7\n",
            "34831/36781 total: 94.7\n",
            "34832/36781 total: 94.7\n",
            "34833/36781 total: 94.7\n",
            "34834/36781 total: 94.71\n",
            "34835/36781 total: 94.71\n",
            "34836/36781 total: 94.71\n",
            "34837/36781 total: 94.71\n",
            "34838/36781 total: 94.72\n",
            "34839/36781 total: 94.72\n",
            "34840/36781 total: 94.72\n",
            "34841/36781 total: 94.73\n",
            "34842/36781 total: 94.73\n",
            "34843/36781 total: 94.73\n",
            "34844/36781 total: 94.73\n",
            "34845/36781 total: 94.74\n",
            "34846/36781 total: 94.74\n",
            "34847/36781 total: 94.74\n",
            "34848/36781 total: 94.74\n",
            "34849/36781 total: 94.75\n",
            "34850/36781 total: 94.75\n",
            "34851/36781 total: 94.75\n",
            "34852/36781 total: 94.76\n",
            "34853/36781 total: 94.76\n",
            "34854/36781 total: 94.76\n",
            "34855/36781 total: 94.76\n",
            "34856/36781 total: 94.77\n",
            "34857/36781 total: 94.77\n",
            "34858/36781 total: 94.77\n",
            "34859/36781 total: 94.77\n",
            "34860/36781 total: 94.78\n",
            "34861/36781 total: 94.78\n",
            "34862/36781 total: 94.78\n",
            "34863/36781 total: 94.79\n",
            "34864/36781 total: 94.79\n",
            "34865/36781 total: 94.79\n",
            "34866/36781 total: 94.79\n",
            "34867/36781 total: 94.8\n",
            "34868/36781 total: 94.8\n",
            "34869/36781 total: 94.8\n",
            "34870/36781 total: 94.8\n",
            "34871/36781 total: 94.81\n",
            "34872/36781 total: 94.81\n",
            "34873/36781 total: 94.81\n",
            "34874/36781 total: 94.82\n",
            "34875/36781 total: 94.82\n",
            "34876/36781 total: 94.82\n",
            "34877/36781 total: 94.82\n",
            "34878/36781 total: 94.83\n",
            "34879/36781 total: 94.83\n",
            "34880/36781 total: 94.83\n",
            "34881/36781 total: 94.83\n",
            "34882/36781 total: 94.84\n",
            "34883/36781 total: 94.84\n",
            "34884/36781 total: 94.84\n",
            "34885/36781 total: 94.85\n",
            "34886/36781 total: 94.85\n",
            "34887/36781 total: 94.85\n",
            "34888/36781 total: 94.85\n",
            "34889/36781 total: 94.86\n",
            "34890/36781 total: 94.86\n",
            "34891/36781 total: 94.86\n",
            "34892/36781 total: 94.86\n",
            "34893/36781 total: 94.87\n",
            "34894/36781 total: 94.87\n",
            "34895/36781 total: 94.87\n",
            "34896/36781 total: 94.88\n",
            "34897/36781 total: 94.88\n",
            "34898/36781 total: 94.88\n",
            "34899/36781 total: 94.88\n",
            "34900/36781 total: 94.89\n",
            "34901/36781 total: 94.89\n",
            "34902/36781 total: 94.89\n",
            "34903/36781 total: 94.89\n",
            "34904/36781 total: 94.9\n",
            "34905/36781 total: 94.9\n",
            "34906/36781 total: 94.9\n",
            "34907/36781 total: 94.9\n",
            "34908/36781 total: 94.91\n",
            "34909/36781 total: 94.91\n",
            "34910/36781 total: 94.91\n",
            "34911/36781 total: 94.92\n",
            "34912/36781 total: 94.92\n",
            "34913/36781 total: 94.92\n",
            "34914/36781 total: 94.92\n",
            "34915/36781 total: 94.93\n",
            "34916/36781 total: 94.93\n",
            "34917/36781 total: 94.93\n",
            "34918/36781 total: 94.93\n",
            "34919/36781 total: 94.94\n",
            "34920/36781 total: 94.94\n",
            "34921/36781 total: 94.94\n",
            "34922/36781 total: 94.95\n",
            "34923/36781 total: 94.95\n",
            "34924/36781 total: 94.95\n",
            "34925/36781 total: 94.95\n",
            "34926/36781 total: 94.96\n",
            "34927/36781 total: 94.96\n",
            "34928/36781 total: 94.96\n",
            "34929/36781 total: 94.96\n",
            "34930/36781 total: 94.97\n",
            "34931/36781 total: 94.97\n",
            "34932/36781 total: 94.97\n",
            "34933/36781 total: 94.98\n",
            "34934/36781 total: 94.98\n",
            "34935/36781 total: 94.98\n",
            "34936/36781 total: 94.98\n",
            "34937/36781 total: 94.99\n",
            "34938/36781 total: 94.99\n",
            "34939/36781 total: 94.99\n",
            "34940/36781 total: 94.99\n",
            "34941/36781 total: 95.0\n",
            "34942/36781 total: 95.0\n",
            "34943/36781 total: 95.0\n",
            "34944/36781 total: 95.01\n",
            "34945/36781 total: 95.01\n",
            "34946/36781 total: 95.01\n",
            "34947/36781 total: 95.01\n",
            "34948/36781 total: 95.02\n",
            "34949/36781 total: 95.02\n",
            "34950/36781 total: 95.02\n",
            "34951/36781 total: 95.02\n",
            "34952/36781 total: 95.03\n",
            "34953/36781 total: 95.03\n",
            "34954/36781 total: 95.03\n",
            "34955/36781 total: 95.04\n",
            "34956/36781 total: 95.04\n",
            "34957/36781 total: 95.04\n",
            "34958/36781 total: 95.04\n",
            "34959/36781 total: 95.05\n",
            "34960/36781 total: 95.05\n",
            "34961/36781 total: 95.05\n",
            "34962/36781 total: 95.05\n",
            "34963/36781 total: 95.06\n",
            "34964/36781 total: 95.06\n",
            "34965/36781 total: 95.06\n",
            "34966/36781 total: 95.07\n",
            "34967/36781 total: 95.07\n",
            "34968/36781 total: 95.07\n",
            "34969/36781 total: 95.07\n",
            "34970/36781 total: 95.08\n",
            "34971/36781 total: 95.08\n",
            "34972/36781 total: 95.08\n",
            "34973/36781 total: 95.08\n",
            "34974/36781 total: 95.09\n",
            "34975/36781 total: 95.09\n",
            "34976/36781 total: 95.09\n",
            "34977/36781 total: 95.1\n",
            "34978/36781 total: 95.1\n",
            "34979/36781 total: 95.1\n",
            "34980/36781 total: 95.1\n",
            "34981/36781 total: 95.11\n",
            "34982/36781 total: 95.11\n",
            "34983/36781 total: 95.11\n",
            "34984/36781 total: 95.11\n",
            "34985/36781 total: 95.12\n",
            "34986/36781 total: 95.12\n",
            "34987/36781 total: 95.12\n",
            "34988/36781 total: 95.13\n",
            "34989/36781 total: 95.13\n",
            "34990/36781 total: 95.13\n",
            "34991/36781 total: 95.13\n",
            "34992/36781 total: 95.14\n",
            "34993/36781 total: 95.14\n",
            "34994/36781 total: 95.14\n",
            "34995/36781 total: 95.14\n",
            "34996/36781 total: 95.15\n",
            "34997/36781 total: 95.15\n",
            "34998/36781 total: 95.15\n",
            "34999/36781 total: 95.16\n",
            "35000/36781 total: 95.16\n",
            "35001/36781 total: 95.16\n",
            "35002/36781 total: 95.16\n",
            "35003/36781 total: 95.17\n",
            "35004/36781 total: 95.17\n",
            "35005/36781 total: 95.17\n",
            "35006/36781 total: 95.17\n",
            "35007/36781 total: 95.18\n",
            "35008/36781 total: 95.18\n",
            "35009/36781 total: 95.18\n",
            "35010/36781 total: 95.19\n",
            "35011/36781 total: 95.19\n",
            "35012/36781 total: 95.19\n",
            "35013/36781 total: 95.19\n",
            "35014/36781 total: 95.2\n",
            "35015/36781 total: 95.2\n",
            "35016/36781 total: 95.2\n",
            "35017/36781 total: 95.2\n",
            "35018/36781 total: 95.21\n",
            "35019/36781 total: 95.21\n",
            "35020/36781 total: 95.21\n",
            "35021/36781 total: 95.21\n",
            "35022/36781 total: 95.22\n",
            "35023/36781 total: 95.22\n",
            "35024/36781 total: 95.22\n",
            "35025/36781 total: 95.23\n",
            "35026/36781 total: 95.23\n",
            "35027/36781 total: 95.23\n",
            "35028/36781 total: 95.23\n",
            "35029/36781 total: 95.24\n",
            "35030/36781 total: 95.24\n",
            "35031/36781 total: 95.24\n",
            "35032/36781 total: 95.24\n",
            "35033/36781 total: 95.25\n",
            "35034/36781 total: 95.25\n",
            "35035/36781 total: 95.25\n",
            "35036/36781 total: 95.26\n",
            "35037/36781 total: 95.26\n",
            "35038/36781 total: 95.26\n",
            "35039/36781 total: 95.26\n",
            "35040/36781 total: 95.27\n",
            "35041/36781 total: 95.27\n",
            "35042/36781 total: 95.27\n",
            "35043/36781 total: 95.27\n",
            "35044/36781 total: 95.28\n",
            "35045/36781 total: 95.28\n",
            "35046/36781 total: 95.28\n",
            "35047/36781 total: 95.29\n",
            "35048/36781 total: 95.29\n",
            "35049/36781 total: 95.29\n",
            "35050/36781 total: 95.29\n",
            "35051/36781 total: 95.3\n",
            "35052/36781 total: 95.3\n",
            "35053/36781 total: 95.3\n",
            "35054/36781 total: 95.3\n",
            "35055/36781 total: 95.31\n",
            "35056/36781 total: 95.31\n",
            "35057/36781 total: 95.31\n",
            "35058/36781 total: 95.32\n",
            "35059/36781 total: 95.32\n",
            "35060/36781 total: 95.32\n",
            "35061/36781 total: 95.32\n",
            "35062/36781 total: 95.33\n",
            "35063/36781 total: 95.33\n",
            "35064/36781 total: 95.33\n",
            "35065/36781 total: 95.33\n",
            "35066/36781 total: 95.34\n",
            "35067/36781 total: 95.34\n",
            "35068/36781 total: 95.34\n",
            "35069/36781 total: 95.35\n",
            "35070/36781 total: 95.35\n",
            "35071/36781 total: 95.35\n",
            "35072/36781 total: 95.35\n",
            "35073/36781 total: 95.36\n",
            "35074/36781 total: 95.36\n",
            "35075/36781 total: 95.36\n",
            "35076/36781 total: 95.36\n",
            "35077/36781 total: 95.37\n",
            "35078/36781 total: 95.37\n",
            "35079/36781 total: 95.37\n",
            "35080/36781 total: 95.38\n",
            "35081/36781 total: 95.38\n",
            "35082/36781 total: 95.38\n",
            "35083/36781 total: 95.38\n",
            "35084/36781 total: 95.39\n",
            "35085/36781 total: 95.39\n",
            "35086/36781 total: 95.39\n",
            "35087/36781 total: 95.39\n",
            "35088/36781 total: 95.4\n",
            "35089/36781 total: 95.4\n",
            "35090/36781 total: 95.4\n",
            "35091/36781 total: 95.41\n",
            "35092/36781 total: 95.41\n",
            "35093/36781 total: 95.41\n",
            "35094/36781 total: 95.41\n",
            "35095/36781 total: 95.42\n",
            "35096/36781 total: 95.42\n",
            "35097/36781 total: 95.42\n",
            "35098/36781 total: 95.42\n",
            "35099/36781 total: 95.43\n",
            "35100/36781 total: 95.43\n",
            "35101/36781 total: 95.43\n",
            "35102/36781 total: 95.44\n",
            "35103/36781 total: 95.44\n",
            "35104/36781 total: 95.44\n",
            "35105/36781 total: 95.44\n",
            "35106/36781 total: 95.45\n",
            "35107/36781 total: 95.45\n",
            "35108/36781 total: 95.45\n",
            "35109/36781 total: 95.45\n",
            "35110/36781 total: 95.46\n",
            "35111/36781 total: 95.46\n",
            "35112/36781 total: 95.46\n",
            "35113/36781 total: 95.47\n",
            "35114/36781 total: 95.47\n",
            "35115/36781 total: 95.47\n",
            "35116/36781 total: 95.47\n",
            "35117/36781 total: 95.48\n",
            "35118/36781 total: 95.48\n",
            "35119/36781 total: 95.48\n",
            "35120/36781 total: 95.48\n",
            "35121/36781 total: 95.49\n",
            "35122/36781 total: 95.49\n",
            "35123/36781 total: 95.49\n",
            "35124/36781 total: 95.49\n",
            "35125/36781 total: 95.5\n",
            "35126/36781 total: 95.5\n",
            "35127/36781 total: 95.5\n",
            "35128/36781 total: 95.51\n",
            "35129/36781 total: 95.51\n",
            "35130/36781 total: 95.51\n",
            "35131/36781 total: 95.51\n",
            "35132/36781 total: 95.52\n",
            "35133/36781 total: 95.52\n",
            "35134/36781 total: 95.52\n",
            "35135/36781 total: 95.52\n",
            "35136/36781 total: 95.53\n",
            "35137/36781 total: 95.53\n",
            "35138/36781 total: 95.53\n",
            "35139/36781 total: 95.54\n",
            "35140/36781 total: 95.54\n",
            "35141/36781 total: 95.54\n",
            "35142/36781 total: 95.54\n",
            "35143/36781 total: 95.55\n",
            "35144/36781 total: 95.55\n",
            "35145/36781 total: 95.55\n",
            "35146/36781 total: 95.55\n",
            "35147/36781 total: 95.56\n",
            "35148/36781 total: 95.56\n",
            "35149/36781 total: 95.56\n",
            "35150/36781 total: 95.57\n",
            "35151/36781 total: 95.57\n",
            "35152/36781 total: 95.57\n",
            "35153/36781 total: 95.57\n",
            "35154/36781 total: 95.58\n",
            "35155/36781 total: 95.58\n",
            "35156/36781 total: 95.58\n",
            "35157/36781 total: 95.58\n",
            "35158/36781 total: 95.59\n",
            "35159/36781 total: 95.59\n",
            "35160/36781 total: 95.59\n",
            "35161/36781 total: 95.6\n",
            "35162/36781 total: 95.6\n",
            "35163/36781 total: 95.6\n",
            "35164/36781 total: 95.6\n",
            "35165/36781 total: 95.61\n",
            "35166/36781 total: 95.61\n",
            "35167/36781 total: 95.61\n",
            "35168/36781 total: 95.61\n",
            "35169/36781 total: 95.62\n",
            "35170/36781 total: 95.62\n",
            "35171/36781 total: 95.62\n",
            "35172/36781 total: 95.63\n",
            "35173/36781 total: 95.63\n",
            "35174/36781 total: 95.63\n",
            "35175/36781 total: 95.63\n",
            "35176/36781 total: 95.64\n",
            "35177/36781 total: 95.64\n",
            "35178/36781 total: 95.64\n",
            "35179/36781 total: 95.64\n",
            "35180/36781 total: 95.65\n",
            "35181/36781 total: 95.65\n",
            "35182/36781 total: 95.65\n",
            "35183/36781 total: 95.66\n",
            "35184/36781 total: 95.66\n",
            "35185/36781 total: 95.66\n",
            "35186/36781 total: 95.66\n",
            "35187/36781 total: 95.67\n",
            "35188/36781 total: 95.67\n",
            "35189/36781 total: 95.67\n",
            "35190/36781 total: 95.67\n",
            "35191/36781 total: 95.68\n",
            "35192/36781 total: 95.68\n",
            "35193/36781 total: 95.68\n",
            "35194/36781 total: 95.69\n",
            "35195/36781 total: 95.69\n",
            "35196/36781 total: 95.69\n",
            "35197/36781 total: 95.69\n",
            "35198/36781 total: 95.7\n",
            "35199/36781 total: 95.7\n",
            "35200/36781 total: 95.7\n",
            "35201/36781 total: 95.7\n",
            "35202/36781 total: 95.71\n",
            "35203/36781 total: 95.71\n",
            "35204/36781 total: 95.71\n",
            "35205/36781 total: 95.72\n",
            "35206/36781 total: 95.72\n",
            "35207/36781 total: 95.72\n",
            "35208/36781 total: 95.72\n",
            "35209/36781 total: 95.73\n",
            "35210/36781 total: 95.73\n",
            "35211/36781 total: 95.73\n",
            "35212/36781 total: 95.73\n",
            "35213/36781 total: 95.74\n",
            "35214/36781 total: 95.74\n",
            "35215/36781 total: 95.74\n",
            "35216/36781 total: 95.75\n",
            "35217/36781 total: 95.75\n",
            "35218/36781 total: 95.75\n",
            "35219/36781 total: 95.75\n",
            "35220/36781 total: 95.76\n",
            "35221/36781 total: 95.76\n",
            "35222/36781 total: 95.76\n",
            "35223/36781 total: 95.76\n",
            "35224/36781 total: 95.77\n",
            "35225/36781 total: 95.77\n",
            "35226/36781 total: 95.77\n",
            "35227/36781 total: 95.77\n",
            "35228/36781 total: 95.78\n",
            "35229/36781 total: 95.78\n",
            "35230/36781 total: 95.78\n",
            "35231/36781 total: 95.79\n",
            "35232/36781 total: 95.79\n",
            "35233/36781 total: 95.79\n",
            "35234/36781 total: 95.79\n",
            "35235/36781 total: 95.8\n",
            "35236/36781 total: 95.8\n",
            "35237/36781 total: 95.8\n",
            "35238/36781 total: 95.8\n",
            "35239/36781 total: 95.81\n",
            "35240/36781 total: 95.81\n",
            "35241/36781 total: 95.81\n",
            "35242/36781 total: 95.82\n",
            "35243/36781 total: 95.82\n",
            "35244/36781 total: 95.82\n",
            "35245/36781 total: 95.82\n",
            "35246/36781 total: 95.83\n",
            "35247/36781 total: 95.83\n",
            "35248/36781 total: 95.83\n",
            "35249/36781 total: 95.83\n",
            "35250/36781 total: 95.84\n",
            "35251/36781 total: 95.84\n",
            "35252/36781 total: 95.84\n",
            "35253/36781 total: 95.85\n",
            "35254/36781 total: 95.85\n",
            "35255/36781 total: 95.85\n",
            "35256/36781 total: 95.85\n",
            "35257/36781 total: 95.86\n",
            "35258/36781 total: 95.86\n",
            "35259/36781 total: 95.86\n",
            "35260/36781 total: 95.86\n",
            "35261/36781 total: 95.87\n",
            "35262/36781 total: 95.87\n",
            "35263/36781 total: 95.87\n",
            "35264/36781 total: 95.88\n",
            "35265/36781 total: 95.88\n",
            "35266/36781 total: 95.88\n",
            "35267/36781 total: 95.88\n",
            "35268/36781 total: 95.89\n",
            "35269/36781 total: 95.89\n",
            "35270/36781 total: 95.89\n",
            "35271/36781 total: 95.89\n",
            "35272/36781 total: 95.9\n",
            "35273/36781 total: 95.9\n",
            "35274/36781 total: 95.9\n",
            "35275/36781 total: 95.91\n",
            "35276/36781 total: 95.91\n",
            "35277/36781 total: 95.91\n",
            "35278/36781 total: 95.91\n",
            "35279/36781 total: 95.92\n",
            "35280/36781 total: 95.92\n",
            "35281/36781 total: 95.92\n",
            "35282/36781 total: 95.92\n",
            "35283/36781 total: 95.93\n",
            "35284/36781 total: 95.93\n",
            "35285/36781 total: 95.93\n",
            "35286/36781 total: 95.94\n",
            "35287/36781 total: 95.94\n",
            "35288/36781 total: 95.94\n",
            "35289/36781 total: 95.94\n",
            "35290/36781 total: 95.95\n",
            "35291/36781 total: 95.95\n",
            "35292/36781 total: 95.95\n",
            "35293/36781 total: 95.95\n",
            "35294/36781 total: 95.96\n",
            "35295/36781 total: 95.96\n",
            "35296/36781 total: 95.96\n",
            "35297/36781 total: 95.97\n",
            "35298/36781 total: 95.97\n",
            "35299/36781 total: 95.97\n",
            "35300/36781 total: 95.97\n",
            "35301/36781 total: 95.98\n",
            "35302/36781 total: 95.98\n",
            "35303/36781 total: 95.98\n",
            "35304/36781 total: 95.98\n",
            "35305/36781 total: 95.99\n",
            "35306/36781 total: 95.99\n",
            "35307/36781 total: 95.99\n",
            "35308/36781 total: 96.0\n",
            "35309/36781 total: 96.0\n",
            "35310/36781 total: 96.0\n",
            "35311/36781 total: 96.0\n",
            "35312/36781 total: 96.01\n",
            "35313/36781 total: 96.01\n",
            "35314/36781 total: 96.01\n",
            "35315/36781 total: 96.01\n",
            "35316/36781 total: 96.02\n",
            "35317/36781 total: 96.02\n",
            "35318/36781 total: 96.02\n",
            "35319/36781 total: 96.03\n",
            "35320/36781 total: 96.03\n",
            "35321/36781 total: 96.03\n",
            "35322/36781 total: 96.03\n",
            "35323/36781 total: 96.04\n",
            "35324/36781 total: 96.04\n",
            "35325/36781 total: 96.04\n",
            "35326/36781 total: 96.04\n",
            "35327/36781 total: 96.05\n",
            "35328/36781 total: 96.05\n",
            "35329/36781 total: 96.05\n",
            "35330/36781 total: 96.06\n",
            "35331/36781 total: 96.06\n",
            "35332/36781 total: 96.06\n",
            "35333/36781 total: 96.06\n",
            "35334/36781 total: 96.07\n",
            "35335/36781 total: 96.07\n",
            "35336/36781 total: 96.07\n",
            "35337/36781 total: 96.07\n",
            "35338/36781 total: 96.08\n",
            "35339/36781 total: 96.08\n",
            "35340/36781 total: 96.08\n",
            "35341/36781 total: 96.08\n",
            "35342/36781 total: 96.09\n",
            "35343/36781 total: 96.09\n",
            "35344/36781 total: 96.09\n",
            "35345/36781 total: 96.1\n",
            "35346/36781 total: 96.1\n",
            "35347/36781 total: 96.1\n",
            "35348/36781 total: 96.1\n",
            "35349/36781 total: 96.11\n",
            "35350/36781 total: 96.11\n",
            "35351/36781 total: 96.11\n",
            "35352/36781 total: 96.11\n",
            "35353/36781 total: 96.12\n",
            "35354/36781 total: 96.12\n",
            "35355/36781 total: 96.12\n",
            "35356/36781 total: 96.13\n",
            "35357/36781 total: 96.13\n",
            "35358/36781 total: 96.13\n",
            "35359/36781 total: 96.13\n",
            "35360/36781 total: 96.14\n",
            "35361/36781 total: 96.14\n",
            "35362/36781 total: 96.14\n",
            "35363/36781 total: 96.14\n",
            "35364/36781 total: 96.15\n",
            "35365/36781 total: 96.15\n",
            "35366/36781 total: 96.15\n",
            "35367/36781 total: 96.16\n",
            "35368/36781 total: 96.16\n",
            "35369/36781 total: 96.16\n",
            "35370/36781 total: 96.16\n",
            "35371/36781 total: 96.17\n",
            "35372/36781 total: 96.17\n",
            "35373/36781 total: 96.17\n",
            "35374/36781 total: 96.17\n",
            "35375/36781 total: 96.18\n",
            "35376/36781 total: 96.18\n",
            "35377/36781 total: 96.18\n",
            "35378/36781 total: 96.19\n",
            "35379/36781 total: 96.19\n",
            "35380/36781 total: 96.19\n",
            "35381/36781 total: 96.19\n",
            "35382/36781 total: 96.2\n",
            "35383/36781 total: 96.2\n",
            "35384/36781 total: 96.2\n",
            "35385/36781 total: 96.2\n",
            "35386/36781 total: 96.21\n",
            "35387/36781 total: 96.21\n",
            "35388/36781 total: 96.21\n",
            "35389/36781 total: 96.22\n",
            "35390/36781 total: 96.22\n",
            "35391/36781 total: 96.22\n",
            "35392/36781 total: 96.22\n",
            "35393/36781 total: 96.23\n",
            "35394/36781 total: 96.23\n",
            "35395/36781 total: 96.23\n",
            "35396/36781 total: 96.23\n",
            "35397/36781 total: 96.24\n",
            "35398/36781 total: 96.24\n",
            "35399/36781 total: 96.24\n",
            "35400/36781 total: 96.25\n",
            "35401/36781 total: 96.25\n",
            "35402/36781 total: 96.25\n",
            "35403/36781 total: 96.25\n",
            "35404/36781 total: 96.26\n",
            "35405/36781 total: 96.26\n",
            "35406/36781 total: 96.26\n",
            "35407/36781 total: 96.26\n",
            "35408/36781 total: 96.27\n",
            "35409/36781 total: 96.27\n",
            "35410/36781 total: 96.27\n",
            "35411/36781 total: 96.28\n",
            "35412/36781 total: 96.28\n",
            "35413/36781 total: 96.28\n",
            "35414/36781 total: 96.28\n",
            "35415/36781 total: 96.29\n",
            "35416/36781 total: 96.29\n",
            "35417/36781 total: 96.29\n",
            "35418/36781 total: 96.29\n",
            "35419/36781 total: 96.3\n",
            "35420/36781 total: 96.3\n",
            "35421/36781 total: 96.3\n",
            "35422/36781 total: 96.31\n",
            "35423/36781 total: 96.31\n",
            "35424/36781 total: 96.31\n",
            "35425/36781 total: 96.31\n",
            "35426/36781 total: 96.32\n",
            "35427/36781 total: 96.32\n",
            "35428/36781 total: 96.32\n",
            "35429/36781 total: 96.32\n",
            "35430/36781 total: 96.33\n",
            "35431/36781 total: 96.33\n",
            "35432/36781 total: 96.33\n",
            "35433/36781 total: 96.34\n",
            "35434/36781 total: 96.34\n",
            "35435/36781 total: 96.34\n",
            "35436/36781 total: 96.34\n",
            "35437/36781 total: 96.35\n",
            "35438/36781 total: 96.35\n",
            "35439/36781 total: 96.35\n",
            "35440/36781 total: 96.35\n",
            "35441/36781 total: 96.36\n",
            "35442/36781 total: 96.36\n",
            "35443/36781 total: 96.36\n",
            "35444/36781 total: 96.36\n",
            "35445/36781 total: 96.37\n",
            "35446/36781 total: 96.37\n",
            "35447/36781 total: 96.37\n",
            "35448/36781 total: 96.38\n",
            "35449/36781 total: 96.38\n",
            "35450/36781 total: 96.38\n",
            "35451/36781 total: 96.38\n",
            "35452/36781 total: 96.39\n",
            "35453/36781 total: 96.39\n",
            "35454/36781 total: 96.39\n",
            "35455/36781 total: 96.39\n",
            "35456/36781 total: 96.4\n",
            "35457/36781 total: 96.4\n",
            "35458/36781 total: 96.4\n",
            "35459/36781 total: 96.41\n",
            "35460/36781 total: 96.41\n",
            "35461/36781 total: 96.41\n",
            "35462/36781 total: 96.41\n",
            "35463/36781 total: 96.42\n",
            "35464/36781 total: 96.42\n",
            "35465/36781 total: 96.42\n",
            "35466/36781 total: 96.42\n",
            "35467/36781 total: 96.43\n",
            "35468/36781 total: 96.43\n",
            "35469/36781 total: 96.43\n",
            "35470/36781 total: 96.44\n",
            "35471/36781 total: 96.44\n",
            "35472/36781 total: 96.44\n",
            "35473/36781 total: 96.44\n",
            "35474/36781 total: 96.45\n",
            "35475/36781 total: 96.45\n",
            "35476/36781 total: 96.45\n",
            "35477/36781 total: 96.45\n",
            "35478/36781 total: 96.46\n",
            "35479/36781 total: 96.46\n",
            "35480/36781 total: 96.46\n",
            "35481/36781 total: 96.47\n",
            "35482/36781 total: 96.47\n",
            "35483/36781 total: 96.47\n",
            "35484/36781 total: 96.47\n",
            "35485/36781 total: 96.48\n",
            "35486/36781 total: 96.48\n",
            "35487/36781 total: 96.48\n",
            "35488/36781 total: 96.48\n",
            "35489/36781 total: 96.49\n",
            "35490/36781 total: 96.49\n",
            "35491/36781 total: 96.49\n",
            "35492/36781 total: 96.5\n",
            "35493/36781 total: 96.5\n",
            "35494/36781 total: 96.5\n",
            "35495/36781 total: 96.5\n",
            "35496/36781 total: 96.51\n",
            "35497/36781 total: 96.51\n",
            "35498/36781 total: 96.51\n",
            "35499/36781 total: 96.51\n",
            "35500/36781 total: 96.52\n",
            "35501/36781 total: 96.52\n",
            "35502/36781 total: 96.52\n",
            "35503/36781 total: 96.53\n",
            "35504/36781 total: 96.53\n",
            "35505/36781 total: 96.53\n",
            "35506/36781 total: 96.53\n",
            "35507/36781 total: 96.54\n",
            "35508/36781 total: 96.54\n",
            "35509/36781 total: 96.54\n",
            "35510/36781 total: 96.54\n",
            "35511/36781 total: 96.55\n",
            "35512/36781 total: 96.55\n",
            "35513/36781 total: 96.55\n",
            "35514/36781 total: 96.56\n",
            "35515/36781 total: 96.56\n",
            "35516/36781 total: 96.56\n",
            "35517/36781 total: 96.56\n",
            "35518/36781 total: 96.57\n",
            "35519/36781 total: 96.57\n",
            "35520/36781 total: 96.57\n",
            "35521/36781 total: 96.57\n",
            "35522/36781 total: 96.58\n",
            "35523/36781 total: 96.58\n",
            "35524/36781 total: 96.58\n",
            "35525/36781 total: 96.59\n",
            "35526/36781 total: 96.59\n",
            "35527/36781 total: 96.59\n",
            "35528/36781 total: 96.59\n",
            "35529/36781 total: 96.6\n",
            "35530/36781 total: 96.6\n",
            "35531/36781 total: 96.6\n",
            "35532/36781 total: 96.6\n",
            "35533/36781 total: 96.61\n",
            "35534/36781 total: 96.61\n",
            "35535/36781 total: 96.61\n",
            "35536/36781 total: 96.62\n",
            "35537/36781 total: 96.62\n",
            "35538/36781 total: 96.62\n",
            "35539/36781 total: 96.62\n",
            "35540/36781 total: 96.63\n",
            "35541/36781 total: 96.63\n",
            "35542/36781 total: 96.63\n",
            "35543/36781 total: 96.63\n",
            "35544/36781 total: 96.64\n",
            "35545/36781 total: 96.64\n",
            "35546/36781 total: 96.64\n",
            "35547/36781 total: 96.65\n",
            "35548/36781 total: 96.65\n",
            "35549/36781 total: 96.65\n",
            "35550/36781 total: 96.65\n",
            "35551/36781 total: 96.66\n",
            "35552/36781 total: 96.66\n",
            "35553/36781 total: 96.66\n",
            "35554/36781 total: 96.66\n",
            "35555/36781 total: 96.67\n",
            "35556/36781 total: 96.67\n",
            "35557/36781 total: 96.67\n",
            "35558/36781 total: 96.67\n",
            "35559/36781 total: 96.68\n",
            "35560/36781 total: 96.68\n",
            "35561/36781 total: 96.68\n",
            "35562/36781 total: 96.69\n",
            "35563/36781 total: 96.69\n",
            "35564/36781 total: 96.69\n",
            "35565/36781 total: 96.69\n",
            "35566/36781 total: 96.7\n",
            "35567/36781 total: 96.7\n",
            "35568/36781 total: 96.7\n",
            "35569/36781 total: 96.7\n",
            "35570/36781 total: 96.71\n",
            "35571/36781 total: 96.71\n",
            "35572/36781 total: 96.71\n",
            "35573/36781 total: 96.72\n",
            "35574/36781 total: 96.72\n",
            "35575/36781 total: 96.72\n",
            "35576/36781 total: 96.72\n",
            "35577/36781 total: 96.73\n",
            "35578/36781 total: 96.73\n",
            "35579/36781 total: 96.73\n",
            "35580/36781 total: 96.73\n",
            "35581/36781 total: 96.74\n",
            "35582/36781 total: 96.74\n",
            "35583/36781 total: 96.74\n",
            "35584/36781 total: 96.75\n",
            "35585/36781 total: 96.75\n",
            "35586/36781 total: 96.75\n",
            "35587/36781 total: 96.75\n",
            "35588/36781 total: 96.76\n",
            "35589/36781 total: 96.76\n",
            "35590/36781 total: 96.76\n",
            "35591/36781 total: 96.76\n",
            "35592/36781 total: 96.77\n",
            "35593/36781 total: 96.77\n",
            "35594/36781 total: 96.77\n",
            "35595/36781 total: 96.78\n",
            "35596/36781 total: 96.78\n",
            "35597/36781 total: 96.78\n",
            "35598/36781 total: 96.78\n",
            "35599/36781 total: 96.79\n",
            "35600/36781 total: 96.79\n",
            "35601/36781 total: 96.79\n",
            "35602/36781 total: 96.79\n",
            "35603/36781 total: 96.8\n",
            "35604/36781 total: 96.8\n",
            "35605/36781 total: 96.8\n",
            "35606/36781 total: 96.81\n",
            "35607/36781 total: 96.81\n",
            "35608/36781 total: 96.81\n",
            "35609/36781 total: 96.81\n",
            "35610/36781 total: 96.82\n",
            "35611/36781 total: 96.82\n",
            "35612/36781 total: 96.82\n",
            "35613/36781 total: 96.82\n",
            "35614/36781 total: 96.83\n",
            "35615/36781 total: 96.83\n",
            "35616/36781 total: 96.83\n",
            "35617/36781 total: 96.84\n",
            "35618/36781 total: 96.84\n",
            "35619/36781 total: 96.84\n",
            "35620/36781 total: 96.84\n",
            "35621/36781 total: 96.85\n",
            "35622/36781 total: 96.85\n",
            "35623/36781 total: 96.85\n",
            "35624/36781 total: 96.85\n",
            "35625/36781 total: 96.86\n",
            "35626/36781 total: 96.86\n",
            "35627/36781 total: 96.86\n",
            "35628/36781 total: 96.87\n",
            "35629/36781 total: 96.87\n",
            "35630/36781 total: 96.87\n",
            "35631/36781 total: 96.87\n",
            "35632/36781 total: 96.88\n",
            "35633/36781 total: 96.88\n",
            "35634/36781 total: 96.88\n",
            "35635/36781 total: 96.88\n",
            "35636/36781 total: 96.89\n",
            "35637/36781 total: 96.89\n",
            "35638/36781 total: 96.89\n",
            "35639/36781 total: 96.9\n",
            "35640/36781 total: 96.9\n",
            "35641/36781 total: 96.9\n",
            "35642/36781 total: 96.9\n",
            "35643/36781 total: 96.91\n",
            "35644/36781 total: 96.91\n",
            "35645/36781 total: 96.91\n",
            "35646/36781 total: 96.91\n",
            "35647/36781 total: 96.92\n",
            "35648/36781 total: 96.92\n",
            "35649/36781 total: 96.92\n",
            "35650/36781 total: 96.93\n",
            "35651/36781 total: 96.93\n",
            "35652/36781 total: 96.93\n",
            "35653/36781 total: 96.93\n",
            "35654/36781 total: 96.94\n",
            "35655/36781 total: 96.94\n",
            "35656/36781 total: 96.94\n",
            "35657/36781 total: 96.94\n",
            "35658/36781 total: 96.95\n",
            "35659/36781 total: 96.95\n",
            "35660/36781 total: 96.95\n",
            "35661/36781 total: 96.95\n",
            "35662/36781 total: 96.96\n",
            "35663/36781 total: 96.96\n",
            "35664/36781 total: 96.96\n",
            "35665/36781 total: 96.97\n",
            "35666/36781 total: 96.97\n",
            "35667/36781 total: 96.97\n",
            "35668/36781 total: 96.97\n",
            "35669/36781 total: 96.98\n",
            "35670/36781 total: 96.98\n",
            "35671/36781 total: 96.98\n",
            "35672/36781 total: 96.98\n",
            "35673/36781 total: 96.99\n",
            "35674/36781 total: 96.99\n",
            "35675/36781 total: 96.99\n",
            "35676/36781 total: 97.0\n",
            "35677/36781 total: 97.0\n",
            "35678/36781 total: 97.0\n",
            "35679/36781 total: 97.0\n",
            "35680/36781 total: 97.01\n",
            "35681/36781 total: 97.01\n",
            "35682/36781 total: 97.01\n",
            "35683/36781 total: 97.01\n",
            "35684/36781 total: 97.02\n",
            "35685/36781 total: 97.02\n",
            "35686/36781 total: 97.02\n",
            "35687/36781 total: 97.03\n",
            "35688/36781 total: 97.03\n",
            "35689/36781 total: 97.03\n",
            "35690/36781 total: 97.03\n",
            "35691/36781 total: 97.04\n",
            "35692/36781 total: 97.04\n",
            "35693/36781 total: 97.04\n",
            "35694/36781 total: 97.04\n",
            "35695/36781 total: 97.05\n",
            "35696/36781 total: 97.05\n",
            "35697/36781 total: 97.05\n",
            "35698/36781 total: 97.06\n",
            "35699/36781 total: 97.06\n",
            "35700/36781 total: 97.06\n",
            "35701/36781 total: 97.06\n",
            "35702/36781 total: 97.07\n",
            "35703/36781 total: 97.07\n",
            "35704/36781 total: 97.07\n",
            "35705/36781 total: 97.07\n",
            "35706/36781 total: 97.08\n",
            "35707/36781 total: 97.08\n",
            "35708/36781 total: 97.08\n",
            "35709/36781 total: 97.09\n",
            "35710/36781 total: 97.09\n",
            "35711/36781 total: 97.09\n",
            "35712/36781 total: 97.09\n",
            "35713/36781 total: 97.1\n",
            "35714/36781 total: 97.1\n",
            "35715/36781 total: 97.1\n",
            "35716/36781 total: 97.1\n",
            "35717/36781 total: 97.11\n",
            "35718/36781 total: 97.11\n",
            "35719/36781 total: 97.11\n",
            "35720/36781 total: 97.12\n",
            "35721/36781 total: 97.12\n",
            "35722/36781 total: 97.12\n",
            "35723/36781 total: 97.12\n",
            "35724/36781 total: 97.13\n",
            "35725/36781 total: 97.13\n",
            "35726/36781 total: 97.13\n",
            "35727/36781 total: 97.13\n",
            "35728/36781 total: 97.14\n",
            "35729/36781 total: 97.14\n",
            "35730/36781 total: 97.14\n",
            "35731/36781 total: 97.15\n",
            "35732/36781 total: 97.15\n",
            "35733/36781 total: 97.15\n",
            "35734/36781 total: 97.15\n",
            "35735/36781 total: 97.16\n",
            "35736/36781 total: 97.16\n",
            "35737/36781 total: 97.16\n",
            "35738/36781 total: 97.16\n",
            "35739/36781 total: 97.17\n",
            "35740/36781 total: 97.17\n",
            "35741/36781 total: 97.17\n",
            "35742/36781 total: 97.18\n",
            "35743/36781 total: 97.18\n",
            "35744/36781 total: 97.18\n",
            "35745/36781 total: 97.18\n",
            "35746/36781 total: 97.19\n",
            "35747/36781 total: 97.19\n",
            "35748/36781 total: 97.19\n",
            "35749/36781 total: 97.19\n",
            "35750/36781 total: 97.2\n",
            "35751/36781 total: 97.2\n",
            "35752/36781 total: 97.2\n",
            "35753/36781 total: 97.21\n",
            "35754/36781 total: 97.21\n",
            "35755/36781 total: 97.21\n",
            "35756/36781 total: 97.21\n",
            "35757/36781 total: 97.22\n",
            "35758/36781 total: 97.22\n",
            "35759/36781 total: 97.22\n",
            "35760/36781 total: 97.22\n",
            "35761/36781 total: 97.23\n",
            "35762/36781 total: 97.23\n",
            "35763/36781 total: 97.23\n",
            "35764/36781 total: 97.23\n",
            "35765/36781 total: 97.24\n",
            "35766/36781 total: 97.24\n",
            "35767/36781 total: 97.24\n",
            "35768/36781 total: 97.25\n",
            "35769/36781 total: 97.25\n",
            "35770/36781 total: 97.25\n",
            "35771/36781 total: 97.25\n",
            "35772/36781 total: 97.26\n",
            "35773/36781 total: 97.26\n",
            "35774/36781 total: 97.26\n",
            "35775/36781 total: 97.26\n",
            "35776/36781 total: 97.27\n",
            "35777/36781 total: 97.27\n",
            "35778/36781 total: 97.27\n",
            "35779/36781 total: 97.28\n",
            "35780/36781 total: 97.28\n",
            "35781/36781 total: 97.28\n",
            "35782/36781 total: 97.28\n",
            "35783/36781 total: 97.29\n",
            "35784/36781 total: 97.29\n",
            "35785/36781 total: 97.29\n",
            "35786/36781 total: 97.29\n",
            "35787/36781 total: 97.3\n",
            "35788/36781 total: 97.3\n",
            "35789/36781 total: 97.3\n",
            "35790/36781 total: 97.31\n",
            "35791/36781 total: 97.31\n",
            "35792/36781 total: 97.31\n",
            "35793/36781 total: 97.31\n",
            "35794/36781 total: 97.32\n",
            "35795/36781 total: 97.32\n",
            "35796/36781 total: 97.32\n",
            "35797/36781 total: 97.32\n",
            "35798/36781 total: 97.33\n",
            "35799/36781 total: 97.33\n",
            "35800/36781 total: 97.33\n",
            "35801/36781 total: 97.34\n",
            "35802/36781 total: 97.34\n",
            "35803/36781 total: 97.34\n",
            "35804/36781 total: 97.34\n",
            "35805/36781 total: 97.35\n",
            "35806/36781 total: 97.35\n",
            "35807/36781 total: 97.35\n",
            "35808/36781 total: 97.35\n",
            "35809/36781 total: 97.36\n",
            "35810/36781 total: 97.36\n",
            "35811/36781 total: 97.36\n",
            "35812/36781 total: 97.37\n",
            "35813/36781 total: 97.37\n",
            "35814/36781 total: 97.37\n",
            "35815/36781 total: 97.37\n",
            "35816/36781 total: 97.38\n",
            "35817/36781 total: 97.38\n",
            "35818/36781 total: 97.38\n",
            "35819/36781 total: 97.38\n",
            "35820/36781 total: 97.39\n",
            "35821/36781 total: 97.39\n",
            "35822/36781 total: 97.39\n",
            "35823/36781 total: 97.4\n",
            "35824/36781 total: 97.4\n",
            "35825/36781 total: 97.4\n",
            "35826/36781 total: 97.4\n",
            "35827/36781 total: 97.41\n",
            "35828/36781 total: 97.41\n",
            "35829/36781 total: 97.41\n",
            "35830/36781 total: 97.41\n",
            "35831/36781 total: 97.42\n",
            "35832/36781 total: 97.42\n",
            "35833/36781 total: 97.42\n",
            "35834/36781 total: 97.43\n",
            "35835/36781 total: 97.43\n",
            "35836/36781 total: 97.43\n",
            "35837/36781 total: 97.43\n",
            "35838/36781 total: 97.44\n",
            "35839/36781 total: 97.44\n",
            "35840/36781 total: 97.44\n",
            "35841/36781 total: 97.44\n",
            "35842/36781 total: 97.45\n",
            "35843/36781 total: 97.45\n",
            "35844/36781 total: 97.45\n",
            "35845/36781 total: 97.46\n",
            "35846/36781 total: 97.46\n",
            "35847/36781 total: 97.46\n",
            "35848/36781 total: 97.46\n",
            "35849/36781 total: 97.47\n",
            "35850/36781 total: 97.47\n",
            "35851/36781 total: 97.47\n",
            "35852/36781 total: 97.47\n",
            "35853/36781 total: 97.48\n",
            "35854/36781 total: 97.48\n",
            "35855/36781 total: 97.48\n",
            "35856/36781 total: 97.49\n",
            "35857/36781 total: 97.49\n",
            "35858/36781 total: 97.49\n",
            "35859/36781 total: 97.49\n",
            "35860/36781 total: 97.5\n",
            "35861/36781 total: 97.5\n",
            "35862/36781 total: 97.5\n",
            "35863/36781 total: 97.5\n",
            "35864/36781 total: 97.51\n",
            "35865/36781 total: 97.51\n",
            "35866/36781 total: 97.51\n",
            "35867/36781 total: 97.52\n",
            "35868/36781 total: 97.52\n",
            "35869/36781 total: 97.52\n",
            "35870/36781 total: 97.52\n",
            "35871/36781 total: 97.53\n",
            "35872/36781 total: 97.53\n",
            "35873/36781 total: 97.53\n",
            "35874/36781 total: 97.53\n",
            "35875/36781 total: 97.54\n",
            "35876/36781 total: 97.54\n",
            "35877/36781 total: 97.54\n",
            "35878/36781 total: 97.54\n",
            "35879/36781 total: 97.55\n",
            "35880/36781 total: 97.55\n",
            "35881/36781 total: 97.55\n",
            "35882/36781 total: 97.56\n",
            "35883/36781 total: 97.56\n",
            "35884/36781 total: 97.56\n",
            "35885/36781 total: 97.56\n",
            "35886/36781 total: 97.57\n",
            "35887/36781 total: 97.57\n",
            "35888/36781 total: 97.57\n",
            "35889/36781 total: 97.57\n",
            "35890/36781 total: 97.58\n",
            "35891/36781 total: 97.58\n",
            "35892/36781 total: 97.58\n",
            "35893/36781 total: 97.59\n",
            "35894/36781 total: 97.59\n",
            "35895/36781 total: 97.59\n",
            "35896/36781 total: 97.59\n",
            "35897/36781 total: 97.6\n",
            "35898/36781 total: 97.6\n",
            "35899/36781 total: 97.6\n",
            "35900/36781 total: 97.6\n",
            "35901/36781 total: 97.61\n",
            "35902/36781 total: 97.61\n",
            "35903/36781 total: 97.61\n",
            "35904/36781 total: 97.62\n",
            "35905/36781 total: 97.62\n",
            "35906/36781 total: 97.62\n",
            "35907/36781 total: 97.62\n",
            "35908/36781 total: 97.63\n",
            "35909/36781 total: 97.63\n",
            "35910/36781 total: 97.63\n",
            "35911/36781 total: 97.63\n",
            "35912/36781 total: 97.64\n",
            "35913/36781 total: 97.64\n",
            "35914/36781 total: 97.64\n",
            "35915/36781 total: 97.65\n",
            "35916/36781 total: 97.65\n",
            "35917/36781 total: 97.65\n",
            "35918/36781 total: 97.65\n",
            "35919/36781 total: 97.66\n",
            "35920/36781 total: 97.66\n",
            "35921/36781 total: 97.66\n",
            "35922/36781 total: 97.66\n",
            "35923/36781 total: 97.67\n",
            "35924/36781 total: 97.67\n",
            "35925/36781 total: 97.67\n",
            "35926/36781 total: 97.68\n",
            "35927/36781 total: 97.68\n",
            "35928/36781 total: 97.68\n",
            "35929/36781 total: 97.68\n",
            "35930/36781 total: 97.69\n",
            "35931/36781 total: 97.69\n",
            "35932/36781 total: 97.69\n",
            "35933/36781 total: 97.69\n",
            "35934/36781 total: 97.7\n",
            "35935/36781 total: 97.7\n",
            "35936/36781 total: 97.7\n",
            "35937/36781 total: 97.71\n",
            "35938/36781 total: 97.71\n",
            "35939/36781 total: 97.71\n",
            "35940/36781 total: 97.71\n",
            "35941/36781 total: 97.72\n",
            "35942/36781 total: 97.72\n",
            "35943/36781 total: 97.72\n",
            "35944/36781 total: 97.72\n",
            "35945/36781 total: 97.73\n",
            "35946/36781 total: 97.73\n",
            "35947/36781 total: 97.73\n",
            "35948/36781 total: 97.74\n",
            "35949/36781 total: 97.74\n",
            "35950/36781 total: 97.74\n",
            "35951/36781 total: 97.74\n",
            "35952/36781 total: 97.75\n",
            "35953/36781 total: 97.75\n",
            "35954/36781 total: 97.75\n",
            "35955/36781 total: 97.75\n",
            "35956/36781 total: 97.76\n",
            "35957/36781 total: 97.76\n",
            "35958/36781 total: 97.76\n",
            "35959/36781 total: 97.77\n",
            "35960/36781 total: 97.77\n",
            "35961/36781 total: 97.77\n",
            "35962/36781 total: 97.77\n",
            "35963/36781 total: 97.78\n",
            "35964/36781 total: 97.78\n",
            "35965/36781 total: 97.78\n",
            "35966/36781 total: 97.78\n",
            "35967/36781 total: 97.79\n",
            "35968/36781 total: 97.79\n",
            "35969/36781 total: 97.79\n",
            "35970/36781 total: 97.8\n",
            "35971/36781 total: 97.8\n",
            "35972/36781 total: 97.8\n",
            "35973/36781 total: 97.8\n",
            "35974/36781 total: 97.81\n",
            "35975/36781 total: 97.81\n",
            "35976/36781 total: 97.81\n",
            "35977/36781 total: 97.81\n",
            "35978/36781 total: 97.82\n",
            "35979/36781 total: 97.82\n",
            "35980/36781 total: 97.82\n",
            "35981/36781 total: 97.82\n",
            "35982/36781 total: 97.83\n",
            "35983/36781 total: 97.83\n",
            "35984/36781 total: 97.83\n",
            "35985/36781 total: 97.84\n",
            "35986/36781 total: 97.84\n",
            "35987/36781 total: 97.84\n",
            "35988/36781 total: 97.84\n",
            "35989/36781 total: 97.85\n",
            "35990/36781 total: 97.85\n",
            "35991/36781 total: 97.85\n",
            "35992/36781 total: 97.85\n",
            "35993/36781 total: 97.86\n",
            "35994/36781 total: 97.86\n",
            "35995/36781 total: 97.86\n",
            "35996/36781 total: 97.87\n",
            "35997/36781 total: 97.87\n",
            "35998/36781 total: 97.87\n",
            "35999/36781 total: 97.87\n",
            "36000/36781 total: 97.88\n",
            "36001/36781 total: 97.88\n",
            "36002/36781 total: 97.88\n",
            "36003/36781 total: 97.88\n",
            "36004/36781 total: 97.89\n",
            "36005/36781 total: 97.89\n",
            "36006/36781 total: 97.89\n",
            "36007/36781 total: 97.9\n",
            "36008/36781 total: 97.9\n",
            "36009/36781 total: 97.9\n",
            "36010/36781 total: 97.9\n",
            "36011/36781 total: 97.91\n",
            "36012/36781 total: 97.91\n",
            "36013/36781 total: 97.91\n",
            "36014/36781 total: 97.91\n",
            "36015/36781 total: 97.92\n",
            "36016/36781 total: 97.92\n",
            "36017/36781 total: 97.92\n",
            "36018/36781 total: 97.93\n",
            "36019/36781 total: 97.93\n",
            "36020/36781 total: 97.93\n",
            "36021/36781 total: 97.93\n",
            "36022/36781 total: 97.94\n",
            "36023/36781 total: 97.94\n",
            "36024/36781 total: 97.94\n",
            "36025/36781 total: 97.94\n",
            "36026/36781 total: 97.95\n",
            "36027/36781 total: 97.95\n",
            "36028/36781 total: 97.95\n",
            "36029/36781 total: 97.96\n",
            "36030/36781 total: 97.96\n",
            "36031/36781 total: 97.96\n",
            "36032/36781 total: 97.96\n",
            "36033/36781 total: 97.97\n",
            "36034/36781 total: 97.97\n",
            "36035/36781 total: 97.97\n",
            "36036/36781 total: 97.97\n",
            "36037/36781 total: 97.98\n",
            "36038/36781 total: 97.98\n",
            "36039/36781 total: 97.98\n",
            "36040/36781 total: 97.99\n",
            "36041/36781 total: 97.99\n",
            "36042/36781 total: 97.99\n",
            "36043/36781 total: 97.99\n",
            "36044/36781 total: 98.0\n",
            "36045/36781 total: 98.0\n",
            "36046/36781 total: 98.0\n",
            "36047/36781 total: 98.0\n",
            "36048/36781 total: 98.01\n",
            "36049/36781 total: 98.01\n",
            "36050/36781 total: 98.01\n",
            "36051/36781 total: 98.02\n",
            "36052/36781 total: 98.02\n",
            "36053/36781 total: 98.02\n",
            "36054/36781 total: 98.02\n",
            "36055/36781 total: 98.03\n",
            "36056/36781 total: 98.03\n",
            "36057/36781 total: 98.03\n",
            "36058/36781 total: 98.03\n",
            "36059/36781 total: 98.04\n",
            "36060/36781 total: 98.04\n",
            "36061/36781 total: 98.04\n",
            "36062/36781 total: 98.05\n",
            "36063/36781 total: 98.05\n",
            "36064/36781 total: 98.05\n",
            "36065/36781 total: 98.05\n",
            "36066/36781 total: 98.06\n",
            "36067/36781 total: 98.06\n",
            "36068/36781 total: 98.06\n",
            "36069/36781 total: 98.06\n",
            "36070/36781 total: 98.07\n",
            "36071/36781 total: 98.07\n",
            "36072/36781 total: 98.07\n",
            "36073/36781 total: 98.08\n",
            "36074/36781 total: 98.08\n",
            "36075/36781 total: 98.08\n",
            "36076/36781 total: 98.08\n",
            "36077/36781 total: 98.09\n",
            "36078/36781 total: 98.09\n",
            "36079/36781 total: 98.09\n",
            "36080/36781 total: 98.09\n",
            "36081/36781 total: 98.1\n",
            "36082/36781 total: 98.1\n",
            "36083/36781 total: 98.1\n",
            "36084/36781 total: 98.1\n",
            "36085/36781 total: 98.11\n",
            "36086/36781 total: 98.11\n",
            "36087/36781 total: 98.11\n",
            "36088/36781 total: 98.12\n",
            "36089/36781 total: 98.12\n",
            "36090/36781 total: 98.12\n",
            "36091/36781 total: 98.12\n",
            "36092/36781 total: 98.13\n",
            "36093/36781 total: 98.13\n",
            "36094/36781 total: 98.13\n",
            "36095/36781 total: 98.13\n",
            "36096/36781 total: 98.14\n",
            "36097/36781 total: 98.14\n",
            "36098/36781 total: 98.14\n",
            "36099/36781 total: 98.15\n",
            "36100/36781 total: 98.15\n",
            "36101/36781 total: 98.15\n",
            "36102/36781 total: 98.15\n",
            "36103/36781 total: 98.16\n",
            "36104/36781 total: 98.16\n",
            "36105/36781 total: 98.16\n",
            "36106/36781 total: 98.16\n",
            "36107/36781 total: 98.17\n",
            "36108/36781 total: 98.17\n",
            "36109/36781 total: 98.17\n",
            "36110/36781 total: 98.18\n",
            "36111/36781 total: 98.18\n",
            "36112/36781 total: 98.18\n",
            "36113/36781 total: 98.18\n",
            "36114/36781 total: 98.19\n",
            "36115/36781 total: 98.19\n",
            "36116/36781 total: 98.19\n",
            "36117/36781 total: 98.19\n",
            "36118/36781 total: 98.2\n",
            "36119/36781 total: 98.2\n",
            "36120/36781 total: 98.2\n",
            "36121/36781 total: 98.21\n",
            "36122/36781 total: 98.21\n",
            "36123/36781 total: 98.21\n",
            "36124/36781 total: 98.21\n",
            "36125/36781 total: 98.22\n",
            "36126/36781 total: 98.22\n",
            "36127/36781 total: 98.22\n",
            "36128/36781 total: 98.22\n",
            "36129/36781 total: 98.23\n",
            "36130/36781 total: 98.23\n",
            "36131/36781 total: 98.23\n",
            "36132/36781 total: 98.24\n",
            "36133/36781 total: 98.24\n",
            "36134/36781 total: 98.24\n",
            "36135/36781 total: 98.24\n",
            "36136/36781 total: 98.25\n",
            "36137/36781 total: 98.25\n",
            "36138/36781 total: 98.25\n",
            "36139/36781 total: 98.25\n",
            "36140/36781 total: 98.26\n",
            "36141/36781 total: 98.26\n",
            "36142/36781 total: 98.26\n",
            "36143/36781 total: 98.27\n",
            "36144/36781 total: 98.27\n",
            "36145/36781 total: 98.27\n",
            "36146/36781 total: 98.27\n",
            "36147/36781 total: 98.28\n",
            "36148/36781 total: 98.28\n",
            "36149/36781 total: 98.28\n",
            "36150/36781 total: 98.28\n",
            "36151/36781 total: 98.29\n",
            "36152/36781 total: 98.29\n",
            "36153/36781 total: 98.29\n",
            "36154/36781 total: 98.3\n",
            "36155/36781 total: 98.3\n",
            "36156/36781 total: 98.3\n",
            "36157/36781 total: 98.3\n",
            "36158/36781 total: 98.31\n",
            "36159/36781 total: 98.31\n",
            "36160/36781 total: 98.31\n",
            "36161/36781 total: 98.31\n",
            "36162/36781 total: 98.32\n",
            "36163/36781 total: 98.32\n",
            "36164/36781 total: 98.32\n",
            "36165/36781 total: 98.33\n",
            "36166/36781 total: 98.33\n",
            "36167/36781 total: 98.33\n",
            "36168/36781 total: 98.33\n",
            "36169/36781 total: 98.34\n",
            "36170/36781 total: 98.34\n",
            "36171/36781 total: 98.34\n",
            "36172/36781 total: 98.34\n",
            "36173/36781 total: 98.35\n",
            "36174/36781 total: 98.35\n",
            "36175/36781 total: 98.35\n",
            "36176/36781 total: 98.36\n",
            "36177/36781 total: 98.36\n",
            "36178/36781 total: 98.36\n",
            "36179/36781 total: 98.36\n",
            "36180/36781 total: 98.37\n",
            "36181/36781 total: 98.37\n",
            "36182/36781 total: 98.37\n",
            "36183/36781 total: 98.37\n",
            "36184/36781 total: 98.38\n",
            "36185/36781 total: 98.38\n",
            "36186/36781 total: 98.38\n",
            "36187/36781 total: 98.39\n",
            "36188/36781 total: 98.39\n",
            "36189/36781 total: 98.39\n",
            "36190/36781 total: 98.39\n",
            "36191/36781 total: 98.4\n",
            "36192/36781 total: 98.4\n",
            "36193/36781 total: 98.4\n",
            "36194/36781 total: 98.4\n",
            "36195/36781 total: 98.41\n",
            "36196/36781 total: 98.41\n",
            "36197/36781 total: 98.41\n",
            "36198/36781 total: 98.41\n",
            "36199/36781 total: 98.42\n",
            "36200/36781 total: 98.42\n",
            "36201/36781 total: 98.42\n",
            "36202/36781 total: 98.43\n",
            "36203/36781 total: 98.43\n",
            "36204/36781 total: 98.43\n",
            "36205/36781 total: 98.43\n",
            "36206/36781 total: 98.44\n",
            "36207/36781 total: 98.44\n",
            "36208/36781 total: 98.44\n",
            "36209/36781 total: 98.44\n",
            "36210/36781 total: 98.45\n",
            "36211/36781 total: 98.45\n",
            "36212/36781 total: 98.45\n",
            "36213/36781 total: 98.46\n",
            "36214/36781 total: 98.46\n",
            "36215/36781 total: 98.46\n",
            "36216/36781 total: 98.46\n",
            "36217/36781 total: 98.47\n",
            "36218/36781 total: 98.47\n",
            "36219/36781 total: 98.47\n",
            "36220/36781 total: 98.47\n",
            "36221/36781 total: 98.48\n",
            "36222/36781 total: 98.48\n",
            "36223/36781 total: 98.48\n",
            "36224/36781 total: 98.49\n",
            "36225/36781 total: 98.49\n",
            "36226/36781 total: 98.49\n",
            "36227/36781 total: 98.49\n",
            "36228/36781 total: 98.5\n",
            "36229/36781 total: 98.5\n",
            "36230/36781 total: 98.5\n",
            "36231/36781 total: 98.5\n",
            "36232/36781 total: 98.51\n",
            "36233/36781 total: 98.51\n",
            "36234/36781 total: 98.51\n",
            "36235/36781 total: 98.52\n",
            "36236/36781 total: 98.52\n",
            "36237/36781 total: 98.52\n",
            "36238/36781 total: 98.52\n",
            "36239/36781 total: 98.53\n",
            "36240/36781 total: 98.53\n",
            "36241/36781 total: 98.53\n",
            "36242/36781 total: 98.53\n",
            "36243/36781 total: 98.54\n",
            "36244/36781 total: 98.54\n",
            "36245/36781 total: 98.54\n",
            "36246/36781 total: 98.55\n",
            "36247/36781 total: 98.55\n",
            "36248/36781 total: 98.55\n",
            "36249/36781 total: 98.55\n",
            "36250/36781 total: 98.56\n",
            "36251/36781 total: 98.56\n",
            "36252/36781 total: 98.56\n",
            "36253/36781 total: 98.56\n",
            "36254/36781 total: 98.57\n",
            "36255/36781 total: 98.57\n",
            "36256/36781 total: 98.57\n",
            "36257/36781 total: 98.58\n",
            "36258/36781 total: 98.58\n",
            "36259/36781 total: 98.58\n",
            "36260/36781 total: 98.58\n",
            "36261/36781 total: 98.59\n",
            "36262/36781 total: 98.59\n",
            "36263/36781 total: 98.59\n",
            "36264/36781 total: 98.59\n",
            "36265/36781 total: 98.6\n",
            "36266/36781 total: 98.6\n",
            "36267/36781 total: 98.6\n",
            "36268/36781 total: 98.61\n",
            "36269/36781 total: 98.61\n",
            "36270/36781 total: 98.61\n",
            "36271/36781 total: 98.61\n",
            "36272/36781 total: 98.62\n",
            "36273/36781 total: 98.62\n",
            "36274/36781 total: 98.62\n",
            "36275/36781 total: 98.62\n",
            "36276/36781 total: 98.63\n",
            "36277/36781 total: 98.63\n",
            "36278/36781 total: 98.63\n",
            "36279/36781 total: 98.64\n",
            "36280/36781 total: 98.64\n",
            "36281/36781 total: 98.64\n",
            "36282/36781 total: 98.64\n",
            "36283/36781 total: 98.65\n",
            "36284/36781 total: 98.65\n",
            "36285/36781 total: 98.65\n",
            "36286/36781 total: 98.65\n",
            "36287/36781 total: 98.66\n",
            "36288/36781 total: 98.66\n",
            "36289/36781 total: 98.66\n",
            "36290/36781 total: 98.67\n",
            "36291/36781 total: 98.67\n",
            "36292/36781 total: 98.67\n",
            "36293/36781 total: 98.67\n",
            "36294/36781 total: 98.68\n",
            "36295/36781 total: 98.68\n",
            "36296/36781 total: 98.68\n",
            "36297/36781 total: 98.68\n",
            "36298/36781 total: 98.69\n",
            "36299/36781 total: 98.69\n",
            "36300/36781 total: 98.69\n",
            "36301/36781 total: 98.69\n",
            "36302/36781 total: 98.7\n",
            "36303/36781 total: 98.7\n",
            "36304/36781 total: 98.7\n",
            "36305/36781 total: 98.71\n",
            "36306/36781 total: 98.71\n",
            "36307/36781 total: 98.71\n",
            "36308/36781 total: 98.71\n",
            "36309/36781 total: 98.72\n",
            "36310/36781 total: 98.72\n",
            "36311/36781 total: 98.72\n",
            "36312/36781 total: 98.72\n",
            "36313/36781 total: 98.73\n",
            "36314/36781 total: 98.73\n",
            "36315/36781 total: 98.73\n",
            "36316/36781 total: 98.74\n",
            "36317/36781 total: 98.74\n",
            "36318/36781 total: 98.74\n",
            "36319/36781 total: 98.74\n",
            "36320/36781 total: 98.75\n",
            "36321/36781 total: 98.75\n",
            "36322/36781 total: 98.75\n",
            "36323/36781 total: 98.75\n",
            "36324/36781 total: 98.76\n",
            "36325/36781 total: 98.76\n",
            "36326/36781 total: 98.76\n",
            "36327/36781 total: 98.77\n",
            "36328/36781 total: 98.77\n",
            "36329/36781 total: 98.77\n",
            "36330/36781 total: 98.77\n",
            "36331/36781 total: 98.78\n",
            "36332/36781 total: 98.78\n",
            "36333/36781 total: 98.78\n",
            "36334/36781 total: 98.78\n",
            "36335/36781 total: 98.79\n",
            "36336/36781 total: 98.79\n",
            "36337/36781 total: 98.79\n",
            "36338/36781 total: 98.8\n",
            "36339/36781 total: 98.8\n",
            "36340/36781 total: 98.8\n",
            "36341/36781 total: 98.8\n",
            "36342/36781 total: 98.81\n",
            "36343/36781 total: 98.81\n",
            "36344/36781 total: 98.81\n",
            "36345/36781 total: 98.81\n",
            "36346/36781 total: 98.82\n",
            "36347/36781 total: 98.82\n",
            "36348/36781 total: 98.82\n",
            "36349/36781 total: 98.83\n",
            "36350/36781 total: 98.83\n",
            "36351/36781 total: 98.83\n",
            "36352/36781 total: 98.83\n",
            "36353/36781 total: 98.84\n",
            "36354/36781 total: 98.84\n",
            "36355/36781 total: 98.84\n",
            "36356/36781 total: 98.84\n",
            "36357/36781 total: 98.85\n",
            "36358/36781 total: 98.85\n",
            "36359/36781 total: 98.85\n",
            "36360/36781 total: 98.86\n",
            "36361/36781 total: 98.86\n",
            "36362/36781 total: 98.86\n",
            "36363/36781 total: 98.86\n",
            "36364/36781 total: 98.87\n",
            "36365/36781 total: 98.87\n",
            "36366/36781 total: 98.87\n",
            "36367/36781 total: 98.87\n",
            "36368/36781 total: 98.88\n",
            "36369/36781 total: 98.88\n",
            "36370/36781 total: 98.88\n",
            "36371/36781 total: 98.89\n",
            "36372/36781 total: 98.89\n",
            "36373/36781 total: 98.89\n",
            "36374/36781 total: 98.89\n",
            "36375/36781 total: 98.9\n",
            "36376/36781 total: 98.9\n",
            "36377/36781 total: 98.9\n",
            "36378/36781 total: 98.9\n",
            "36379/36781 total: 98.91\n",
            "36380/36781 total: 98.91\n",
            "36381/36781 total: 98.91\n",
            "36382/36781 total: 98.92\n",
            "36383/36781 total: 98.92\n",
            "36384/36781 total: 98.92\n",
            "36385/36781 total: 98.92\n",
            "36386/36781 total: 98.93\n",
            "36387/36781 total: 98.93\n",
            "36388/36781 total: 98.93\n",
            "36389/36781 total: 98.93\n",
            "36390/36781 total: 98.94\n",
            "36391/36781 total: 98.94\n",
            "36392/36781 total: 98.94\n",
            "36393/36781 total: 98.95\n",
            "36394/36781 total: 98.95\n",
            "36395/36781 total: 98.95\n",
            "36396/36781 total: 98.95\n",
            "36397/36781 total: 98.96\n",
            "36398/36781 total: 98.96\n",
            "36399/36781 total: 98.96\n",
            "36400/36781 total: 98.96\n",
            "36401/36781 total: 98.97\n",
            "36402/36781 total: 98.97\n",
            "36403/36781 total: 98.97\n",
            "36404/36781 total: 98.98\n",
            "36405/36781 total: 98.98\n",
            "36406/36781 total: 98.98\n",
            "36407/36781 total: 98.98\n",
            "36408/36781 total: 98.99\n",
            "36409/36781 total: 98.99\n",
            "36410/36781 total: 98.99\n",
            "36411/36781 total: 98.99\n",
            "36412/36781 total: 99.0\n",
            "36413/36781 total: 99.0\n",
            "36414/36781 total: 99.0\n",
            "36415/36781 total: 99.0\n",
            "36416/36781 total: 99.01\n",
            "36417/36781 total: 99.01\n",
            "36418/36781 total: 99.01\n",
            "36419/36781 total: 99.02\n",
            "36420/36781 total: 99.02\n",
            "36421/36781 total: 99.02\n",
            "36422/36781 total: 99.02\n",
            "36423/36781 total: 99.03\n",
            "36424/36781 total: 99.03\n",
            "36425/36781 total: 99.03\n",
            "36426/36781 total: 99.03\n",
            "36427/36781 total: 99.04\n",
            "36428/36781 total: 99.04\n",
            "36429/36781 total: 99.04\n",
            "36430/36781 total: 99.05\n",
            "36431/36781 total: 99.05\n",
            "36432/36781 total: 99.05\n",
            "36433/36781 total: 99.05\n",
            "36434/36781 total: 99.06\n",
            "36435/36781 total: 99.06\n",
            "36436/36781 total: 99.06\n",
            "36437/36781 total: 99.06\n",
            "36438/36781 total: 99.07\n",
            "36439/36781 total: 99.07\n",
            "36440/36781 total: 99.07\n",
            "36441/36781 total: 99.08\n",
            "36442/36781 total: 99.08\n",
            "36443/36781 total: 99.08\n",
            "36444/36781 total: 99.08\n",
            "36445/36781 total: 99.09\n",
            "36446/36781 total: 99.09\n",
            "36447/36781 total: 99.09\n",
            "36448/36781 total: 99.09\n",
            "36449/36781 total: 99.1\n",
            "36450/36781 total: 99.1\n",
            "36451/36781 total: 99.1\n",
            "36452/36781 total: 99.11\n",
            "36453/36781 total: 99.11\n",
            "36454/36781 total: 99.11\n",
            "36455/36781 total: 99.11\n",
            "36456/36781 total: 99.12\n",
            "36457/36781 total: 99.12\n",
            "36458/36781 total: 99.12\n",
            "36459/36781 total: 99.12\n",
            "36460/36781 total: 99.13\n",
            "36461/36781 total: 99.13\n",
            "36462/36781 total: 99.13\n",
            "36463/36781 total: 99.14\n",
            "36464/36781 total: 99.14\n",
            "36465/36781 total: 99.14\n",
            "36466/36781 total: 99.14\n",
            "36467/36781 total: 99.15\n",
            "36468/36781 total: 99.15\n",
            "36469/36781 total: 99.15\n",
            "36470/36781 total: 99.15\n",
            "36471/36781 total: 99.16\n",
            "36472/36781 total: 99.16\n",
            "36473/36781 total: 99.16\n",
            "36474/36781 total: 99.17\n",
            "36475/36781 total: 99.17\n",
            "36476/36781 total: 99.17\n",
            "36477/36781 total: 99.17\n",
            "36478/36781 total: 99.18\n",
            "36479/36781 total: 99.18\n",
            "36480/36781 total: 99.18\n",
            "36481/36781 total: 99.18\n",
            "36482/36781 total: 99.19\n",
            "36483/36781 total: 99.19\n",
            "36484/36781 total: 99.19\n",
            "36485/36781 total: 99.2\n",
            "36486/36781 total: 99.2\n",
            "36487/36781 total: 99.2\n",
            "36488/36781 total: 99.2\n",
            "36489/36781 total: 99.21\n",
            "36490/36781 total: 99.21\n",
            "36491/36781 total: 99.21\n",
            "36492/36781 total: 99.21\n",
            "36493/36781 total: 99.22\n",
            "36494/36781 total: 99.22\n",
            "36495/36781 total: 99.22\n",
            "36496/36781 total: 99.23\n",
            "36497/36781 total: 99.23\n",
            "36498/36781 total: 99.23\n",
            "36499/36781 total: 99.23\n",
            "36500/36781 total: 99.24\n",
            "36501/36781 total: 99.24\n",
            "36502/36781 total: 99.24\n",
            "36503/36781 total: 99.24\n",
            "36504/36781 total: 99.25\n",
            "36505/36781 total: 99.25\n",
            "36506/36781 total: 99.25\n",
            "36507/36781 total: 99.26\n",
            "36508/36781 total: 99.26\n",
            "36509/36781 total: 99.26\n",
            "36510/36781 total: 99.26\n",
            "36511/36781 total: 99.27\n",
            "36512/36781 total: 99.27\n",
            "36513/36781 total: 99.27\n",
            "36514/36781 total: 99.27\n",
            "36515/36781 total: 99.28\n",
            "36516/36781 total: 99.28\n",
            "36517/36781 total: 99.28\n",
            "36518/36781 total: 99.28\n",
            "36519/36781 total: 99.29\n",
            "36520/36781 total: 99.29\n",
            "36521/36781 total: 99.29\n",
            "36522/36781 total: 99.3\n",
            "36523/36781 total: 99.3\n",
            "36524/36781 total: 99.3\n",
            "36525/36781 total: 99.3\n",
            "36526/36781 total: 99.31\n",
            "36527/36781 total: 99.31\n",
            "36528/36781 total: 99.31\n",
            "36529/36781 total: 99.31\n",
            "36530/36781 total: 99.32\n",
            "36531/36781 total: 99.32\n",
            "36532/36781 total: 99.32\n",
            "36533/36781 total: 99.33\n",
            "36534/36781 total: 99.33\n",
            "36535/36781 total: 99.33\n",
            "36536/36781 total: 99.33\n",
            "36537/36781 total: 99.34\n",
            "36538/36781 total: 99.34\n",
            "36539/36781 total: 99.34\n",
            "36540/36781 total: 99.34\n",
            "36541/36781 total: 99.35\n",
            "36542/36781 total: 99.35\n",
            "36543/36781 total: 99.35\n",
            "36544/36781 total: 99.36\n",
            "36545/36781 total: 99.36\n",
            "36546/36781 total: 99.36\n",
            "36547/36781 total: 99.36\n",
            "36548/36781 total: 99.37\n",
            "36549/36781 total: 99.37\n",
            "36550/36781 total: 99.37\n",
            "36551/36781 total: 99.37\n",
            "36552/36781 total: 99.38\n",
            "36553/36781 total: 99.38\n",
            "36554/36781 total: 99.38\n",
            "36555/36781 total: 99.39\n",
            "36556/36781 total: 99.39\n",
            "36557/36781 total: 99.39\n",
            "36558/36781 total: 99.39\n",
            "36559/36781 total: 99.4\n",
            "36560/36781 total: 99.4\n",
            "36561/36781 total: 99.4\n",
            "36562/36781 total: 99.4\n",
            "36563/36781 total: 99.41\n",
            "36564/36781 total: 99.41\n",
            "36565/36781 total: 99.41\n",
            "36566/36781 total: 99.42\n",
            "36567/36781 total: 99.42\n",
            "36568/36781 total: 99.42\n",
            "36569/36781 total: 99.42\n",
            "36570/36781 total: 99.43\n",
            "36571/36781 total: 99.43\n",
            "36572/36781 total: 99.43\n",
            "36573/36781 total: 99.43\n",
            "36574/36781 total: 99.44\n",
            "36575/36781 total: 99.44\n",
            "36576/36781 total: 99.44\n",
            "36577/36781 total: 99.45\n",
            "36578/36781 total: 99.45\n",
            "36579/36781 total: 99.45\n",
            "36580/36781 total: 99.45\n",
            "36581/36781 total: 99.46\n",
            "36582/36781 total: 99.46\n",
            "36583/36781 total: 99.46\n",
            "36584/36781 total: 99.46\n",
            "36585/36781 total: 99.47\n",
            "36586/36781 total: 99.47\n",
            "36587/36781 total: 99.47\n",
            "36588/36781 total: 99.48\n",
            "36589/36781 total: 99.48\n",
            "36590/36781 total: 99.48\n",
            "36591/36781 total: 99.48\n",
            "36592/36781 total: 99.49\n",
            "36593/36781 total: 99.49\n",
            "36594/36781 total: 99.49\n",
            "36595/36781 total: 99.49\n",
            "36596/36781 total: 99.5\n",
            "36597/36781 total: 99.5\n",
            "36598/36781 total: 99.5\n",
            "36599/36781 total: 99.51\n",
            "36600/36781 total: 99.51\n",
            "36601/36781 total: 99.51\n",
            "36602/36781 total: 99.51\n",
            "36603/36781 total: 99.52\n",
            "36604/36781 total: 99.52\n",
            "36605/36781 total: 99.52\n",
            "36606/36781 total: 99.52\n",
            "36607/36781 total: 99.53\n",
            "36608/36781 total: 99.53\n",
            "36609/36781 total: 99.53\n",
            "36610/36781 total: 99.54\n",
            "36611/36781 total: 99.54\n",
            "36612/36781 total: 99.54\n",
            "36613/36781 total: 99.54\n",
            "36614/36781 total: 99.55\n",
            "36615/36781 total: 99.55\n",
            "36616/36781 total: 99.55\n",
            "36617/36781 total: 99.55\n",
            "36618/36781 total: 99.56\n",
            "36619/36781 total: 99.56\n",
            "36620/36781 total: 99.56\n",
            "36621/36781 total: 99.56\n",
            "36622/36781 total: 99.57\n",
            "36623/36781 total: 99.57\n",
            "36624/36781 total: 99.57\n",
            "36625/36781 total: 99.58\n",
            "36626/36781 total: 99.58\n",
            "36627/36781 total: 99.58\n",
            "36628/36781 total: 99.58\n",
            "36629/36781 total: 99.59\n",
            "36630/36781 total: 99.59\n",
            "36631/36781 total: 99.59\n",
            "36632/36781 total: 99.59\n",
            "36633/36781 total: 99.6\n",
            "36634/36781 total: 99.6\n",
            "36635/36781 total: 99.6\n",
            "36636/36781 total: 99.61\n",
            "36637/36781 total: 99.61\n",
            "36638/36781 total: 99.61\n",
            "36639/36781 total: 99.61\n",
            "36640/36781 total: 99.62\n",
            "36641/36781 total: 99.62\n",
            "36642/36781 total: 99.62\n",
            "36643/36781 total: 99.62\n",
            "36644/36781 total: 99.63\n",
            "36645/36781 total: 99.63\n",
            "36646/36781 total: 99.63\n",
            "36647/36781 total: 99.64\n",
            "36648/36781 total: 99.64\n",
            "36649/36781 total: 99.64\n",
            "36650/36781 total: 99.64\n",
            "36651/36781 total: 99.65\n",
            "36652/36781 total: 99.65\n",
            "36653/36781 total: 99.65\n",
            "36654/36781 total: 99.65\n",
            "36655/36781 total: 99.66\n",
            "36656/36781 total: 99.66\n",
            "36657/36781 total: 99.66\n",
            "36658/36781 total: 99.67\n",
            "36659/36781 total: 99.67\n",
            "36660/36781 total: 99.67\n",
            "36661/36781 total: 99.67\n",
            "36662/36781 total: 99.68\n",
            "36663/36781 total: 99.68\n",
            "36664/36781 total: 99.68\n",
            "36665/36781 total: 99.68\n",
            "36666/36781 total: 99.69\n",
            "36667/36781 total: 99.69\n",
            "36668/36781 total: 99.69\n",
            "36669/36781 total: 99.7\n",
            "36670/36781 total: 99.7\n",
            "36671/36781 total: 99.7\n",
            "36672/36781 total: 99.7\n",
            "36673/36781 total: 99.71\n",
            "36674/36781 total: 99.71\n",
            "36675/36781 total: 99.71\n",
            "36676/36781 total: 99.71\n",
            "36677/36781 total: 99.72\n",
            "36678/36781 total: 99.72\n",
            "36679/36781 total: 99.72\n",
            "36680/36781 total: 99.73\n",
            "36681/36781 total: 99.73\n",
            "36682/36781 total: 99.73\n",
            "36683/36781 total: 99.73\n",
            "36684/36781 total: 99.74\n",
            "36685/36781 total: 99.74\n",
            "36686/36781 total: 99.74\n",
            "36687/36781 total: 99.74\n",
            "36688/36781 total: 99.75\n",
            "36689/36781 total: 99.75\n",
            "36690/36781 total: 99.75\n",
            "36691/36781 total: 99.76\n",
            "36692/36781 total: 99.76\n",
            "36693/36781 total: 99.76\n",
            "36694/36781 total: 99.76\n",
            "36695/36781 total: 99.77\n",
            "36696/36781 total: 99.77\n",
            "36697/36781 total: 99.77\n",
            "36698/36781 total: 99.77\n",
            "36699/36781 total: 99.78\n",
            "36700/36781 total: 99.78\n",
            "36701/36781 total: 99.78\n",
            "36702/36781 total: 99.79\n",
            "36703/36781 total: 99.79\n",
            "36704/36781 total: 99.79\n",
            "36705/36781 total: 99.79\n",
            "36706/36781 total: 99.8\n",
            "36707/36781 total: 99.8\n",
            "36708/36781 total: 99.8\n",
            "36709/36781 total: 99.8\n",
            "36710/36781 total: 99.81\n",
            "36711/36781 total: 99.81\n",
            "36712/36781 total: 99.81\n",
            "36713/36781 total: 99.82\n",
            "36714/36781 total: 99.82\n",
            "36715/36781 total: 99.82\n",
            "36716/36781 total: 99.82\n",
            "36717/36781 total: 99.83\n",
            "36718/36781 total: 99.83\n",
            "36719/36781 total: 99.83\n",
            "36720/36781 total: 99.83\n",
            "36721/36781 total: 99.84\n",
            "36722/36781 total: 99.84\n",
            "36723/36781 total: 99.84\n",
            "36724/36781 total: 99.85\n",
            "36725/36781 total: 99.85\n",
            "36726/36781 total: 99.85\n",
            "36727/36781 total: 99.85\n",
            "36728/36781 total: 99.86\n",
            "36729/36781 total: 99.86\n",
            "36730/36781 total: 99.86\n",
            "36731/36781 total: 99.86\n",
            "36732/36781 total: 99.87\n",
            "36733/36781 total: 99.87\n",
            "36734/36781 total: 99.87\n",
            "36735/36781 total: 99.87\n",
            "36736/36781 total: 99.88\n",
            "36737/36781 total: 99.88\n",
            "36738/36781 total: 99.88\n",
            "36739/36781 total: 99.89\n",
            "36740/36781 total: 99.89\n",
            "36741/36781 total: 99.89\n",
            "36742/36781 total: 99.89\n",
            "36743/36781 total: 99.9\n",
            "36744/36781 total: 99.9\n",
            "36745/36781 total: 99.9\n",
            "36746/36781 total: 99.9\n",
            "36747/36781 total: 99.91\n",
            "36748/36781 total: 99.91\n",
            "36749/36781 total: 99.91\n",
            "36750/36781 total: 99.92\n",
            "36751/36781 total: 99.92\n",
            "36752/36781 total: 99.92\n",
            "36753/36781 total: 99.92\n",
            "36754/36781 total: 99.93\n",
            "36755/36781 total: 99.93\n",
            "36756/36781 total: 99.93\n",
            "36757/36781 total: 99.93\n",
            "36758/36781 total: 99.94\n",
            "36759/36781 total: 99.94\n",
            "36760/36781 total: 99.94\n",
            "36761/36781 total: 99.95\n",
            "36762/36781 total: 99.95\n",
            "36763/36781 total: 99.95\n",
            "36764/36781 total: 99.95\n",
            "36765/36781 total: 99.96\n",
            "36766/36781 total: 99.96\n",
            "36767/36781 total: 99.96\n",
            "36768/36781 total: 99.96\n",
            "36769/36781 total: 99.97\n",
            "36770/36781 total: 99.97\n",
            "36771/36781 total: 99.97\n",
            "36772/36781 total: 99.98\n",
            "36773/36781 total: 99.98\n",
            "36774/36781 total: 99.98\n",
            "36775/36781 total: 99.98\n",
            "36776/36781 total: 99.99\n",
            "36777/36781 total: 99.99\n",
            "36778/36781 total: 99.99\n",
            "36779/36781 total: 99.99\n",
            "36780/36781 total: 100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi5XS4FbmqVK",
        "outputId": "c05840dd-8eaf-4999-d3f4-2808e139170e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd scripts/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tensorflow-yolov4-tflite/scripts\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7YDztBlmfDG"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DlMxdNDmIAP"
      },
      "source": [
        "mkdir dataset"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwaiHnTJmDEk"
      },
      "source": [
        "cd ./data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFqk3QoPlMdd",
        "outputId": "ca056944-6518-46cb-b7f5-0c1eda2914f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "#!/bin/bash\n",
        "\n",
        "### Recommended to run 'nohup ./<this_script> &' to prevent interruption from SSH session termination.\n",
        "\n",
        "wait_to_finish() {\n",
        "    for pid in \"${download_pids[@]}\"; do\n",
        "        while kill -0 \"$pid\"; do\n",
        "            sleep 30\n",
        "        done\n",
        "    done\n",
        "}\n",
        "\n",
        "\n",
        "# Update for default OS specific package manager.\n",
        "# sudo yum -y install java-1.8.0\n",
        "# sudo yum -y remove java-1.7.0-openjdk\n",
        "\n",
        "mkdir -p coco/images/ coco/annotations/\n",
        "\n",
        "download_pids=()\n",
        "\n",
        "### 2017 COCO Dataset ###\n",
        "\n",
        "echo \"Downloading COCO dataset...\"\n",
        "curl -OL \"http://images.cocodataset.org/zips/val2017.zip\" &\n",
        "download_pids+=(\"$!\")\n",
        "curl -OL \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\" &\n",
        "download_pids+=(\"$!\")\n",
        "\n",
        "wait_to_finish download_pids\n",
        "\n",
        "inflate_pids=()\n",
        "\n",
        "unzip 'val2017.zip' -d coco/images/ &\n",
        "inflate_pids+=(\"$!\")\n",
        "unzip 'annotations_trainval2017.zip' -d coco/annotations/ & # Inflates to 'coco/annotations'.\n",
        "inflate_pids+=(\"$!\")\n",
        "\n",
        "wait_to_finish inflate_pids"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading COCO dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  241M  100  241M    0     0  31.3M      0  0:00:07  0:00:07 --:--:-- 34.8M\n",
            "100  777M  100  777M    0     0  33.9M      0  0:00:22  0:00:22 --:--:-- 35.3M\n",
            "environment: line 6: kill: (391) - No such process\n",
            "environment: line 6: kill: (392) - No such process\n",
            "environment: line 6: kill: (391) - No such process\n",
            "environment: line 6: kill: (392) - No such process\n",
            "Archive:  val2017.zip\n",
            "Archive:  annotations_trainval2017.zip\n",
            "replace coco/annotations/annotations/instances_train2017.json? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
            "(EOF or read error, treating as \"[N]one\" ...)\n",
            "replace coco/images/val2017/000000212226.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
            "(EOF or read error, treating as \"[N]one\" ...)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66dG-d-LjdR0"
      },
      "source": [
        "!sh scripts/get_coco_dataset_2017.sh\n",
        "# Dont use this command, use the command above to download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URVSHP_GjWX2"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVTkUAo8jRsZ",
        "outputId": "14dd6818-c907-4f19-f6f1-a9640b183dad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd tensorflow-yolov4-tflite/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tensorflow-yolov4-tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1ymj5Qgic3W",
        "outputId": "75c88c89-fdf0-4b8d-f615-bcff41f330c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/vinhqngo5/tensorflow-yolov4-tflite.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'tensorflow-yolov4-tflite' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}